\documentclass{article}

% Latex Tutorial beginners: http://www.latex-tutorial.com/tutorials/beginners/
\newcommand{\comment}[1]{}

\newcommand{\note}[1]{{\tiny (#1)}}

% Packages explained - Adding more functions to LATEX http://www.latex-tutorial.com/tutorials/beginners/lesson-3/

\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{textgreek}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[]{babel}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.3in]{geometry}

% fügt die Literatur auch zum Inhaltsverzeichnis hinzu (ist so nicht default)
\usepackage[nottoc,notlot,notlof,numbib]{tocbibind}

% Adding a bibliography http://www.latex-tutorial.com/tutorials/beginners/lesson-7/
\bibliographystyle{apalike}

\graphicspath{ {charts/} }
% Nested numbered enumerations, eg. 1.1, 1.2: http://tex.stackexchange.com/questions/78842/nested-enumeration-numbering
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}


\title{
    Fitting Generalized Additive Models for\\ very large data sets with Apache Spark \\[7pt]
    \large Chapter Excerpt
}
\date{Future}
\author{Kai Thomas Brusch}

\begin{document}

    \pagenumbering{arabic}

    \maketitle

    \paragraph{Summary}

    This document contains three introductory chapters for my bachelor thesis titled "Fitting General Additive Models for very large datasets with Apache Spark".

    \paragraph{Contact} \texttt{kai.brusch@gmail.com}

    \paragraph{Location} Hochschule für Angewandte Wissenschaften Hamburg
    \paragraph{Department} Dept. Informatik
    \paragraph{Examiner} Prof. Dr. Michael Köhler-Bußmeier
    \paragraph{Second examiner} Dipl.-Mathematiker Univ. Markus Schmaus

    \newpage

    \tableofcontents

    \newpage

    
        \section{Statistical Learning}
    
     \cite{hastie} introduces statistical learning as a set of tools for modeling and understanding complex data sets. Recent developments in statistics and computer science have created an interesting field that empowers modern statistical with powerful computational methods. Statistical learning provides a powerful framework to ask and answer interesting questions with data. This thesis will focus on the generalized additive model, a method of the supervised learning branch of statistical learning. Supervised learning methods build a statistical model for predictions and estimations of an output variable based on one or many input variables. Statistical learning methods are essential for modern scientific research and have empowered many other branches of quantitative sciences. The professional world has been raving about the potential of statistical learning giving birth terms such as: Data Science and Big Data.
    
        \subsection{Why statistical learning?}
    
    Ideas are sometimes best conveyed through example. \cite{hastie} illustrated the benefits of statistical learning on a simple yet illustrative example. In the example we assume the be working for as statistical consultant in charge of advising a client on how to improve the sales of a given product. The client provides us with a data set containing the sales of a given product across several markets along with the budged for the product in each market in three different media outlets: TV, radio and newspaper. The client is not able to directly impact the amount of sales but he may to change amount spend on each media outlet. If we can understand the number of items sold as a function of spend on media we can suggest our client to adjust spending to increase units sold. In \ref{fig:sales} the sales are plotted on the Y axis and the budget for a media outlet on the X axis. \cite{hastie} uses this example to introduce terminology as well as the framework. The media budgets are the input variable and sales are the output variables. The input variables are commonly denoted with $X_i$, in example $X_1$ is the TV spend, $X_2$ the radio spend and $X_3$ the newspaper spend. The name for input variables has become inconsistent as the the terms predictor, independent variables and features can be used interchangeably. The output variable, in this example the sales, is also called the response or dependant variable and denoted with Y.
    
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{sales}
            \caption{\cite{hastie} illustrates the units sold as a function of spend on each of the advertisement channels. The blue line is a linear model describing the underlying relationship between units sold and spend on media outlet.}
            \label{fig:sales}
        \end{figure}
        
    This example illustrates the notion behind statistical learning but fails to capture it's broad applicability. To generalize this example the essential question of statistical learning needs to be generalized. Supposed a data set contains observations of a quantitative response $Y$ and several predictors $X_1, X_2 , ... X_i$. Statistical learning is the practice of looking for a relationship $f()$ that describes the relationship in the following form:
    \begin{equation}
        Y = f(X) + \epsilon
    \end{equation}
    The function $f()$ is a fixed but unknown function of the input variables $X_1, ... X_p$. $\epsilon$ is random, identical, independent distributed error term. Another way of framing this is to see the function $f()$ as the systematic information in the observations. All statistical learning methods are concerned with finding an optimal $f()$. This thesis will discuss three, related methods of finding $f()$. Each method with different idea on how to find $f()$ and with a different interpretation of optimal.


    \subsection{Why Estimate f()?}
    Framing the question of relationship between dependent and independent variable as the problem to find a function $f()$ has benefits. \cite{hastie} argues two essential reasons for estimating $f()$: inference and prediction. Inference is concerned with understanding a system while prediction wants to use the estimated function to predict $Y$ on different values for $X$.
    
    Once a function $f()$ has been estimated on a given data set we can use that function to predict the value of Y for different $X_1 , ... ,X_p$. Predicting the value of $Y$ new $X$ values can be simple when the error term averages to zero. The value of Y can computed with the following equation:
    
    \begin{equation}
        \widehat{Y} = \widehat{f}(X)
    \end{equation}
    
    The wide hat notation stands for an approximation of an unknown. $\widehat{f}$ is the estimated function for the true, unknown relationship $f()$ and $\widehat{Y}$ is an estimation of $Y$. The accuracy of our predicted $\widehat{Y}$ depends on two quantities: the reducible error and the irreducible error. The reducible error yields from the approximating nature of $\widehat{f()}$. The choice of $\widehat{f}$ causes the reducible error. A different approximation can change the reducible error. However, some of the error will always be unexplained and inherent in the measurements taken. This implicit error in our measurement can not be reduced and is thus called the irreducible error. \cite{hastie} formally describes the two types of error as
    \begin{equation}
        E(Y-\widehat{Y}) = E[f(X)+\epsilon - \widehat{f}(X)]^2 = [f(X)-\widehat{f}(X)] + Var(\epsilon)
    \end{equation}
    
    With $Var(\epsilon)$ being the irreducible error and $[f(X)-\widehat{f}(X)]$ the reducible part. Estimating $f$ to reduce the reducible error stands at the heart of every method introduced in this thesis. However, the irreducible error can not be neglected from discussion. The irreducible error is only assumed to be zero mean sum on the easiest of models, an important feature of models introduced later in this thesis is the assumption that the irreducible is in fact not zero sum average. 

    
    While prediction is focused on looking forward, inference is somewhat focused on looking backwards. Inference is the analytical perspective of statistical learning. Inference focuses on analysing the estimated $\widehat{f}$. Examining the respective contribution of each $X_1, ... , X_p$ to $Y$ can give qualitative insight into finding the best predictors $Y$ our of many. Returning to the previous example of budget spend on a media outlet. In \ref{fig:sales} the number of sales responds differently to additional spend for each media category. Comparing the different $X$s we see that TV is by far the most efficient place to spend the budget. This simple analysis illustrates the power of inference to understand the underlying system. Understanding the basic nature of $X$'s influence on $Y$ is a key aspect of inference. Does $Y$ in- or decrease with a change in $X$ and if so by how much.  Inference further examines the fit of the function, \ref{fig:sales} assumes a linear relationship between each $X$ and $Y$, while this tread works well for TV it fails to capture the nature of the Newspaper category. A linear relationship between sales and Newspaper seems unreasonable after examination.   
    

     
    \subsection{How to estimate f()?}
    There are many approaches to estimate an unknown function given data points. \cite{hastie} states that all share certain characteristics. First all methods need training set of data. This training set contains tuples of dependant and independent variables. This set of tuples is used as input for an algorithm that approximates function $\widehat{f}()$. Methods for approximating $\widehat{f}()$ can broadly be categories into two categories: parametric and non-parametric.
    
    Parametric models assume a specific shape of the true underlying function $f()$. This assumptions manifests itself in the two step approach to to parametric models. First, make an assumption about the functional form or shape of $f$. The simples assumption and subject of the next chapter is the linear model. A linear assumption results in the following form:
    
    \begin{equation}
        f(X) = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p
    \end{equation}
    
    The function f is linear in the influence of the parameter $\beta$. The linear assumption poses tight restrictions of the shape $f$ but also on space of potential functions. While a non-parametric function implies a search in an arbitrary p-dimensional function space the linear assumption limits the search to p+1 coefficients, $\beta_0,\beta_1, ... ,\beta_p$ in this example. 
    
    After deciding on shape for $f()$ the function needs to be estimated. The process of actually finding $f$ is also referred to as training or fitting a function. In this example this means fitting means to find the coefficients $\beta_0,\beta_1, ... ,\beta_p$. Thus the full problem of fitting a linear model becomes:
    
    \begin{equation}
        Y \approx \beta_0 + \beta_1 X_1 + ... + \beta_p X_p
    \end{equation}
    
    
    The shape decided in step one does not imply a method of actually finding the coefficients though there are best practices associated with each. The linear assumption in this example is usually fit with a least squares method, which can be computer with different methods. The choice of a parametric form solely dictated the nature and number of the parameters involved, thus the name: parametric.
    
    Choosing a functional shape for $f()$ drastically simplifies the fitting process by predetermining the amount of coefficients to be estimated. However, the chosen shape may enforce conditions that are very different to the original function $f$. A poor choice of the parametric form will directly translate into a poor estimate. This problem can be addressed with more flexible methods that allow for a more parameters. The higher number of parameters has to be carefully weighed with a phenomena called over fitting. Over fitting is the result of giving a functional form too many parameters, causing the approximation to follow the error too closely. A major benefit of parametric methods is the ability to interpret the results. Assuming a functional shape usually yields from information about the function that is not in the data itself. Accounting for outside information though a choice of parametric shape allows for an easy interpret ability. The choice of form enforces a certain model of the data I want to examine, the result will be within the shape and thus are interpretable through the model.
    
    \cite{hastie} describes the non-parametric approach as the complement to the parametric methods. Non-parametric methods do not assume the functional form of $f$ before estimating it. This brings a major advantage over parametric methods since they allows $f$ to follow the data as closely as possible without dealing with a determined shape. Allowing $f$ to take any arbitrary form is a large advantage and offers much more flexibility then parametric methods. While non-parametric methods avoid the pitfall of a bad functional shape they have to search the whole possible space of f. Searching an arbitrary functional space is very time and space intensive. Non-parametric methods provide much more flexibility than parametric methods and are usually preferred for prediction. Their arbitrary shape of the results makes the interpretation of the result harder to understand and are a clear disadvantages over parametric methods.
    
    The method central to this thesis follows a progression through the parametric, non-parametric framework. The simple linear model will be the first model introduced and is a parametric method. The linear model assumes a linear relationship between $X$ and $Y$ which makes it the easiest to estimate and interpret. Enforcing very strict limitations on functional form, only allowing a linear relationship with a normal distributed response variable and a zero mean error term. The generalized linear model extends the linear model by easing the restriction on the response variable and allowing for a varying error term. Even though the restrictions on the functional shape are eased in the generalized linear model they are still a parametric methods with all the advantages and disadvantages. Generalized additive models stand at the heart of the thesis and present an interesting extension of the generalized linear model. Generalized additive models allow for a non-parametric method called smoothing splines to be used as part of generalized linear models. Splines are a highly flexible non-parametric method that allow to fit arbitrary functions. Generalized additive models offer much greater flexibility than generalized linear models but rely on similar estimation technique and theory. This progression with an implementation of generalized additive models in a cluster environment will be the central piece of this thesis.
    
    
    \subsection{Statistical Computing Environment: R}
        
    R is a free software environment for statistical computing and graphics. R is designed to express statistical models and comes with an development environment that is build to illustrate and discover data. Academia has recognized R as it's language and continuously published learning material and research alongside R code. \cite{hastie} and \cite{gamBook} employ R to illustrate statistical concepts in a computational environment. I will do the same and use R to express statistical concepts and example throughout this thesis. I will now introduce some syntax for matrix manipulation in R:
    
  <<chunk1>>=
    # A vector is the essential data type in R, R uses row vectors
    vector = c(1,2,3,4)
    
    # Indexes start at 1
    vector[1]
    
    # Create a matrix with n rows m columns filled with random values
    n = 5
    m = 3
    X = matrix(runif(15),n,m)

    # Display the generated matrix
    X
    # Transpose the matrix
    t(X)
    
    # Multiply a matrix with its transpose
    X %*% t(X)
    
    # Basic Indexing
    X[1,2]
    
    # Extract subset of a matrix
    X[1:3]
    
    # Assign values to subset of a matrix
    X[1:3] = 2
    X
    
    # Extract column of a matrix, implicit conversion to a row vector
    X[,2]
    
    # Assign values to a column of a matrix
    X[,2] = 5
    X
    
    # Add a column to a matrix
    cbind(X,c(1,2,3,4,5))
    
    # Add a row to a matrix
    rbind(X,c(1,2,3))
    @
        
    
    
    \section{Linear Models}
       I will examine the linear and generalized linear model in this section. The linear model is the an elementary member of statistical models and enforces a strict parametric form on the estimated function. Though linear models have limited real world applications they introduce many relevant ideas and concept for the following methods. The generalized linear model is the generalized version of the linear model and relaxes some restrictions set by the linear model. Both models are essential for understanding the generalized additive model which is the main subject of this thesis. Generalized additive models are an extension to the generalized linear model. For both models I will first introduce the relevant theory for the estimated function then explain how this function is estimated and conclude with an model example. 
        
        \subsection{Simple Linear Model}
        
        The linear model, also linear regression, is a simple but interesting method central to many other more sophisticated methods. The simple linear model is the simplest linear model. A simple linear model explains a normal distributed response variable to be explained in terms of one explanatory variable. \cite{gamBook} describes the theory for a linear model with one explanatory variables as following: Given $n$ observations of $x_i$ and $y_i$. Where $y_i$ is the observations of a random variable $Y_i$ with expectation $\mu_i \equiv E(Y_i)$. A linear model assumes a model of the following parametric form:
        
        \begin{equation}
            Y_i = x_i \beta + \epsilon_i \ where \mu_i = x_i \beta
        \end{equation}
      
        With $\beta$ being an unknown parameter that is to be estimated and $\epsilon_i$ error term for that row. A linear model assumes that $\epsilon_i$ are mutually independent zero mean random variables with the same variance $\sigma^2$. The linear model explains the response variable $Y$ in terms of a predictor variable $x$  multiplied by an estimated coefficient $\beta$ plus a random error term $\epsilon$. 
        
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{simple_linear_model}
            \caption{\cite{gamBook} visually described all relevant variables and their relationship}
            \label{fig:simple_linear_model}
        \end{figure}
        
        
        $y_i$ and $x_i$ are known but $\beta$ is unknown and thus needs to be estimated. The simple model linear model gives one approach to estimate $\beta$ from the given data $x_i$,$y_i$. The simple linear model seeks to find the $\beta$ that minimizes the squared difference between $y_i$ and $x_i \beta$. Formalizing this notion amounts to defining the least squares measure  $S$ as:
        
        \begin{equation} 
        \mathbf{S} =  \sum_{i=1}^{n}(y\textsubscript{i} - x\textsubscript{i}\beta)^{2} = \sum_{i=1}^{n}(y\textsubscript{i} - \mu\textsubscript{i})^{2} 
        \end{equation}
        Per this definition a good choice of $\beta$ minimizes the difference between $y_i$ and $\mu_i$. As $S$ converges $0$ the better of an estimated $\beta$  becomes . The problem of least square estimation becomes minimizing $S$ with respect to $\beta$. To minimize S, differentiate with respect to (w.r.t.) $\beta$.
        
        \begin{equation} \label{partialDer} \frac{\partial S}{\partial \beta} = - \sum_{i=1}^{n} 2x_i(y_i-x_i\beta) \end{equation}

       By setting the result to $0$ the best estimate for $\beta$ can be found. The best estimate for $\beta$ is called $\widehat{\beta}$.  After setting the equation to zero and rearranging term to express $\widehat{\beta}$ in terms of $x_i$ and $y_i$ the full problem of estimating $beta$ can be stated as:

        \begin{equation}  - \sum_{i=1}^{n} 2x_i(y_i-x_i\widehat{\beta}) = 0  \end{equation}

        \begin{equation} - \sum_{i=1}^{n} x_i y_i-\widehat{\beta} \sum_{i=1}^{n} x_i^2 = 0  \end{equation}

        \begin{equation} \widehat{\beta} = \sum_{i=1}^{n} x_i y_i /\sum_{i=1}^{n} x_i^2 \end{equation}


        Minimizing $S$ w.r.t. $\beta$ to compute $\widehat{\beta}$ is a reasonable approach when dealing with one explanatory variable. The Gauss-Markov theorem shows that estimating $\beta$ with $\widehat{\beta}$ is best linear unbiased estimator. However, most relevant linear models involve more explanatory variables than one. The question for finding must be account for multiple explanatory variables but the measure of fit $S$ remains.
      
        \subsection{Linear Model}
         \cite{gamBook} introduces the linear model as the generalization of the simple linear model, allowing for the response variable to be explained with multiple predictor variables. Though the linear model generalizes the simple linear model it still assumes a normal distributed $Y$
        \subsubsection{Linear Model in the vector-matrix form}
        Allowing for several predictor variables amounts to rewriting the simple linear model in terms of vectors and matrices. A major benefit of using the matrix-vector forms is that the problem of finding $\beta$ becomes the problem of solving a overdetermined system of equations. Again, given n observations of $y_i$ and $x_i$ plus some additive constant. Explicitly writing each $\mu_i = x_i \beta_i$ illustrates the shape of the system of equations.
        \begin{equation}
        \begin{align*} 
        \begin{matrix}
            \mu_1 = \beta_0 + x_1 \beta_1 \\
            \mu_2 = \beta_0 + x_2 \beta_1 \\
            ... \\
            \mu_n = \beta_0 + x_n \beta_1 \\
        \end{matrix}
        \end{align*}     
        \end{equation}
        
        For the linear model this system of equations must now be rewritten in matrix form resulting in the matrix-vector form.  The matrix-vector form for the simple linear model takes the following shape. 
        \begin{equation}
       \left[ \begin{array}{c} \mu_1 \\ \mu_2 \\ \mu_3 \\ ... \\  \mu_n\end{array} \right] = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ 1 & x_3 \\ ... \\1 & x_n   \end{bmatrix}  \left[ \begin{array}{c} \beta_0 \\ \beta_1 \end{array} \right]   
        \end{equation}
        
        Now, adding predictor variables and generalizing the simple linear model amounts to appending a predictor variable vector to the matrix and introducing a new coefficient to the coefficient vector.
        
        \begin{equation}
           \left[ \begin{array}{c} \mu_1 \\ \mu_2 \\ \mu_3 \\ ... \\  \mu_n\end{array} \right] = \begin{bmatrix} 1 & x_1 & x_2\\ 1 & x_2 & x_2 \\ 1 & x_3 & x_3 \\ ... \\1 & x_n & x_n   \end{bmatrix}  \left[ \begin{array}{c} \beta_0 \\ \beta_1 \\ \beta_2 \end{array} \right]   
        \end{equation}
    
        The matrix-vector form yields the general form the linear model $\mu = X\beta$. This means that the value vector $\mu$ is given by the model matrix $X$ times the unknown parameter vector $\beta$. The model matrix can account for an arbitrary amount of predictor variables by adding a column for each predictor. The nature of the predictor variable: continuous or discrete plays an important role in the construction of the model matrix. Dummy and factor encoding are the two approaches when dealing with factor variables. For an thorough text on factor encoding in linear models see \cite{regression}. The model matrix is also known as design matrix and the terms are used interchangeably. $\mu = X\beta$ is the canonical form of a linear model. The method for estimating $\beta$ for one predictor do not work for the matrix-vector form and need to adjusted for estimating the unknown vector $\beta$. 
        
        \subsubsection{Estimating Linear Models}
        \cite{gamBook} states estimating the vector of unknown parameters as finding the least squares solution to $\beta$. The computationally stablest and fastest method involves QR decomposing the model matrix to express $\widehat{\beta}$ in terms of the upper triangular matrix. This method is widely used in statistical packages and relevant for the numerically optimized version of  generalized additive model. This approach involves stats with the linear model in the full matrix-vector form:
         
         \begin{equation}
             \mu = X \beta, y \approx N(\mu, I_n, \sigma^2)
         \end{equation}
        
        The model matrix $X$ is a matrix with n rows and p columns. With n being the number of observations and p is a product of predictors and their encoding. The least squares estimation for $\beta$ is the best linear unbiased estimator with least variance, \cite{gamBook} provides a full proof. Estimating $\beta$ relies on the Euclidean length of a vector. The Euclidean length of a vector is the sum squared of its elements. For a vector v in n dimensional space the Euclidean length is defined as:
        
        \begin{equation}
            \left \| v \right \|^2 \equiv v^T v \equiv \sum_{n}^{i=1} v^2
        \end{equation}
        
        An essential fact about the Euclidean length is that the rotation a matrix does not change its length. The practical method for fitting least squares heavily leverages this fact. This also holds for the rotation of $y-X\beta$. Bootstrapping the Euclidean length to define our measure $S$ in the matrix-vector framework yields
        
        \begin{equation}
           \label{S} S =  \left \| y - \mu \right \|^2 = \| y = X\beta \|^2 
        \end{equation}
        
        \cite{gamBook} describes QR decomposition of a matrix as the essential method for estimating $\beta$ and also an essential method for finding generalized additive models. Any real matrix X can always decomposed into an orthogonal matrix and triangular matrix.
        
        \begin{equation}
        X = Q\begin{bmatrix}R \\ 0 \end{bmatrix} = Q_f R
        \end{equation}
        
        $R$ is the upper triangular matrix with p rows and p columns and $Q$ is an orthogonal matrix with n rows and n columns of which the first p columns form $Q_f$. By definition does multiplying a vector with an orthogonal matrix not change the their length. The matrix algebra appendix provide more detail on these concepts. The QR decomposition decomposes a matrix into two district matrices that have different properties but maintain their length. Multiplying an orthogonal matrix with $S$ does not change the length. Applying the QR decomposition of the model matrix in \ref{S} yields in:
        
        \begin{equation}
             \left \| y - X\beta^2\right \|  =  \left \| Q^Ty - Q^T X\beta^2\right  \| =  \| Q^T-y \begin{bmatrix} R \\ 0 \end{bmatrix} \beta \|^2
        \end{equation}
        
        Only the orthogonal matrix of the QR decomposed model matrix gets multiplies to the the response variable vector and the original model matrix. Multiplying $Q^T$ with the response vector can be stated as $Q^T y = \begin{bmatrix} f \\ r \end{bmatrix}$. Where $f$ is vector of p dimensions and hence r is a vector of  $n-p$ dimension.
        
        \begin{equation}
            \left \| y - X\beta\right \|^2  =  \left \| \begin{bmatrix} f \\ r \end{bmatrix} - \begin{bmatrix} R \\ 0 \end{bmatrix}\beta\right \|^2 =  \| f-R \beta \|^2 + \|r \|^2
        \end{equation}
        
        This complicated form exposes the residual error $r$ as independent of $\beta$. $\| f - R \beta \|^2$ can be reduced to zero by choosing $\beta$ so that $R\beta$ equals $f$, thus the estimator $\widehat{\beta}$ can be stated as 
        
        \begin{equation}
            \widehat{\beta} = R^{-1} f
        \end{equation}
        
        The reducible error, also called residual error is the difference between the model matrix multiplies with the estimated $\beta$ minus the value for y. $\|r\|^2 = \|y-X \widehat{\beta\x}\|^2$. 
        
    \subsubsection{Influence Matrix}
    The influence matrix or hat matrix is required for the prediction piece of linear models and plays an important role for generalized additive models. \cite{gamBook} introduces the influence matrix as the matrix that yields the predicted fitted value vector $\widehat{\mu}$. To obtain  $\widehat{\mu} $the hat matrix is post multiplied with the data vector $y$. The influence matrix is required to compute the estimated value vector. The first p columns of Q make up the matrix $Q_f$. $f$ is then defined as $f = Q_f y$ which implies that 
    
    \begin{equation}
        \widehat{\beta} = R^-1 Q_f^T y 
    \end{equation}
    
    Given that $\widehat{\mu} = X \widehat{\beta}$ and $X = Q_fR$ the fitted value vector can be written as 
    \begin{equation}
        \widehat{\mu} = Q_f RR^-1 y = Q_fQ_f^T_f y  
    \end{equation}
    Since the upper triangular matrix $R$ is multiplied its inverse we can factor them out resulting with the hat matrix $A \equiv Q_f Q_f^T$ that fulfills $\widehat{\mu} = Ay$.
    
    \subsubsection{Simple linear model example}
        
        
    \subsection{Generalized Linear Model}
    The generalized linear model (GLMs) is an extension to the general linear model, allowing the response variable $Y$ to be of any exponential family distributions. The exponential family of distributions contains many practical useful distributions including Poisson, bionomial, gamma and normal distributions. A formal description of it's basis structure can be given as:

        \begin{equation} g(\mu_i) = X_i \beta_i \end{equation}


   $X_i$ is the $i^th$ row of a model matrix X and $\beta$ is a vector of unknowns parameters with $\mu_i \equiv E(Y_i)$ and $Y_i$ is distributed according to some exponential family. Every exponential family distribution has a canonical link function $g()$. The link function allows to estimate the linear functions of the predictor variables by transforming the right side of the equation $f(x)$. The data are then fit in this transformed scale but the expected variance is calculated on the original scale of the predictor variables. The link functions is essential in modelling exponential functions in linear terms. Fitting several distributions with a single framework at a cost: While the least squares approach was sufficient for estimating $\beta$ for normal distributed data with zero mean error the generalized estimation method has to account for an arbitrary amount of distribution parameters. \cite{glm} introduces iterative reweighted least squares (IRLS) as a method to obtain maximum likelihood estimates (MLE) for a particular exponential family distribution. IRLS is the generalization of the least squares approach from normal distribution to any exponential family distribution. A practical feature of GLMs is that they can all be fitted using IRLS, independent of response variable distribution. 
 
    \subsubsection{Maximum-Likelihood Estimation}
    
    \cite{glm} describes the principle of maximum likelihood estimation as searching for probability distribution parameters that makes the observed data most likely. This results in a search for the parameter vector that maximizes the likelihood function $L(\beta|y)$. The parameter vector is found by searching the multi-dimensional parameter space.

    \cite{MLE} describes MLE as finding the parameters for a distribution that were most likely to produce the given data. Random variable are defined in terms of one or several parameters. A good choice of parameters results in a distribution that emulates the given data. The least squares linear model estimated the $\mu$ and $\sigma$ for $N( \mu ,\sigma^2)$ by minimizing $S$ is a special case of the MLE, only allowing for normal distributed data. \cite{glm} introduces a method to account estimate parameters for any exponential family distributions members. Exponential family distributions have arbitrary number of parameters. Examle for these parameters are the Poisson distribution  $P(\lambda)$  and gamma distribution $G(K,\theta)$. Maximum likelihood estimation provides a single framework to allow parameter estimation for any of the exponential family distributions.

    From a statistical analysis point of view the vector $y$ of observed data is a random sample from an unknown population. The goal of maximum likelihood estimation is to find the parameters of the given distribution that most likely have produced this sample. This process is described with a probability density function (PDF) $f()$ of observed data $y$ given a parameter $\beta$: $f(y|\beta)$. If individual observations, $y_i$’s, are statistically independent of one another, the PDF for the data $y$ given the parameter vector $\beta$ can be expressed as a multiplication of PDFs for individual observations.

    \begin{equation} f(y|\beta) = f((y_1, y_2,...,y_n) | (\beta_1,\beta_2, ... \beta_n )) = \prod_{i=1}^{n} f_i(y_i|\beta_i) ) \end{equation}

    Given a set of parameter values, the corresponding PDF will show that some data are more probable than other data.  However, the data is already given and we are searching for the parameters of the distribution that most likely produced the data. The casual term most likely is referring to maximizing likelihood. We thus inverse our function $f(y|\beta)$ to $L(\beta|y)$ to produce the likelihood of $y$ given the parameters $\beta$. Theoretically MLE need not exist nor be unique. But if they exist and are unique they can be estimated. For computational convenience, the MLE estimate is obtained by maximizing the log-likelihood function, $ln(L(\beta|y))$: This is because the two functions, $ln(L(\beta|y))$ and $L(w|y)$; are monotonically related to each other so the same MLE estimate is obtained by maximizing either one and the log-likelihood is preferred for obvious reasons. Assuming that the log-likelihood function, $ln(L(\beta|y))$ is differentiable, if $\beta$ exists, it must satisfy the following partial differential equation known as the likelihood equation:

    \begin{equation} \frac{\partial ln L(\beta|y)}{\partial \beta_i} = 0 \end{equation}

    This properties are given because the definition of maximum or minimum of a continuous differentiable function implies that its first derivatives vanish at such points. The likelihood equation represents a necessary condition for the existence of an MLE estimate. An additional condition must also be satisfied to ensure that $ln(L(\beta|y))$ is a maximum and not a minimum, since the first derivative cannot reveal this. The log likelihood must be convex near w. This is verified by calculating the second derivatives of the log-likelihoods and checking if they are negative.

    \begin{equation} \frac{\partial^2 ln L(\beta|y)}{\partial \beta^2_i} < 0 \end{equation}

    This form is only of theoretical value as practical model often involves many parameters and have highly non-linear PDFs. Thus the estimate must be sought numerically using nonlinear optimization methods. The basic idea for these methods is to find optimal parameters that maximize the log-likelihood focusing on smaller sub-sets of the multi-dimensional parameter space. This is the preferred because exhaustively searching the whole parameter space becomes intractable with an increasing amount of parameters. The practical method for MLE searches by trial and error over the course of a series of iterative steps. Each iteration changes the results from the previous iteration by a small value. The choice of the value is tailored to improve the log-likelihood. \cite{glm} introduces a metthod called iterative reweighted least squares and is subject in the next section.
        
        \subsubsection{Fitting Generalized Linear Models}
    
    \cite{gamBook} introduces iterative reweighted least squares (IRLS) as the practical method for fitting GLMs. The method method must find the distribution parameters and account for unequal variance. GLM models an n-vector of independent response variables $Y$, where $\mu$ is the expected value of $Y$ as:
    
    \begin{equation}
        g(\mu_i) = X_i \beta
    \end{equation}
    
    With $\mu \equiv E(Y)$ and the link transformation of $Y_i$ as $Y_i \approx f_\theta_i (y_i)$ where $f_\theta_i$ stands for the canonical link transformation of an exponential family distribution. The value of $mu_i$ is the to determined by finding $\beta$. With this in mind the probability mass function for any exponential family distribution can be written as:
    
    \begin{equation}
        f_\theta = exp[\{y\theta-b(\theta)\}/a(\phi)+c(y, \phi)]
    \end{equation}
    
    $a,b$ and $c$ are arbitrary functions, $\phi$ an arbitrary scale parameter and the $\theta$ the canonical link parameters. The interesting property of this form is that the general expression for mean and variance of exponential family distributions can be expressed in terms of $a,b$ and $\phi$. The mean and the variance are the required parameters to fit any exponential family distribution and are computed via deriving the log-likelihood. Since the natural logarithm is the inverse of the exponential function the log-likelihood has a familiar form.
    
    \begin{equation}
        l(\theta) = [y\theta - b(\theta)] / a(\phi) + c(y,\phi) 
    \end{equation}
    
    $E(Y)$ can now computing by differentiating $l$ w.r.t. $\theta$ and treating $l$ as a random variable. This amounts to replacing the particular value $y$ with the random variable $Y$.
    
    \begin{equation}
        \frac{\partial l }{\partial \theta} = [y-{b}'(\theta))]/a(\phi))    
    \end{equation}
    
    \begin{equation}
        E(\frac{\partial l }{\partial \theta}) = [E(Y)-{b}'(\theta))]/a(\phi))
    \end{equation}
    
    Given that $E(\partial l / \partial \theta) = 0$ the expected value of the random variable Y can be stated as:
    
    \begin{equation}
        E(Y) = {b}'(\theta)
    \end{equation}
    
    This results in a method to compute the mean of any exponential family random variable by the first derivative of b w.r.t $\theta$. This the direct link between the $\beta$ and the model parameter. Allowing the parameter $\beta$ to determine the mean of the response variable and consequentially the canonical parameter is the proper way to find that value. The variance of that random variable can be computed from the second derivative of the log-likelihood:
    
    \begin{equation}
        \frac{\partial^2 l }{\partial^2 \theta} = -{b}''(\theta)/a(\phi)
    \end{equation}
    
    \cite{gamBook} states that re-arranging this formula yields the equation for the variance of $Y$.
    
    \begin{equation}
        var(Y) = {b}''(\theta) a(\phi)
    \end{equation}
    
    A very important feature of GLMs is that the variance can vary. To account for varying variance the variance is divided by a weight $w$ that penalizes data points with high relative variance.
    
    \begin{equation}
        var(Y) = {b}''(\theta) a(\phi)/w
    \end{equation}
    
    For simplicity the function $V(\mu)$ is defined as $V(\mu) = {b}''(\theta)$ such that $var(Y) = V\(\mu)$.
    
    Given vector $y$ of an observation random variable $Y$ the maximum likelihood estimation of $\beta$ is possible due to the independence of all $Y_i$. The formal notation for the MLE of the likelihood of $\beta$, $L(\beta)$ becomes
    
    \begin{equation}
        L(\beta) \prod_{i=1}^{n} f_\theta_i (y_i)
    \end{equation}
    
    Given the previously stated the probability mass function for exponential family members the log likelihood $l()$ of $\beta$ can be written as:
    
    \begin{equation}
        l(\beta) = \sum_{i=1}^{n} log[f_\theta_i(y_i))] = \sum_{i=1}^{n} (y_i\theta_i-b_i(\theta_i)) /a_i(\phi)+c_i(y_i, \phi)
    \end{equation}
    
    The log function transforms the product to a sum. $\phi$ is assumed to be constant for all $i$ and \cite{gamBook} states that only choices for $\phi$ are relevant that can be stated as $a_i(\phi) = \phi / w_i$ with $w_i$ being a constant. This assumption allows us the rewrite the previous formula as
    
    \begin{equation}
        l(\beta) = \sum_{i=1}^{n}\ w_i(y_i\theta_i-b_i(\theta_i)\)/a_i(\phi)+c_i(y_i, \phi)
    \end{equation}
    
    The process of finding the parameter vector $\beta$ amounts to maximizing the log-likelihood by partially differentiating $l$ w.r.t. each element of $\beta$, setting the resulting expressions to zero and solving for $\beta$.
    
    
    \begin{equation}
    \frac{\partial l }{\partial \beta_j} = 
     \sum_{i=1}^{n} w_i \begin{pmatrix} y_i \frac{\partial \theta_i }{\partial \beta_j} - {b}'(\theta_i) \frac{\partial \theta_i }{\partial \beta_j} \end{pmatrix}
    \end{equation}
    
    \cite{gamBook} details that applying the chain rule,  $E(Y) = {b}'(\theta)$ and term substitution the following form can be produced:
    
    \begin{equation}
        \frac{\partial l }{\partial \beta_j} = \frac{1}{\phi} \sum_{i=1}^{n} \frac{[y_i - {b_i}'(\theta_i)]}{b''_i(\theta_i)/w_i} \frac{\partial \mu_i }{\partial \beta_j}
    \end{equation}
    
    Applying the framework developed above to determine variance and expected value the full form for the estimation of $\beta$ can be stated as:
    
    \begin{equation}
         \sum_{i=1}^{n} \frac{(y_i - \mu_i)^2}{V(\mu_i)} \frac{\partial \mu_i }{\partial \beta_j} = 0 \ \forall \ j.
    \end{equation}
    
    This matches exactly the same equation that would have to be solved to find $\beta$ by non-linear weighted least squares if the weights for $(V(\mu_i)$ were known in advance and were independent of $\beta$. In that scenario the least square objective would be
    
      \begin{equation}
          S = \sum_{i=1}^{n} \frac{(y_i - \mu_i)^2}{V(\mu_i)}
      \end{equation}
    
    In the formulation $\mu_i$ depends non-linearly on $\beta$ but the weights $V(\mu)$ are treated as fixed. To find the least squares estimated $\partial S / \partial \beta_j$ must equal to zero for all js. Formulating the search for $\beta$ as finding the optimal choice of distribution parameters of $Y$ invites an iterative approach that chooses some parameter values and e .
    
    \subsubsection{Iterative Reweighted Least Squares}
    \cite{gamBook} describes this process formally to introduce the iterative reweighted least squares algorithm. Let $\widehat{\beta}^{[k]}$ be the estimated parameter vector at the $k^{th}$ iteration. $\eta^{[k]}$ is a vector with the elements $\eta^{[k]}=X_i \widehat{\beta}^{[k]}$ and $\mu_{i}^{[k]}}$ is defined as inverse of the link function. $\mu_{i}^{[k]} = g^{-1}(\eta_{i}^{[k]}$. Given these definitions the algorithm can be stated.
    
    \begin{enumerate}
   \item Compute the variance weighted variance $V(\mu_{i}^{[k]})$ terms implied by the current estimate for $\widehat{\beta}^{[k]}$
   \item Use these estimates and apply the method describe to minimize the least square objective with respect to $\beta$ to obtain $\widehat{\beta}^{[k]}$
   \item Set k to k+1
   \end{enumerate}
    
    To come to a computational formulation the least square objective for IRLS can be written as 
    
    \begin{equation}
        S = \left \| \sqrt{W}^{[k]}  \left ( z^{[k]} - X\beta \right )  \right \|^2
    \end{equation}
    
    Where $z^{[k]$ is so called "pseudodata" and $W}^{[k]]$  the diagonal weight matrix, each defined as
    
    \begin{equation}
        z_{i}^{[k]} = {g}'(\mu^{[k]})(y_i - \mu_i^{[k]} + \eta_i^{[k]}
    \end{equation}
    
    \begin{equation}
        W_{ii}^{[k]} = \frac{1} { V(\mu_{i}^{[k]}) \mu_{i}^{[k]}}
    \end{equation}

    With these definition we can finally write the full form of the practically used IRLS.
    \begin{enumerate}
        \item Use current $\eta^{[k]}$ and $\mu^{[k]}$ to calculate calculate pseudo data $z^{[k]}$ and iterative weights for the weights matrix $W^{[k]}$
        \item Minimize the least squares objective $\left \| \sqrt{W}^{[k]}  \left ( z^{[k]} - X\beta \right )  \right \|^2$ w.r.t. $\beta$ to obtain  $\widehat{\beta}^{[k+1]}$ and the resulting $\eta^{[k+1]}=X_i \widehat{\beta}^{[k+1]}$ and $\mu_{i}^{[k+1]}}$
        \item Set k to k + 1 
    \end{enumerate}
        
        The proposed method will converge on the optimal parameter vector $\widehat{\beta}$
        \subsubsection{Generalized Linear Model Example}
    
    \section{Generalized Additive Models}
    Generalized additive model (GAM) allow predictor variables of GLM to be estimated with non-parametric methods. GAM has the interpretability advantages of GLMs where the contribution of each predictor variable to the function is clearly encoded. However, it has substantially more flexibility because the relationships between predictor and response variable are not assumed to be linear. GAMs assume the relationship between predictor in response variable to be the sum of functions. These functions are regularized by enforcing a smooth patter. These smooth functions can be estimated with a scatter plot smoother. \cite{gam} introduced GAMs in 1990 and described many approaches to scatter plot smoothing. For practicability and applicability this thesis will focus on regression splines, especially cubic splines and cyclic cubic splines. \cite{gamBook} formally describe GAMs as:
    
    \begin{equation}
        g(\mu_i) = X_i^* \theta + f_1(x_1i) + f_2(x_2i) + ... + f_3(x_3i)
    \end{equation}
    
    GAM inherit the link function $g()$, parameter vector $\theta$ and the model matrix $X_i^*$ from the GLM. Following the GLM definition $\mu \equiv E(Y_i)$ and $Y$ is some exponential family distribution. $X_i^*$ is the ith row of the model matrix. GAM introduce smooth functions $f_j$ over the the predictor variables $x_k$. Specifying a model in terms of an non-parametric, smooth functions allow $x_k$ to have an arbitrary pattern. Allowing $f_j(x_k)$ to follow any shape can give insight into response variable behavior that the parametric form GLMs fail to capture. The gained flexibility comes with the question of how to find these smooth functions.
    
    \subsection{Smooth functions estimation with regression splines}
    Smooth functions are any function that explains $x_i$ in terms of $y_i$ with some error $\epsilon$. The error term  is a random variable that is independent, identically distributed with $N(0, \sigma^2)$. This is identical to the definition of non-parametric methods from the first section of this thesis. Formally we are searching for function $f()$ that satisfies:
    
    \begin{equation}
        y_i = f(x_i) + \epsilon_i
    \end{equation}
    
    Bootstraping the methods used for fitting a simple linear model we assume the the function $f$ to be linear in $x_i$. This assumption is guarantees that the function can be found with linear parametric methods. \cite{gamBook}
            
            \subsubsection{Regression Splines}
            \subsubsection{Basis Functions}
            Haar form
            \subsubsection{Cubic Regression Splines}
            Haar form for b=4
            \subsubsection{Cyclic Cubic Regression Splines}
        \subsubsection{Fitting Generalized Additive Models}
        \subsection{Penalized Iterative Reweighted Least Squares}
        \subsection{Generalized Additive Model Example}
    
    \section{Numerical Optimizations For Large Data Sets}
    
    \section{Apache Spark}
        \subsection{General In Memory Cluster Computing Framework}
        \subsection{The Language Scala and Breeze}
        \subsection{Machine Learning Pipeline}
    
    \subsection{Implementing Generalized Additive Models in Apache Spark}
    
    \section{Matrix Algebra Appendix}
        \subsection{QR Decomposition}
        
    


    \newpage

    \bibliography{thesis}    % reference to thesis.bib

    \newpage

\end{document}
