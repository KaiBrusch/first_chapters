\documentclass{article}

% Latex Tutorial beginners: http://www.latex-tutorial.com/tutorials/beginners/
\newcommand{\comment}[1]{}

\newcommand{\note}[1]{{\tiny (#1)}}

% Packages explained - Adding more functions to LATEX http://www.latex-tutorial.com/tutorials/beginners/lesson-3/

\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{textgreek}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.3in]{geometry}

% fügt die Literatur auch zum Inhaltsverzeichnis hinzu (ist so nicht default)
\usepackage[nottoc,notlot,notlof,numbib]{tocbibind}

% Adding a bibliography http://www.latex-tutorial.com/tutorials/beginners/lesson-7/
\bibliographystyle{apalike}

\graphicspath{ {charts/} }
% Nested numbered enumerations, eg. 1.1, 1.2: http://tex.stackexchange.com/questions/78842/nested-enumeration-numbering
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}


\title{
    Fitting Generalized Additive Models for\\ very large data sets with Apache Spark \\[7pt]
    \large Chapter Excerpt
}
\date{Future}
\author{Kai Thomas Brusch}

\begin{document}

    \pagenumbering{arabic}

    \maketitle

    \paragraph{Summary}

    This document contains three introductory chapters for my bachelor thesis titled "Fitting General Additive Models for very large datasets with Apache Spark".

    \paragraph{Contact} \texttt{kai.brusch@gmail.com}

    \paragraph{Location} Hochschule für Angewandte Wissenschaften Hamburg
    \paragraph{Department} Dept. Informatik
    \paragraph{Examiner} Prof. Dr. Michael Köhler-Bußmeier
    \paragraph{Second examiner} Dipl.-Mathematiker Univ. Markus Schmaus

    \newpage

    \tableofcontents

    \newpage

    
        \section{Statistical Learning}
    
    The topic of statistical learning might be alien to most computer science undergraduates. Thus, this first section is dedicated to introducing the area along with its problems and naming conventions \cite{hastie} describes Statistical learning as a set of tools for modeling and understanding complex data sets. Recent developments in statistics and computer science have created an interesting field that empowers modern statistical  with powerful computational methods. Statistical learning gives us the ability to ask interesting questions and use to data to find answers. This thesis will focus on one method of the supervised learning branch of statistical learning. Supervised learning methods build a statistical model for predictions and estimations of an output variable based on one or many input variables. Statistical learning methods are a cornerstone of modern scientific research and have given powerful methods to physics, neuroscience, biology, economics and every other relevant science. The professional world has been raving about the potential of statistical learning giving birth terms such as: Data Science and Big Data.
    
        \subsection{Why statistical learning?}
    
    Ideas are sometimes best conveyed through example. \cite{hastie} illustrated the benefits of statistical learning on a simple yet illustrative example. In the example we assume the be working for as statistical consultant in charge of advising a client on how to improve the sales of a given product. The client provides us with a data set containing the sales of a given product across several markets along with the budged for the product in each market in three different media outlets: TV, radio and newspaper. The client is not able to directly impact the amount of sales but he has the ability to change amount spend on each media outlet. If we can understand the number of items sold as a function of spend on media we can suggest our client to adjust spending to increase units sold. In \ref{fig:sales} the sales are plotted on the Y axis and the budget for a media outlet on the X axis. \cite{hastie} uses this example to introduce very important terminology. The media budgets are the input variable and sales are the output variables. The input variables are commonly denoted with $X_i$, in example $X_1$ is the TV spend, $X_2$ the radio spend and $X_3$ the newspaper spend. The naming for input variables has become inconsistent the the terms predictor, independent variables and features can be used interchangeably. The output variable, in this example the sales, is also called the response or dependant variable and denoted with Y.
    
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{sales}
            \caption{\cite{hastie} illustrates the units sold as a function of spend on each of the advertisement channels. The blue line is a linear model describing the underlying relationship between units sold and spend on media outlet.}
            \label{fig:sales}
        \end{figure}
        
    To generalize our sales example we can state the essential question of statistical learning. Supposed we observe a quantitative response Y and several predictors $X_1, X_2 , ... X_i$ we are looking for a relationship $f()$ that describes the relationship in the following form:
    \begin{equation}
        Y = f(X) + \epsilon
    \end{equation}
    The function $f()$ is a fixed but unknown function of the input variables $X_1, ... X_p$. $\epsilon$ is random, identical, independent distributed error term with zero mean. Another way of framing this is to see the function $f()$ as the systematic information in the observations. All statistical learning methods are concerned with finding an optimal $f()$. This thesis will introduce three, related methods of finding $f()$. Each with different idea on how to find $f()$ and with a different interpretation what optimal means.


    \subsection{Why Estimate f()?}
    While the example in the previous section showed statistical learning as a powerful tool to discover a relationship between a dependent and independent variable we have not explained why do we are keen to find a function $f()$. \cite{hastie} argues two essential reasons for estimating $f()$: inference and prediction. Inference is concerned with understanding a system while prediction wants to use the system to predict $Y$ on different values for $X$.
    
    Once a function $f()$ has been estimated on a given data set we can use that function to predict the value of Y for different $X_1 , ... ,X_p$. Predicting the value of Y new X values can be simple when the error term averages to zero. The value of Y can computed with the following equation:
    
    \begin{equation}
        \widehat{Y} = \widehat{f}(X)
    \end{equation}
    
    The wide hat notation stands for an approximation of an unknown value. $\widehat{f}$ is the estimated function for the true, unknown relationship $f()$ and $\widehat{Y}$ is an estimation of Y. The accuracy of our predicted $\widehat{Y}$ depends on two quantities: the reducible error and the irreducible error. The reducible error yields from the approximating nature of $\widehat{f()}$. The choice of $\widehat{f}$ causes the reducible error, a different approximation can change the reducible error. However, some of the error will always be unexplained and inherent in the measurements taken. This implicit error in our measurement can not be reduced and is thus called the irreducible error. \cite{hastie} formally describes the two types of error as
    \begin{equation}
        E(Y-\widehat{Y}) = E[f(X)+\epsilon - \widehat{f}(X)]^2 = [f(X)-\widehat{f}(X)] + Var(\epsilon)
    \end{equation}
    
    With $Var(\epsilon)$ being the irreducible error and $[f(X)-\widehat{f}(X)]$ the reducible part. Estimating $f$ to reduce the reducible error stands at the heart of statistical learning, the irreducible error can not be neglected however. The irreducible error is only assumed to be zero mean sum on the easiest of models, an important feature of models introduced later in this thesis is the assumption that the irreducible is in fact not zero sum average. 

    
    While prediction is focused on the looking forward, inference is the analytical perspective of statistical learning. Inference focuses on analysing the estimated $\widehat{f}$. Examining the respective contribution of each $X_1, ... , X_p$ to $Y$ can give qualitative insight into finding the best predictors $Y$ our of many. Returning to the previous example of budget spend on a media outlet. In \ref{fig:sales} we see that the number of sales respond differently to additional spend for each media category. Comparing the different $X$s we see that TV is by far the most efficient place to spend the budget. This simple analysis illustrates the power of inference to understand the underlying system. Understanding the basic nature of $X$'s influence on $Y$ is a key aspect of inference, does $Y$ in- or decrease with a change in $X$.  Inference further examines the fit of the function, \ref{fig:sales} assumes a linear relationship between each $X$ and $Y$, while this tread works well for TV it fails to capture the nature of the Newspaper category. A linear relationship between sales and Newspaper seems unreasonable after examination.   
    

     
    \subsection{How to estimate f()?}
    There are many approaches to estimate an unknown function given data points. \cite{hastie} states that all share certain characteristics. First all methods need training set of data. This training set contains tuples of dependant and independent variables. This set of tuples is used as input for an algorithm that approximates function $\widehat{f}()$. The methods for finding $\widehat{f}()$ can broadly be categories into two categories: parametric and non-parametric.
    
    Parametric models assume a specific shape of the true underlying function $f()$. This assumptions manifests itself in the two step approach to to parametric models, first make an assumption about the functional form or shape of f. The simples assumption and subject of the next chapter is the linear model. A linear assumption results in the following form:
    
    \begin{equation}
        f(X) = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p
    \end{equation}
    
    The function f is linear in the influence of the parameter $\beta$. The linear assumption poses tight restrictions of the shape $f$ but also on space of potential functions. While a non-parametric function implies a search in an arbitrary p-dimensional function space the linear assumption limits the search to p+1 coefficients, $\beta_0,\beta_1, ... ,\beta_p$ in this example. 
    
    After deciding on shape for $f()$ the function needs to be estimated. The process of actually finding $f$ is also referred to as training or fitting a function. In this example this means fitting means to find the coefficients $\beta_0,\beta_1, ... ,\beta_p$. Thus the full problem of fitting a linear model becomes:
    
    \begin{equation}
        Y \approx \beta_0 + \beta_1 X_1 + ... + \beta_p X_p
    \end{equation}
    
    
    The shape decided in step one does not imply a method of actually finding the coefficients though there are best practices associated with each. The linear assumption in this example is mostly fit with a least squares method which again can be computer with different methods. So the choice of a parametric form solely dictated the nature and number of the parameters involved, thus the name: parametric.
    
    Choosing a functional shape for $f()$ drastically simplifies the fitting process by predetermining the amount of coefficients to be estimated. The chosen shape may however enforce conditions that are very different to the original function f. A poor choice of the parametric form will directly translate into a poor estimate. This problem can be addressed with more flexible methods that allow for a more parameters. The higher number of parameters has to be carefully weighed with a phenomena called over fitting. Over fitting is the result of giving a functional form too many parameters, causing the approximation to follow the error too closely. A major benefit of parametric methods is the ability to interpret the results. Assuming a functional shape usually yields from information about the function that is not in the data itself. Accounting for outside information though a choice of parametric shape allows for an easy interpret ability as the result must fit the picked framework.
    
    \cite{hastie} describes the non-parametric approach as the complement to the parametric methods. Non-parametric methods do not assume the functional form of $f$ before estimating it. This brings a major advantage over parametric methods since they allows $f$ to follow the data as closely as possible without dealing with a determined shape. Allowing $f$ to take any arbitrary form is a large advantage and offers much more flexibility then parametric methods. While non-parametric methods avoid the pitfall of  a bad functional shape they don't reduce the space of f by predetermining the number of coefficients. Non-parametric methods provide much more flexibility than parametric methods and are usually preferred for prediction. Their arbitrary results makes the interpretation of the result harder to understand and are a clear disadvantages over parametric methods.
    
    The method central to this thesis follows a progression through the parametric, non-parametric framework. The simple linear model will be the first model introduced and is a parametric method. The linear model assumes a linear relationship between $X$ and $Y$ which makes it the easiest to estimate and interpret. Enforcing very strict limitations on functional form, only allowing a linear relationship with a normal distributed response variable and a zero mean error term. The generalized linear model extends the linear model by easing the restriction on the response variable and allowing for a varying error term. Even though the restrictions on the functional shape are eased in the generalized linear model they are still a parametric methods with all the advantages and disadvantages. Generalized additive models stand at the heart of the thesis and present an interesting extension of the generalized linear model. Generalized additive models allow for a non-parametric method called splines to be used as part of generalized linear models. Splines are a highly flexible non-parametric method that allow to fit arbitrary functions. Generalized additive models offer much greater flexibility than generalized linear models but rely on similar estimation technique and theory. This progression with an implementation of generalized additive models in a cluster environment will be the central piece of this thesis.
    
    
    \subsection{Statistical Computing Environment: R}
        
    R is a free software environment for statistical computing and graphics. R is designed to express statistical models and comes with an development environment that is build to illustrate and discover data. Academia has recognized R as it's language and continuously published learning material and research alongside R code. \cite{hastie} and \cite{gamBook} employ R to illustrate statistical concepts in a computational environment. I will do the same and use R to express statistical concepts and example throughout this thesis. I will now introduce some syntax for matrix manipulation in R:
    
  <<chunk1>>=
    # A vector is the essential data type in R, R uses row vectors
    vector = c(1,2,3,4)
    
    # Indexes start at 1
    vector[1]
    
    # Create a matrix with n rows m columns filled with random values
    n = 5
    m = 3
    X = matrix(runif(15),n,m)

    # Display the generated matrix
    X
    # Transpose the matrix
    t(X)
    
    # Multiply a matrix with its transpose
    X %*% t(X)
    
    # Basic Indexing
    X[1,2]
    
    # Extract subset of a matrix
    X[1:3]
    
    # Assign values to subset of a matrix
    X[1:3] = 2
    X
    
    # Extract column of a matrix, implicit conversion to a row vector
    X[,2]
    
    # Assign values to a column of a matrix
    X[,2] = 5
    X
    
    # Add a column to a matrix
    cbind(X,c(1,2,3,4,5))
    
    # Add a row to a matrix
    rbind(X,c(1,2,3))
    @
        
    
    
    \section{Linear Models}
       I will introduce the linear and generalized linear model in this section. The linear model is the most basic member of statistical models and enforces a strict parametric form on the estimated function. Though linear models have limited real world applications they introduce many relevant ideas and concept for the following methods. The generalized linear model is the generalized version of the linear model and relaxes some restrictions set by the linear model. Both models are essential for understanding the generalized additive model which is the main subject of this thesis, the generalized linear model being the actual basis for a generalized additive model. For both models I will first introduce the relevant theory for the estimated function then explain how this function is estimated and conclude with an model example. 
        
        \subsection{Simple Linear Model}
        
        The linear model, also linear regression, is a simple but interesting estimation method central to many other more sophisticated methods. We will start with the simple linear model. A simple linear model only allows the response variable to be explained in terms of one explanatory variable. \cite{gamBook} describes the theory for a linear model with one explanatory variables as following: Given $n$ observations of $x_i$ and $y_i$ where $y_i$ is the observations of a random variable $Y_i$ with expectation $\mu_i \equal E(Y_i)$. A linear model assumes a model of the following form:
        
        \begin{equation}
            Y_i = \mu + \epsilon_i where \mu_i = x_i \beta
        \end{equation}
      
        With $\beta$ being an unknown parameter that is to be estimated. The $\epsilon_i$ is the error term for that row. A linear model assumes that $\epsilon_i$ are mutually independent zero mean random variables with the same variance $\sigma^2$. The linear model explains the response variable Y in terms of a predictor variable $x$  multiplied by an estimated coefficient $\beta$ plus a random error term $\epsilon$. 
        
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{simple_linear_model}
            \caption{\cite{gamBook} visually described all relevant variables and their relationship}
            \label{fig:simple_linear_model}
        \end{figure}
        
        
        The essential question that arises is: How can $\beta$ be estimated from the given data $x_i$,$y_i$. To find $beta$ for our desired function we need to define a measure explains of well a model with a particular $\beta$ fits the data. The canonical choice for linear models is the residual sum of squares:
        \begin{equation} 
        \mathbf{S} =  \sum_{i=1}^{n}(y\textsubscript{i} - x\textsubscript{i}\beta)^{2} = \sum_{i=1}^{n}(y\textsubscript{i} - \mu\textsubscript{i})^{2} 
        \end{equation}
        We define a good choice of $\beta$ to minimizes the difference between $y_i$ and $\mu_i$.  A well estimated $\beta$ results on a $S$ closer to zero and the problem of least square estimation becomes minimizing S with respect to $\beta$. To minimize S, differentiate with respect to (w.r.t.) $\beta$.
        
        \begin{equation} \label{partialDer} \frac{\partial S}{\partial \beta} = - \sum_{i=1}^{n} 2x_i(y_i-x_i\beta) \end{equation}

       Setting the result to $0$ we can find the best estimate for $\beta$ called $\widehat{\beta}$, the hat denotes that this is an approximation of a true unknown value $\beta$. Rewriting the partial derivative to yield $\widehat{\beta}$ gives a very good idea of $\beta$.

        \begin{equation}  - \sum_{i=1}^{n} 2x_i(y_i-x_i\widehat{\beta}) = 0  \end{equation}

        \begin{equation} - \sum_{i=1}^{n} x_i y_i-\widehat{\beta} \sum_{i=1}^{n} x_i^2 = 0  \end{equation}

        \begin{equation} \widehat{\beta} = \sum_{i=1}^{n} x_i y_i /\sum_{i=1}^{n} x_i^2 \end{equation}


        Minimizing S w.r.t. $\beta$ to compute $\widehat{\beta}$ is a reasonable approach when dealing with one explanatory variable. The Gauss-Markov theorem shows that estimating $\beta$ with $\widehat{\beta}$ is best linear unbiased estimator. However, most relevant linear models involve more explanatory variables than one. The question for finding must be account for multiple explanatory variables but the measure of fit $S$ remains.
      
        \subsection{Linear Model}
        \subsubsection{Linear Model in the vector-matrix form}
        \cite{gamBook} introduces the linear model as the generalization of the simple linear model, allowing for the response variable to be explained with multiple predictor variables. Though the linear model generalizes the simple linear model it is not the generalized linear model which will be subject in the next section. Allowing for several predictor variables amounts to rewriting the simple linear model in terms of vectors and matrices resulting in a system of equations. Again, given n observations of $y_i$ and $x_i$ plus some additive constant. tuples we are interested in finding $\mu_i = x_i \beta_i$ which stated as a system of equations takes the following form.
        \begin{equation}
        \begin{align*} 
        \begin{matrix}
            \mu_1 = \beta_0 + x_1 \beta_1 \\
            \mu_2 = \beta_0 + x_2 \beta_1 \\
            ... \\
            \mu_n = \beta_0 + x_n \beta_1 \\
        \end{matrix}
        \end{align*}     
        \end{equation}
        
        For the linear model this system of equations must now be rewritten in matrix form resulting in the matrix-vector form. The matrix-vector is a powerful tool to extend existing theory to multiple predictor variables.  
        \begin{equation}
       \left[ \begin{array}{c} \mu_1 \\ \mu_2 \\ \mu_3 \\ ... \\  \mu_n\end{array} \right] = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ 1 & x_3 \\ ... \\1 & x_4   \end{bmatrix}  \left[ \begin{array}{c} \beta_0 \\ \beta_1 \end{array} \right]   
        \end{equation}
        
        The matrix-vector form yields the general form the linear model $\mu = X\beta$. This means that the value vector $\mu$ is given by the model matrix X times the unknown parameter vector $\beta$. The model matrix can account for an arbitrary amount of predictor variables by adding a column for each predictor. The nature of the predictor variable: continuous or discrete plays an important role in the construction of the model matrix. Dummy and factor encoding are the two approaches when dealing with factor variables. For an thorough text on factor encoding in linear models see \cite{regression}.  The model matrix is also known as design matrix and the terms are used interchangeably. $\mu = X\beta$ is the canonical form of a linear model. The method for estimating $\beta$ for one predictor do not work for the matrix-vector form and need to adjusted for estimating the unknown vector $\beta$. 
        
        \subsubsection{Estimating Linear Models}
        \cite{gamBook} states estimating the vector of unknown parameters as finding the least squares solution to $\beta$. The linear model in its full form is:
         
         \begin{equation}
             \mu = X \beta, y N(\mu, I_n, \sigma^2)
         \end{equation}
        
        The model matrix $X$ is a matrix with n rows and p columns. With n being the number of observations and p is a product of predictors and their encoding. The least squares estimation for $\beta$ is the best linear unbiased estimator with least variance, \cite{gamBook} provides a full proof. The method for estimating $\beta$ relies on the Euclidean length of a vector which is the sum squared of its elements. For a vector v in n dimensional space the Euclidean length is defined as:
        
        \begin{equation}
            \left \| v \right \|^2 \equiv v^T v \equiv \sum_{n}^{i=1} v^2
        \end{equation}
        
        A relevant information about the Euclidean length is that rotating a matrix does not change the length, the essential method for fitting least squares heavily leverages this fact. This also holds for the rotation of $y-X\beta$. Bootstrapping the Euclidean length to define our measure $S$ in the matrix-vector framework yields
        
        \begin{equation}
           \label{S} S =  \left \| y - \mu \right \|^2 = \| y = X\beta \|^2 
        \end{equation}
        
        The QR decomposition of a matrix is the essential method for estimating $\beta$ and also an essential method for finding generalized additive models. Any real matrix X can always decomposed into an orthogonal matrix and triangular matrix.
        
        \begin{equation}
        X = Q\begin{bmatrix}R \\ 0 \end{bmatrix} = Q_f R
        \end{equation}
        
        $R$ is the upper triangular matrix with p rows and p columns and $Q$ is an orthogonal matrix with n rows and n columns of which the first p columns form $Q_f$. By definition does multiplying a vector with an orthogonal matrix does not change the their length. The matrix algebra appendix provide more detail on there concepts. The QR decomposition separate a matrix in two pieces with out changing their Euclidean length we can thus multiply the problem of least squares with an orthogonal matrix without changing the length. It is important to remember that we are only interested in the length of the vector from \ref{S}. Applying the QR decomposition to \ref{S} the problem of least squares becomes
        
        
        
        
        
        \subsection{Linear Model Example: How good is my jizz?}
        
        
        \subsection{Generalized Linear Model}
        \subsubsection{Iterative Reweighted Least Squares}
        \subsubsection{Generalized Linear Model Example}
    
    \section{Generalized Additive Models}
        \subsection{Regression Splines}
            \subsubsection{Basis Functions}
            \subsubsection{Cyclic Cubic Regression Splines}
        \subsection{Penalized Iterative Reweighted Least Squares}
        \subsection{Generalized Additive Models Example}
    
    \section{Numerical Optimizations For Large Data Sets}
    
    \section{Apache Spark}
        \subsection{General In Memory Cluster Computing Framework}
        \subsection{The Language Scala and Breeze}
        \subsection{Machine Learning Pipeline}
        \subsection{Implementing Generalized Additive Models in Apache Spark}
    
    \section{Matrix Algebra Appendix}
        \subsection{QR Decomposition}
        
    


    \newpage

    \bibliography{thesis}    % reference to thesis.bib

    \newpage

\end{document}
