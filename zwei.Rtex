\documentclass{article}

% Latex Tutorial beginners: http://www.latex-tutorial.com/tutorials/beginners/
\newcommand{\comment}[1]{}

\newcommand{\note}[1]{{\tiny (#1)}}

% Packages explained - Adding more functions to LATEX http://www.latex-tutorial.com/tutorials/beginners/lesson-3/

\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{textgreek}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{epigraph}

\usepackage[]{babel}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.3in]{geometry}

% f√ºgt die Literatur auch zum Inhaltsverzeichnis hinzu (ist so nicht default)
\usepackage[nottoc,notlot,notlof,numbib]{tocbibind}

% Adding a bibliography http://www.latex-tutorial.com/tutorials/beginners/lesson-7/
\bibliographystyle{apalike}

\graphicspath{ {charts/} }
% Nested numbered enumerations, eg. 1.1, 1.2: http://tex.stackexchange.com/questions/78842/nested-enumeration-numbering
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}


\title{
    Fitting Generalized Additive Models for\\ very large data sets with Apache Spark \\ or \\ 
    Fitting Generalized Additive Models with Apache Spark\\ or \\
    Fitting Generalized Addittve Models for very large data sets
    \\[7pt]
    \large Chapter Excerpt
}
\date{Future}
\author{Kai Thomas Brusch}

\begin{document}

    \pagenumbering{arabic}

    \maketitle

    \paragraph{Summary}

    This document contains three introductory chapters for my bachelor thesis titled "Fitting General Additive Models for very large datasets with Apache Spark".

    \paragraph{Contact} \texttt{kai.brusch@gmail.com}

    \paragraph{Location} Hochschule f\"ur Angewandte Wissenschaften Hamburg
    \paragraph{Department} Dept. Informatik
    \paragraph{Examiner} Prof. Dr. Michael K\"ohler-Bu\ss{}meier
    \paragraph{Second examiner} Dipl.-Mathematiker Univ. Markus Schmaus

    \newpage

    \tableofcontents

    \newpage

    \section{Introduction} \label{introduction}
    
    
    \epigraph{Statistics is the the grammar of science}{\textit{Karl Pearson}}

    Karl Pearson, the father of applied statistics famously described statistics as the grammar of science. Leveraging data as a method of inquiry has become the maxim of modern scientific thought. Hypothesis testing, statistical modeling and measurement are areas in which statistics are essential to every quantitative science. "The coming century is surely the century of data" David Donoho recognizes the 21th century as the century of data and statistics. The ever growing ability to store and process data has facilitated a quantum leap in science, technology and business. Consumer electronics and the modern Internet have generated more data in the past two years than in the prior 2000 years. Modern statistics has developed sophisticated models to discover and explain hidden patterns in data. The glut of data and the complexity of statistical models have placed the computer at the heart of statistics. Advances in computing have bridged the gap between large data sets and statistical inquiry. Cloud computing frameworks allow statistical models to process gigabytes of data. This thesis focuses on a statistical model called generalized additive model ("GAM"). \cite{gam} describes the GAM as the compromise of interpretable and flexible models. The GAM provides the interpretability of traditional linear models but allows for flexible estimation of some variables. 
    
    GAMs and much of modern statistical research is published in the programming language R. \cite{gamBook} implemented an optimized version of the GAM in an R packaged called Mixed GAM computation Vehicle ("mgcv"). R was written by statisticians and is the lingua fracta for statistical research. However, R is not designed for parallelization and cluster computing. Apache Spark ("Spark") has emerged as the most prominent general purpose cluster computing frame work. The ability to perform distributed, in-memory computation while preserving a high level of abstraction makes Spark a popular cluster computing framework. The distributed nature of Spark allows the performance of iterative operations on large data sets while maintaining the data in-memory. Spark appears to be an ideal computation environment for GAMs, because GAMs are estimated by a highly iterative process. Exploring the advantages of using Spark rather than R for GAM estimation is the goal of this thesis. 
    
    This thesis comprises \ref{end} sections. The goal of the first three sections is to introduce the GAM by explaining its components in order of increased complexity. Starting with the framework of statistical learning and the linear model followed by the generalized linear model and finishing with the GAM. Sections 5 details the implementation of GAMs in Spark and includes a language comparison of Spark and R. The final two sections evaluate the implementation by way of comparison and conclude with a discussion on future work.
    

    
        \section{Statistical Learning} \label{statlearn}
     
    Statistical learning is a set of tools for modeling and understanding complex data sets. Recent developments in statistics and computer science have created an interesting field that empowers modern statistical methods with computational ones. Statistical learning provides a  framework for asking and answering questions by using data. This thesis will focus on the generalized additive model, a method of the supervised learning branch of statistical learning. Supervised learning methods provide models for prediction and inference. They provide a framework for explaining response variables in terms of predictor variables. Statistical learning methods are essential for modern scientific research and have empowered many other branches of quantitative sciences. The practical application of these methods in the corporate world has coined the term Data Science.
    
    \subsection{Introduction to Statistical Learning}
    
    The benefits of statistical learning can be best explaining by the use of the following example \cite{hastie}. Assuming a statistical consultant is in charge of advising a client on how to improve the sales of a given product. The client provides a data set containing the sales of a given product across several markets and the associated marketing budget for three different media outlets: TV, radio and newspaper. The client is not able to impact the amount of sales directly, but he is able to change the amount spent on each media outlet. Understanding the number of items sold as a function of media spending enables the consultant to suggest a modified spending plan. This spending plan aims to increase sales by modifying media spending. In \ref{fig:sales} the sales are plotted on the Y-axis and the budget for a media outlet on the X-axis. This model interprets the media budgets as the input variables and sales as the output variables. The input variables are commonly denoted with $X_i$, in this example $X_1$ is the TV spent, $X_2$ the radio spent and $X_3$ the newspaper spent. The name for input variables has become inconsistent. The terms predictor, independent variables and features are treated as synonyms. The output variable, in this example the sales, is also called the response or dependant variable and denoted with $Y$.
    
        \begin{figure}["h"]
            \centering
            \includegraphics[width=\textwidth]{sales}
            \caption{\cite{hastie} describes the units sold as a function of spending on each of the advertisement channels. The blue line is a linear model describing the underlying relationship between units sold and spending on media outlets.}
            \label{fig:sales}
            
        \end{figure}
        
    The applied method can generalized by the following question: Given a data set with observations of a quantitative response $Y$ and several predictors $X_1, X_2 , ... X_i$, what is the underlying, unknown relationship $f()$? $f()$ explains $Y$ in terms of $X$. This can be formally stated as
    \begin{equation}
        Y = f(X) + \epsilon
    \end{equation}
    The function $f()$ is a fixed but unknown function of the input variables $X_1, ... X_p$. $\epsilon$ is a random, identical and independent distributed error term. Another way of framing this is to see the function $f()$ as the systematic information in the observations. All statistical learning methods are concerned with finding an optimal $f()$. This thesis will discuss three related methods of finding $f()$. Each method with a different approach to finding $f()$ and with a different interpretation of the optimal function $f$.


    \subsection{Inference and Prediction}
    Exploring the relationship between dependent and independent variables by estimating a function that satisfies $Y = f(X) + \epsilon$ is beneficial for two reasons: inference and prediction. \cite{hastie} argues that inference is concerned with understanding a system while prediction uses the estimated function to predict $Y$ on different values for $X$.
    
    Once a function $f()$ has been estimated on a particular data set, this function can also be used to predict the value of $Y$ for different $X_1 , ... ,X_p$. Predicting the value of $Y$ new $X$ is simple when the error term averages to zero. In this case the value of $Y$ can be computed with the following equation:
    
    \begin{equation}
        \widehat{Y} = \widehat{f}(X)
    \end{equation}
    
    The wide hat notation stands for an approximated value. $\widehat{f}$ is the estimated function for the true, unknown relationship $f()$ and $\widehat{Y}$ is an estimation of $Y$. The accuracy of our predicted $\widehat{Y}$ depends on two quantities: the reducible error and the irreducible error. The reducible error yields from the approximating nature of $\widehat{f()}$. The choice of $\widehat{f}$ directly causes the reducible error. A different approximation can influence the reducible error. However, some of the error will always be unexplained and is inherent in the measurements taken. This implicit error in our measurement can not be reduced and is thus called the irreducible error. \cite{hastie} formally describes the two types of error as:
    \begin{equation}
        E(Y-\widehat{Y}) = E[f(X)+\epsilon - \widehat{f}(X)]^2 = [f(X)-\widehat{f}(X)] + Var(\epsilon)
    \end{equation}
    
    With $Var(\epsilon)$ being the irreducible error and $[f(X)-\widehat{f}(X)]$ the reducible error. Estimating $f$ to reduce the reducible error lies at the heart of every method discussed in this thesis. However, the irreducible error can not be neglected from discussion. The irreducible error is only assumed to be zero mean sum for the easiest of models.

    
    While prediction is forward-looking, inference, the analytical perspective of statistical learning, is focused on looking backwards. Inference focuses on analyzing the estimated function $\widehat{f}$. Examining the respective contribution of each $X_1, ... , X_p$ to $Y$ can give qualitative insight into finding the best predictors. Returning to the previous example of budget spending on media outlets: In \ref{fig:sales} sales respond differently to additional spending on each media category. An analysis of each function of $X$ shows that TV has the steepest slope. Thus additional spending on TV advertisement leads to the largest increase in sales. This analysis illustrates the use of inference to understanding the underlying mechanism. Understanding  $X$'s influence on $Y$ is a key aspect of inference and therefore the question is: Does $Y$ in- or decrease with a change in $X$ and if so by how much? Inference also examines the fit of the function. \ref{fig:sales} assumes a linear relationship between each $X$ and $Y$. While this trend aptly explains the mechanism of the TV sales, it does not explain the Newspaper category. A linear relationship between sales and Newspaper budget spending seems unreasonable.
    
     
    \subsection{Estimating an unknown function}
    There are many approaches to estimating an unknown function for a given data set. \cite{hastie} states that all approaches share certain characteristics. First, all methods require a set of training data. This training set contains tuples of dependant and independent variables. This set of tuples is used as input for an algorithm that approximates  $\widehat{f}()$. Methods for approximating $\widehat{f}()$ can broadly be categorized as  parametric and non-parametric.
    
    Parametric models, such as the linear and generalized linear model, assume a specific shape of the true underlying function $f()$. Parametric models are always estimated in two steps. First, an assumption about the functional form of $f$ is made. Then the function is estimated. The simplest assumption is the linear model, which is subject of the following section. A linear assumption enforces the following form:
    
    \begin{equation}
        f(X) = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p
    \end{equation}
    
    The function $f$ is linear in the influence of the parameters $\beta$. The linear assumption poses tight restrictions on the shape of $f$ but also on search space of potential functions. A non-parametric function requires searching in an arbitrary p-dimensional function space. However, the linear assumption limits the search space to p+1 coefficients.
    
    After deciding on a shape for $f$ the function needs to be estimated. The process of actually finding $f$ is also referred to as training or fitting a function. In this example fitting amounts to finding the coefficients $\beta_0,\beta_1, ... ,\beta_p$. Therefore the full problem of fitting a linear model is:
    
    \begin{equation}
        Y \approx \beta_0 + \beta_1 X_1 + ... + \beta_p X_p
    \end{equation}
    
    
    The shape decided in step one does not dictate a specific method of finding the coefficients, however there are certain methods commonly associated with particular shapes. The linear assumption in this example is usually fit with a least squares method, which can be computed with different methods. The choice of a parametric form solely dictates the position and number of the parameters involved, thus the name: parametric.
    
    Choosing a functional shape for $f()$ drastically simplifies the fitting process by predetermining the amount of coefficients to be estimated. However, the chosen shape may enforce conditions that are very different to the original function $f$. A poor choice of the parametric form leads to a poor estimate. This problem can be addressed with more flexible methods that allow for more parameters. A higher number of parameters, however, bears the associated risk of over fitting the data. Over fitting estimates a function too close the original data points, possibly omitting important systemic information. A major benefit of parametric methods is the ability to interpret the results. A functional shape is usually determined by factors outside of the data set. As the choice of shape enforces a certain model, the result will be within the shape and is therefore interpretable through the model.
    
    The non-parametric approach is the complement of the parametric one \cite{hastie}. Non-parametric methods, such as regression splines, do not assume the functional form of $f$ before estimating it. This brings a major advantage over parametric methods since they allows $f$ to follow the data as closely as possible without the influence of a predetermined shape. Allowing $f$ to take any arbitrary form offers much more flexibility then parametric methods. While non-parametric methods avoid the pitfall of a bad functional shape, they have to search the whole possible space of f. Searching an arbitrary functional space is very time- and space-intensive. Non-parametric methods provide much more flexibility than parametric methods and are usually preferred for prediction. However, their results' arbitrary shape makes the interpretation of the result harder to understand.
    
    The GAM constitutes an interesting collation of parametric as well as non-parametric elements: it leverages non-parametric estimation of some variables of a parametric method. It uses regression splines estimation to estimate arbitrary functions for some variables of a generalized linear model. 
    
    
    
    \subsection{Statistical Computing Environment: R}
        
    R is a free software environment for statistical computing and graphics. R is designed to express statistical models and comes with an development environment that is built to illustrate and discover data. R is widely recognized as language for statistical research. This thesis uses R to express statistical concepts and examples. This section introduces relevant syntax for matrix manipulation in R:
    
  <<chunk1>>=
    # A vector is the essential data type in R
    y = c(1,2,3,4,5)
    
    # Indexes start at 1
    y[1]
    
    # Create a matrix with n rows m columns filled with random values
    n = 5
    m = 3
    X = matrix(runif(15),n,m)
    
    # A linear model to explain y in terms of X
    mod <- lm(y ~ X)
    
    # Return the coefficients of the model
    coef(mod)
    @
        
    
    
    \section{Linear Models}  \label{glm}
        The linear model is the elementary form of statistical models. It enforces a strict parametric form on the estimated function and belongs to the parametric methods. Even though linear models have limited real world applications they introduce many relevant ideas and concepts for the following methods. The generalized linear model is the generalized version of the linear model and eases some restrictions set by the linear model. Both models are essential for understanding the GAM, which is the main subject of this thesis. The GAM is an extension of the generalized linear model. In the following, the theory behind linear and generalized linear models is explained, before these theories are used in the estimation process. Finally, the theories are applied in an exemplary fitting process.  
        
        
        \subsection{Simple Linear Model}
        
        The simple linear model (or linear regression), explains the response variable as the linear combination of a predictor variable and the estimated coefficients. It is the simplest linear model and explains a normal distributed response variable in terms of one explanatory variable. \cite{gamBook} describes the theory for a linear model with one explanatory variable as follows: Given $n$ observations of $x_i$ and $y_i$ where $y_i$ is the observations of a random variable $Y_i$ with expectation $\mu_i \equiv E(Y_i)$, a linear model has the following parametric form:
        
        \begin{equation}
            Y_i = x_i \beta + \epsilon_i \ where \mu_i = x_i \beta
        \end{equation}
      
        $\epsilon_i$ is the error term for that row and the estimated parameter $\beta$ that. A linear model assumes that $\epsilon_i$ are mutually independent zero mean random variables with variance $\sigma^2$. The linear model explains the response variable $Y$ in terms of a predictor variable $x$  multiplied by an estimated coefficient $\beta$ plus a random error term $\epsilon$. 
        
        \begin{figure}["h"]
            \centering
            \includegraphics[width=\textwidth]{simple_linear_model}
            \caption{\cite{gamBook} visually described all relevant variables and their relationship}
            \label{fig:simple_linear_model}
        \end{figure}
        
        
        $y_i$ and $x_i$ are known but $\beta$ is unknown and thus needs to be estimated. The simple model linear model gives one approach to estimate $\beta$ from the given data $x_i$,$y_i$. The simple linear model seeks to find the $\beta$ that minimizes the squared difference between $y_i$ and $x_i$. Formalizing this notion leads to the definition of the least squares measure $S$:
        
        \begin{equation} 
        \mathbf{S} =  \sum_{i=1}^{n}(y\textsubscript{i} - x\textsubscript{i}\beta)^{2} = \sum_{i=1}^{n}(y\textsubscript{i} - \mu\textsubscript{i})^{2} 
        \end{equation}
        Per this definition a good choice of $\beta$ minimizes the difference between $y_i$ and $\mu_i$. As $S$ converges $0$ the better of an estimated $\beta$  becomes . The problem of least square estimation becomes minimizing $S$ with respect to $\beta$. To minimize S, differentiate with respect to (w.r.t.) $\beta$.
        
        \begin{equation} \label{partialDer} \frac{\partial S}{\partial \beta} = - \sum_{i=1}^{n} 2x_i(y_i-x_i\beta) \end{equation}

       By setting the result to $0$ the best estimate for $\beta$ can be found. The best estimate for $\beta$ is called $\widehat{\beta}$.  After setting the equation to zero and rearranging term to express $\widehat{\beta}$ in terms of $x_i$ and $y_i$ the full problem of estimating $beta$ can be stated as:

        \begin{equation}  - \sum_{i=1}^{n} 2x_i(y_i-x_i\widehat{\beta}) = 0  \end{equation}

        \begin{equation} - \sum_{i=1}^{n} x_i y_i-\widehat{\beta} \sum_{i=1}^{n} x_i^2 = 0  \end{equation}

        \begin{equation} \widehat{\beta} = \sum_{i=1}^{n} x_i y_i /\sum_{i=1}^{n} x_i^2 \end{equation}


        Minimizing $S$ w.r.t. $\beta$ to compute $\widehat{\beta}$ is a reasonable approach when dealing with one explanatory variable. The Gauss-Markov theorem shows that estimating $\beta$ with $\widehat{\beta}$ is best linear unbiased estimator. However, most relevant linear models involve more explanatory variables than one. The question for finding must be account for multiple explanatory variables but the measure of fit $S$ remains.
      
        \subsection{Linear Model}
         \cite{gamBook} introduces the linear model as the generalization of the simple linear model, allowing for the response variable to be explained with multiple predictor variables. Though the linear model generalizes the simple linear model it still assumes a normal distributed $Y$.
        \subsubsection{Linear Model in the vector-matrix form}
        Several predictor variables require rewriting the simple linear model in terms of vectors and matrices. A major benefit of using the matrix-vector forms is that the problem of finding $\beta$ becomes the problem of solving an overdetermined system of equations. Again, given n observations of $y_i$ and $x_i$ plus some additive constant. Explicitly writing each $\mu_i = x_i \beta_i$ illustrates the shape of the system of equations.
        \begin{equation}
        \begin{align*} 
        \begin{matrix}
            \mu_1 = \beta_0 + x_1 \beta_1 \\
            \mu_2 = \beta_0 + x_2 \beta_1 \\
            ... \\
            \mu_n = \beta_0 + x_n \beta_1 \\
        \end{matrix}
        \end{align*}     
        \end{equation}
        
        For the linear model this system of equations must now be rewritten in matrix form resulting in the matrix-vector form.  The matrix-vector form for the simple linear model takes the following shape: 
        \begin{equation}
       \left[ \begin{array}{c} \mu_1 \\ \mu_2 \\ \mu_3 \\ ... \\  \mu_n\end{array} \right] = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ 1 & x_3 \\ ... \\1 & x_n   \end{bmatrix}  \left[ \begin{array}{c} \beta_0 \\ \beta_1 \end{array} \right]   
        \end{equation}
        
        Adding predictor variables and generalizing the simple linear model amounts to appending a predictor variable vector to the matrix and introducing a new coefficient to the coefficient vector:
        
        \begin{equation}
           \left[ \begin{array}{c} \mu_1 \\ \mu_2 \\ \mu_3 \\ ... \\  \mu_n\end{array} \right] = \begin{bmatrix} 1 & x_1 & x_2\\ 1 & x_2 & x_2 \\ 1 & x_3 & x_3 \\ ... \\1 & x_n & x_n   \end{bmatrix}  \left[ \begin{array}{c} \beta_0 \\ \beta_1 \\ \beta_2 \end{array} \right]   
        \end{equation}
    
        The matrix-vector form yields the general form of the linear model $\mu = X\beta$. This means that the value vector $\mu$ is given by the model matrix $X$ multiplied by the unknown parameter vector $\beta$. The model matrix can account for an arbitrary amount of predictor variables by adding a column for each predictor. Continuous and discrete predictor variables require special encoding in the model matrix. Dummy and factor encoding are the two approaches for factor variables. For a detailed explanation on factor encoding in linear models see \cite{regression}. The model matrix is also known as a design matrix and the terms are used interchangeably. $\mu = X\beta$ is the canonical form of a linear model. The method for estimating $\beta$ for one predictor does not work for the matrix-vector form and requires adjustment for estimating the unknown vector $\beta$. 
        
        \subsubsection{Estimating Linear Models}
        \cite{gamBook} describes the estimation process for unknown parameter vectors as finding the least squares solution to $\beta$. The computationally stablest and fastest method involves QR-decomposing the model matrix to express $\widehat{\beta}$ in terms of the upper triangular matrix. This method is widely used in statistical packages and is relevant for the numerically optimized version of the GAM. This approach involves starts with the linear model in the full matrix-vector form:
         
         \begin{equation}
             \mu = X \beta, y \approx N(\mu, I_n, \sigma^2)
         \end{equation}
        
        The model matrix $X$ is a matrix with n rows and p columns. With n being the number of observations and p is a product of predictors and their encoding. The least squares estimation for $\beta$ is the best linear unbiased estimator with least variance, \cite{gamBook} provides a full proof. Estimating $\beta$ relies on the minimizing the Euclidean length of a vector. The Euclidean length of a vector is the sum squared of its elements. For a vector v in n dimensional space the Euclidean length is defined as:
        
        \begin{equation}
            \left \| v \right \|^2 \equiv v^T v \equiv \sum_{n}^{i=1} v^2
        \end{equation}
        
        An essential fact about the Euclidean length is that the rotation of a matrix does not change its length. This property can be used to estimate $\beta$ from a rotated orthogonal matrix. This also applies to the rotation of $y-X\beta$. Bootstrapping the Euclidean length to define our measure $S$ in the matrix-vector framework yields
        
        \begin{equation}
           \label{S} S =  \left \| y - \mu \right \|^2 = \| y = X\beta \|^2 
        \end{equation}
        
        \cite{gamBook} describes QR-decomposition of a matrix as the essential method for estimating $\beta$ and also as an essential method for finding generalized additive models. Any real matrix X can be decomposed into an orthogonal matrix and triangular matrix.
        
        \begin{equation}
        X = Q\begin{bmatrix}R \\ 0 \end{bmatrix} = Q_f R
        \end{equation}
        
        $R$ is the upper triangular matrix with p rows and p columns and $Q$ is an orthogonal matrix with n rows and n columns of which the first p columns form $Q_f$. By definition multiplying a vector with an orthogonal matrix does not change their length. The matrix algebra appendix provides more detail on these concepts. The QR-decomposition divides a matrix into two district matrices that have different properties but maintain their lengths. Multiplying an orthogonal matrix with $S$ does not change the length. Applying the QR decomposition of the model matrix in \ref{S} yields in:
        
        \begin{equation}
             \left \| y - X\beta^2\right \|  =  \left \| Q^Ty - Q^T X\beta^2\right  \| =  \| Q^T-y \begin{bmatrix} R \\ 0 \end{bmatrix} \beta \|^2
        \end{equation}
        
        Only the orthogonal matrix of the QR-decomposed model matrix gets multiplied with the response variable vector and the original model matrix. Multiplying $Q^T$ with the response vector can be stated as $Q^T y = \begin{bmatrix} f \\ r \end{bmatrix}$. Where $f$ is vector of p dimensions and hence r is a vector of $n-p$ dimension.
        
        \begin{equation}
            \left \| y - X\beta\right \|^2  =  \left \| \begin{bmatrix} f \\ r \end{bmatrix} - \begin{bmatrix} R \\ 0 \end{bmatrix}\beta\right \|^2 =  \| f-R \beta \|^2 + \|r \|^2
        \end{equation}
        
        This complicated form exposes the residual error $r$ as independent of $\beta$. $\| f - R \beta \|^2$ can be reduced to zero by choosing $\beta$ so that $R\beta$ equals $f$, the estimator $\widehat{\beta}$ can be stated as 
        
        \begin{equation}
            \widehat{\beta} = R^{-1} f
        \end{equation}
        
        The reducible error, also called residual error is the difference between the model matrix multiplied by the estimated $\beta$ minus the value for y. $\|r\|^2 = \|y-X \widehat{\beta\x}\|^2$. 
        
    \subsubsection{Influence Matrix}
    The hat or influence matrix is the projection matrix that maps the $y$ vector into the column space of $X$. Given a design matrix $X$ with n rows and p columns and $n > p$, the $y$ vector cannot lie in the column space of X. The hat matrix is the projection of $y$ into the column space of $X$. Since the projection is an approximation the matrix is called hat matrix. The hat matrix is required for the prediction piece of linear models and plays an important role for generalized additive models. \cite{gamBook} introduces the influence matrix as the matrix that yields the predicted fitted value vector $\widehat{\mu}$. To obtain $\widehat{\mu}$, the hat matrix is post multiplied with the data vector $y$. The influence matrix is required to compute the estimated value vector. The first p columns of Q make up the matrix $Q_f$. $f$ is then defined as $f = Q_f y$ which implies that 
    
    \begin{equation}
        \widehat{\beta} = R^{-1} Q_f^T y 
    \end{equation}
    
    Given that $\widehat{\mu} = X \widehat{\beta}$ and $X = Q_fR$, the fitted value vector can be written as 
    \begin{equation}
        \widehat{\mu} = Q_f RR^-1 y = Q_fQ_f^T_f y  
    \end{equation}
    Since the upper triangular matrix $R$ is multiplied by its inverse we can factor them out resulting with the hat matrix $A \equiv Q_f Q_f^T$ that fulfills $\widehat{\mu} = Ay$.  The hat matrix is essential in cross validating the estimated $\widehat{y}$ values. Generalized additive models iteratively minimize the difference in distance between the $\widehat{y}$ and $y$. The hat matrix provides a stable and efficient way to compute the $\widehat{y}$ values. 
    
    \subsubsection{Simple linear model example}
        
        
    \subsection{Generalized Linear Model}
    The generalized linear model ("GLM") is an extension of the general linear model. It it is less restrictive on the response variable by allowing it to be distributed according to any of the exponential family distributions. The general linear model only allows normally distributed response variables. Exponential family distributions contain many practical distributions including Poisson, binomial and gamma. The binomial distribution is commonly used to model a binary outcome and the Poisson distribution is used to model occurrences.The GLM can be formally described as:

        \begin{equation} g(\mu_i) = X_i \beta_i \end{equation}

    The GLM models the link function $g()$ of the expected value $\mu_i$ as the linear combination of the model matrix $X$ and the estimated coefficients $\beta$. $X_i$ is the $i^th$ row of a model matrix X and $\beta$ is a vector of unknown parameters. The GLM models the expected value of the random variable $Y$ with $\mu_i \equiv E(Y_i)$. $Y_i$ is assumed to be distributed according to some exponential family. Every exponential family distribution has a canonical link function $g()$. The link function transfers the exponential functions into linear space and each exponential distribution comes with a canonical link function. The canonical link function transforms the expected value into the space of the estimated coefficients. The coefficients are estimated by transforming the right side of the equation. The data is then fit in this transformed scale but the expected variance is calculated on the original scale of the predictor variables. While the least squares approach was sufficient for estimating the coefficients for a normal distributed response with zero mean error, it fails to estimate an arbitrary amount of distribution parameters required for the GLM. \cite{glm} introduces iterative re-weighted least squares ("IRLS") as a method to obtain maximum likelihood estimates ("MLE") for a particular exponential family distribution. IRLS can be interpreted as the generalization of the least squares approach from normal distribution to any exponential family distribution. A practical feature of GLMs is that they can all be fitted using IRLS, independent of response variable distribution. 
 
    \subsubsection{Maximum-Likelihood Estimation}
    
    \cite{glm} describes the principle of maximum likelihood estimation as searching for probability distribution parameters that makes the observed data most likely. This results in a search for the parameter vector that maximizes the likelihood function $L(\beta|y)$. The parameter vector is found by searching the multi-dimensional parameter space.

    \cite{MLE} describes MLE as finding the parameters for a distribution that were most likely to produce the given data. Instead of searching for a coefficient vector that minimizes the squared distance, MLE searches for a particular distribution that most likely would have generate the data. Random variables are defined in terms of one or several parameters. A good choice of parameters results in a distribution that emulates the given data. The least squares linear model estimates the $\mu$ and $\sigma$ for $N( \mu ,\sigma^2)$ by minimizing $S$ and is a special case of the MLE, only allowing for normally distributed data. \cite{glm} introduces a method to estimate parameters for any exponential family distribution. Exponential family distributions have arbitrary numbers of parameters.  The Poisson, for example, only requires one parameter $P(\lambda)$ and the gamma distribution $G(K,\theta)$ two. Maximum likelihood estimation provides a single framework to allow parameter estimation for any of the exponential family distributions.

    From a statistical analysis point of view the vector $y$ of observed data is a random sample from an unknown population. The goal of maximum likelihood estimation is to find the parameters of the given distribution that most likely  produced this sample. This process is described with a probability density function (PDF) $f()$ of observed data $y$ given a parameter $\beta$: $f(y|\beta)$. If individual observations, $y_i$‚Äôs, are statistically independent of one another, the PDF for the data $y$, given the parameter vector $\beta$, can be expressed as a multiplication of PDFs for individual observations.

    \begin{equation} f(y|\beta) = f((y_1, y_2,...,y_n) | (\beta_1,\beta_2, ... \beta_n )) = \prod_{i=1}^{n} f_i(y_i|\beta_i) ) \end{equation}

    Given a set of parameter values, the corresponding PDF will show that some data are more probable than other data. However, the data is already given and the search is for the parameters of the distribution that most likely produced the data. Thus the conditional probability is reversed. From $f(y|\beta)$ to $L(\beta|y)$ produces the likelihood of $y$ given the parameters $\beta$. For computational convenience, the MLE is obtained by maximizing the log-likelihood function, $ln(L(\beta|y))$. This is because the two functions, $ln(L(\beta|y))$ and $L(w|y)$ are monotonically related to each other. Consequently the same MLE estimate is obtained by maximizing either one. However, the log-likelihood is preferred for obvious reasons. Assuming that the log-likelihood function $ln(L(\beta|y))$ is differentiable, if $\beta$ exists, it must satisfy the following partial differential equation known as the likelihood equation:

    \begin{equation} \frac{\partial ln L(\beta|y)}{\partial \beta_i} = 0 \end{equation}

    These properties are given because the definition of the maximum or minimum of a continuous differentiable function implies that its first derivatives vanish at such points. The likelihood equation represents a necessary condition for the existence of an MLE estimate. An additional condition must also be satisfied to ensure that $ln(L(\beta|y))$ is a maximum and not a minimum, since the first derivative cannot reveal this. The log-likelihood must be convex near w. This is verified by calculating the second derivatives of the log-likelihoods and checking if they are negative.

    \begin{equation} \frac{\partial^2 ln L(\beta|y)}{\partial \beta^2_i} < 0 \end{equation}

    This form is only of theoretical value as practical models often involve many parameters and have highly non-linear PDFs. Thus the estimate must be sought numerically using non-linear optimization methods. The basic idea for these methods is to find optimal parameters that maximize the log-likelihood focusing on smaller sub-sets of the multi-dimensional parameter space. This is the preferred approach because exhaustively searching the whole parameter space becomes intractable with an increasing amount of parameters. The practical method for MLE searches by trial and error over the course of a series of iterative steps. Each iteration changes the results from the previous iteration by a small value. The choice of the value is tailored to improve the log-likelihood. GLM are fit by a method called iterative re-weighted least squares and is subject of the next section.
        
\subsubsection{Fitting Generalized Linear Models}
    
    GLMs models are fit via distribution parameter estimation with MLE. The GLM assume a specific structure for the mean and variance of the distribution. This section  demonstrates a general approach to estimating the mean and variance of any exponential distribution with IRLS. IRLS iteratively computes the expected values for a given set of distribution parameters and checks if they have improved. Improved in this context means that the new choice of distribution parameters produces better expected values that are closer to the original $y$ values than the previous estimate. To account for a varying error term each observation is weighted according to its local average. This method gives more weight to observations that are extraordinary compared to its local mean. IRLS applies the weights to each observation to account for the varying error. The GLM models an n-vector of independent response variables $Y$, where $\mu$ is the expected value of $Y$:
    
    \begin{equation}
        g(\mu_i) = X_i \beta
    \end{equation}
    
    With $\mu \equiv E(Y)$ and the link transformation of $Y_i$ as $Y_i \approx f_\theta_i (y_i)$ where $f_\theta_i$ stands for the canonical link transformation of an exponential family distribution. The value of $mu_i$ is determined by finding $\beta$. IRLS provides an estimation process for every exponential family distribution. The estimation of distribution parameters are hence derived from a equation that is general to any exponential family. The GLM assumes that every exponential family distribution can be expressed in terms of its mean and variance. The following equation shows how to generate the mean and the variance of any exponential distribution via the probability mass function (PMF).
    
    \begin{equation}
        f_\theta = exp[\{y\theta-b(\theta)\}/a(\phi)+c(y, \phi)]
    \end{equation}
    
    $a,b$ and $c$ are arbitrary functions, $\phi$ an arbitrary scale parameter and the $\theta$ the canonical link parameters. The interesting property of this form is that the general expression for mean and variance of exponential family distributions can be expressed in terms of $a,b$ and $\phi$. The mean and the variance are the required parameters to fit any exponential family distribution and are computed via deriving the log-likelihood. Since the natural logarithm is the inverse of the exponential function, the log-likelihood can be written as:
    
    \begin{equation}
        l(\theta) = [y\theta - b(\theta)] / a(\phi) + c(y,\phi) 
    \end{equation}
    
    $E(Y)$ can now compute by differentiating $l$ w.r.t. $\theta$ and treating $l$ as a random variable. This leads to a replacement of the particular value $y$ with the random variable $Y$.
    
    \begin{equation}
        \frac{\partial l }{\partial \theta} = [y-{b}'(\theta))]/a(\phi))    
    \end{equation}
    
    \begin{equation}
        E(\frac{\partial l }{\partial \theta}) = [E(Y)-{b}'(\theta))]/a(\phi))
    \end{equation}
    
    Given that $E(\partial l / \partial \theta) = 0$, the expected value of the random variable $Y$ can be stated as:
    
    \begin{equation}
        E(Y) = {b}'(\theta)
    \end{equation}
    
    This results in a method to compute the mean of any exponential family random variable by the first derivative of b w.r.t $\theta$. This the direct link between the $\beta$ and the model parameter. Allowing the parameter $\beta$ to determine by the mean of the response variable and hence the canonical parameter, is the proper way to finding these values. The variance of that random variable can be computed from the second derivative of the log-likelihood:
    
    \begin{equation}
        \frac{\partial^2 l }{\partial^2 \theta} = -{b}''(\theta)/a(\phi)
    \end{equation}
    
    \cite{gamBook} states that re-arranging this formula yields the equation for the variance of $Y$:
    
    \begin{equation}
        var(Y) = {b}''(\theta) a(\phi)
    \end{equation}
    
    A very important feature of GLMs is that the variance can vary. To account for varying variance, the variance is divided by a weight $w$ that penalizes data points with high relative variance.
    
    \begin{equation}
        var(Y) = {b}''(\theta) a(\phi)/w
    \end{equation}
    
    For reasons of simplicity the function $V(\mu)$ is defined as $V(\mu) = {b}''(\theta)$ such that $var(Y) = V\(\mu)$.
    
    Given vector $y$ of an observation random variable $Y$, the maximum likelihood estimation of $\beta$ is possible due to the independence of all $Y_i$. The formal notation for the MLE of the likelihood of $\beta$, $L(\beta)$ becomes
    
    \begin{equation}
        L(\beta) \prod_{i=1}^{n} f_\theta_i (y_i)
    \end{equation}
    
    Given the previously stated probability mass function for exponential family members, the log likelihood $l()$ of $\beta$ can be written as:
    
    \begin{equation}
        l(\beta) = \sum_{i=1}^{n} log[f_\theta_i(y_i))] = \sum_{i=1}^{n} (y_i\theta_i-b_i(\theta_i)) /a_i(\phi)+c_i(y_i, \phi)
    \end{equation}
    
    The log function transforms the product to a sum. $\phi$ is assumed to be constant for all $i$. \cite{gamBook} states that the only relevant choices for $\phi$ are ones that can be stated as $a_i(\phi) = \phi / w_i$ with $w_i$ being a constant. This assumption allows us the rewrite the previous formula as
    
    \begin{equation}
        l(\beta) = \sum_{i=1}^{n}\ w_i(y_i\theta_i-b_i(\theta_i)\)/a_i(\phi)+c_i(y_i, \phi)
    \end{equation}
    
    The process of finding the parameter vector $\beta$ amounts to maximizing the log-likelihood by partially differentiating $l$ w.r.t. each element of $\beta$, setting the resulting expressions to zero and solving for $\beta$.
    
    
    \begin{equation}
    \frac{\partial l }{\partial \beta_j} = 
     \sum_{i=1}^{n} w_i \begin{pmatrix} y_i \frac{\partial \theta_i }{\partial \beta_j} - {b}'(\theta_i) \frac{\partial \theta_i }{\partial \beta_j} \end{pmatrix}
    \end{equation}
    
    \cite{gamBook} details that by applying the chain rule,  $E(Y) = {b}'(\theta)$ and term substitution, the following form can be produced:
    
    \begin{equation}
        \frac{\partial l }{\partial \beta_j} = \frac{1}{\phi} \sum_{i=1}^{n} \frac{[y_i - {b_i}'(\theta_i)]}{b''_i(\theta_i)/w_i} \frac{\partial \mu_i }{\partial \beta_j}
    \end{equation}
    
    By applying the framework developed above to determine variance and expected value, the full form for the estimation of $\beta$ can be stated as:
    
    \begin{equation}
         \sum_{i=1}^{n} \frac{(y_i - \mu_i)^2}{V(\mu_i)} \frac{\partial \mu_i }{\partial \beta_j} = 0 \ \forall \ j.
    \end{equation}
    
    This matches exactly the same equation that would have to be solved to find $\beta$ by non-linear weighted least squares if the weights for $(V(\mu_i)$ were known in advance and were independent of $\beta$. In that scenario the least square objective would be
    
      \begin{equation}
          S = \sum_{i=1}^{n} \frac{(y_i - \mu_i)^2}{V(\mu_i)}
      \end{equation}
    
    In the formulation, $\mu_i$ depends non-linearly on $\beta$ but the weights $V(\mu)$ are treated as fixed. To find the least squares estimated, $\partial S / \partial \beta_j$ must equal to zero for all js. Formulating the search for $\beta$ as finding the optimal choice of distribution parameters of $Y$ invites an iterative approach that chooses some parameter values and successively adjusts these.
    
    \subsubsection{Iterative Reweighted Least Squares}
    IRLS is the generalized method to estimate distribution parameters for any exponential family. The computational method described in this section seeks to estimate distributing parameters by minimizing the distance of expected value to expected value given a current estimate of distribution parameters. \cite{gamBook} formally describes the process as: Let $\widehat{\beta}^{[k]}$ be the estimated parameter vector at the $k^{th}$ iteration. $\eta^{[k]}$ is a vector with the elements $\eta^{[k]}=X_i \widehat{\beta}^{[k]}$ and $\mu_{i}^{[k]}}$ is defined as the inverse of the link function. $\mu_{i}^{[k]} = g^{-1}(\eta_{i}^{[k]}$. Given these definitions, the IRLS algorithm can be stated:
    
    \begin{enumerate}
   \item Compute the weighted variance $V(\mu_{i}^{[k]})$ terms implied by the current estimate for $\widehat{\beta}^{[k]}$
   \item Use these estimates and apply the method described to minimize the least squares objective with respect to $\beta$ to obtain $\widehat{\beta}^{[k]}$
   \item Set k to k+1
   \end{enumerate}
    
    To come to a computational formulation the least square objective for IRLS can be written as 
    
    \begin{equation}
        S = \left \| \sqrt{W}^{[k]}  \left ( z^{[k]} - X\beta \right )  \right \|^2
    \end{equation}
    
    Where $z^{[k]$ is the so called "pseudo data" and $W}^{[k]]$ is the diagonal weight matrix, each defined as
    
    \begin{equation}
        z_{i}^{[k]} = {g}'(\mu^{[k]})(y_i - \mu_i^{[k]} + \eta_i^{[k]}
    \end{equation}
    
    \begin{equation}
        W_{ii}^{[k]} = \frac{1} { V(\mu_{i}^{[k]}) \mu_{i}^{[k]}}
    \end{equation}

    With these definitions we can finally write the full form of the practically used IRLS.
    \begin{enumerate}
        \item Use current $\eta^{[k]}$ and $\mu^{[k]}$ to calculate pseudo data $z^{[k]}$ and iterative weights for the weights matrix $W^{[k]}$
        \item Minimize the least squares objective $\left \| \sqrt{W}^{[k]}  \left ( z^{[k]} - X\beta \right )  \right \|^2$ w.r.t. $\beta$ to obtain  $\widehat{\beta}^{[k+1]}$ and the resulting $\eta^{[k+1]}=X_i \widehat{\beta}^{[k+1]}$ and $\mu_{i}^{[k+1]}}$
        \item Set k to k + 1 
    \end{enumerate}
        
         The proposed method is an iterative method that produces a vector of pseudo data $z$ with the current parameter estimated $\widehat{\beta}$ multiplied with the model matrix $X$ and the expected value of $Y_i$. The distance between the pseudo data vector and the model matrix multiplied by the weight is then minimized to produce a new set of estimates. \cite{glm} have shown that the introduced method will converge on the optimal parameter vector $\widehat{\beta}$. 
        
        
    \subsubsection{Generalized Linear Model Example}
    
    \section{Generalized Additive Models}  \label{gam}
    Generalized additive model (GAM) allow predictor variables of GLM to be estimated with non-parametric methods. GAM has the interpretability advantages of GLMs where the contribution of each predictor variable to the function is clearly encoded. However, it has substantially more flexibility because the relationships between predictor and response variable are not assumed to be linear. GAMs assume the relationship between predictor in response variable to be the sum of functions. These functions are regularized by penalizing the second derivative. Because the acceleration of the functions are penalized they are commonly referred to as smoothing functions. Hastie introduced GAMs and described many approaches to estimate the smooth functions \cite{gam}. Modern approaches have highlighted the use of regression splines for smooth functions estimation \cite{gamBook}. Woods states the smooth function can best be represented as regression splines. This thesis focuses focus on representing the unknown smooth function via regression splines. The implementation will only feature cubic splines and cyclic cubic splines. \cite{gamBook} formally describe GAMs as:
    
    \begin{equation}
        g(\mu_i) = X_i^* \theta + f_1(x_1i) + f_2(x_2i) + ... + f_3(x_3i)
    \end{equation}
    
    GAM inherit the link function $g()$, parameter vector $\theta$ and the model matrix $X_i^*$ from the GLM. Following the GLM definition $\mu \equiv E(Y_i)$ and $Y$ is some exponential family distribution. $X_i^*$ is the ith row of the model matrix. GAM introduce smooth functions $f_j$ over the the predictor variables $x_k$. Specifying a model in terms of an non-parametric, smooth functions allow $x_k$ to have an arbitrary pattern. Allowing $f_j(x_k)$ to follow any shape can give insight into response variable behavior that the parametric form GLMs fail to capture. The gained flexibility comes with the question of how to find these smooth functions.
    
    \subsection{Smooth functions}
    Smooth functions are any function that explains $x_i$ in terms of $y_i$ with some error $\epsilon$. The error term  is a random variable that is independent, identically distributed with $N(0, \sigma^2)$. This is identical to the definition of non-parametric methods from the first section of this thesis. Formally we are searching for function $f()$ that satisfies:
    
    \begin{equation}
        y_i = f(x_i) + \epsilon_i
    \end{equation}
    
    Bootstraping the methods used for fitting a simple linear model we assume the the function $f$ to be linear in $x_i$. This assumption is guarantees that the function can be found with linear parametric methods. Basis functions are a reasonable approach to estimate the function $f$ \cite{gamBook}. The basis function allows to represent the function $f$ as a combination of a basis function and a parameter vector $\beta$. The function $f$ can thus be written as:
    
    \begin{equation}
        f(x) = \sum_{i=1}^{q} b_i(x)\beta_i
    \end{equation}

    The function is $f$ is represented as the sum of basis functions times the parameter vector. The sum goes up the basis dimension which is determined by the chosen basis. The influence of the $\beta$ is linear combination, satisfying the linearity condition.\cite{hastie} states that the basis over the entire range of the data would amount to a polynomial regression. Polynomial regression can be useful but suffer from instability at the edges and insufficient for interpolation. The suitable alternative is splines estimation. Splines divide the the unknown function into sections. Each section is then fit with an individual polynomial. Each piecewise polynomial is required to be continuous at the intersection with the adjacent piecewise polynomial. Basis splines find the piecewise polynomial through a linear combination of function basis and parameters $\beta$. This approach gives enough flexibility while satisfying the linearity constraint.
    
    \subsection{Basis Splines}
    Splines constitute an ideal method for the nonparametric estimation of any unknown function. Basis splines (B-Splines) are of particular interest since they are very easy to set up satisfy in the linearity condition. Their linear nature allows to fit them via the simple model from sections 1. B-Splines consists of two parts: the knots and the functions. A B-spline of order n is a piecewise polynomial function of degree $< n$ in a variable x. Each polynomial is of degree $<n$ \cite{splines} states that B-splines are defined by their order m and number of interior knots N. There are two endpoints which are themselves knots so the total number of knots will be $N + 2$ . The degree of the B-spline polynomial will be the spline order m minus one. The degree n of a B-Splines is thus $m-1$. The knots are formally described as a vector in non-descending order with N interior knots and knots at the endpoints.
    
    \begin{equation}
        t_0 \leq  t_1 \leq ..  t_N \leq t_N+1
    \end{equation}
    
    The location and the number of knots are a matter of design and impact the resulting basis function. The B-Spline function is unique for any given sequence of knots. The B-Spline function is the $b()$ of the previous section. \cite{hastie01statisticallearning} states that for the basis function given a sequence of knots and x values can be given in the Haar form. The Haar form allows for a short, recursive definition of the basis function. 
    
    \begin{equation}
        b_{i,1}(x) := \begin{cases}
 & 1 \text{ if } t_i \leq x \leq t_i+1 \\ 
    & 0 \text{ otherwise}\\ 
        \end{cases}
    \end{equation}
    
    \begin{equation}
           b_{i,k}(x) := \frac{x-t_i}{t_{i+k-1} - t_i} b_{i,k-1}(x) + \frac{t_{i+k}-x}{t_{i+k}-t_{i+1}}b_{i+1,k-1}(x)
           \label{haar}
    \end{equation}
            
    The B-Spline basis is strictly local function. For a given basis order of m+2 the basis function is only non-zero between the m+3 knots. Each basis function is only non-zero over the intervals between m+3 adjacent knots. \cite{regressionspline} provides an insightful example of a B-Spline. B-Splines cover a broad range of basis functions and offer flexibility. Specifying an order for the basis am
            
            
    \begin{figure}["h"]
            \centering
            \includegraphics[width=\textwidth]{bsplines}
            \caption{\cite{regressionspline} gives a visual example of a basis spline.}
            \label{fig:bsplines}
    \end{figure}
    
    The figure on the left is the basis for the given x values and the amended knots. The figure on the right is the resulting B-Spline.
    
    \subsubsection{Fitting Cubic Splines}
    A cubic basis is a reasonable choice for a B-Spline. Cubic splines B-Splines of order 4 with the additional condition that the first and second derivative are equal at the knot location. \cite{gamBook} illustrates the process of fitting a cubic spline to a data set with the predictor variable $x$ and response $y$. For this example the author chooses a specific cubic splines basis called rk. The function rk defines a cubic cyclic basis given a vector of values x and the relevant knots xk. The basis a possible cubic spline choice and introduced by \cite{gamBook}. With the function rk the model matrix to find a cubic spline can be written.  
    The function spl.X sets up the model matrix to fit a cubic spline. The function takes the data vector x and knot vector xk to produce the full model matrix. 
    
    <<echo=FALSE, cache=TRUE>>=
    rk<-function(x,z) # R(x,z) for cubic spline on [0,1] 
        { ((z-0.5)^2-1/12)*((x-0.5)^2-1/12)/4-((abs(x-z)-0.5)^4-(abs(x-z)-0.5)^2/2+7/240)/24
        }
    @
     <<cache=TRUE>>=
     spl.X<-function(x,xk)
            # set up model matrix for cubic penalized regression spline    
        {   q<-length(xk)+2             # number of parameters
            n<-length(x)                # number of data
            X<-matrix(1,n,q)            # initialized model matrix
            X[,2]<-x                    # set second column to x
            X[,3:q]<-outer(x,xk,FUN=rk) # and remaining to R(x,xk) 
            X
        }
     @
     
     The rows of the model matrix are determined by the number of elements in the data vector x. The first two columns of the model matrix are for encoding and of less interest for the thesis. The remaining elements of the model matrix are computing through the basis function rk. Given that the splines were designed to be linear in the unknown parameter $\beta$ the splines can be estimated with the methods introduced in section 1. The ability to fit an arbitrary function by defining the basis and solving the resulting matrix with a linear method is a key aspect of regression splines. To fit the spline we estimate the coefficients from the model matrix with a linear model. The exact fitting happens via the QR Decomposition described in section 1. The estimated coefficients get multiplied by the same design matrix that produced the coefficients to estimates fitted values. This process essentially fits the function to the data that was used to estimate the coefficients. 
     
     <<cache=TRUE>>=
        
        size<-c(1.42,1.58,1.78,1.99,1.99,1.99,2.13,2.13,2.13, 2.32,2.32,2.32,2.32,2.32,2.43,2.43,2.78,2.98,2.98) 
        wear<-c(4.0,4.2,2.5,2.6,2.8,2.4,3.2,2.4,2.6,4.8,2.9, 3.8,3.0,2.7,3.1,3.3,3.0,2.8,1.7) 
        x<-size-min(size)
        x<-x/max(x)
        plot(x,wear,xlab="Scaled engine size",ylab="Wear index")
        xk<-1:4/5 # choose some knots 
        X<-spl.X(x,xk) # generate model matrix 
        mod.1<-lm(wear~X-1) # fit model with out the first column
        xp<-0:100/100 # x values for prediction 
        Xp<-spl.X(xp,xk) # prediction matrix 
        lines(xp,Xp%*%coef(mod.1)) # plot fitted spline
     @
    
    The resulting plot gives a good idea of the estimated method $f()$. The choice of basis dimensions, $q = knots+2$ was arbitrary but highly influential for the function $f()$. The next section introduces theory to justify the choice of basis dimension and a way of penalizing over fit of the data.
    \subsubsection{Penalized Cubic Splines}
    
    Smoothing cubic splines seek to find the smoothest interpolating spline by penalizing the 'wigglynes". \cite{gamBook} argues that the choice of spline basis order does not suffice to control the smoothness of the resulting spline. The author goes on to introduce the penalized regression spline, specifically the penalized cubic spline. Penalized cubic splines introduce a smoothing parameter that penalizes the over-fitting of data. Over-fitting has been stated as a major weakness of non-parametric methods in chapter one, the penalized regression spline is a compromise. As stated the subject of fitting a function is to find a parameter vector $\beta$ that minimizes the following equation.
    
    \begin{equation}
        \left \| y- X\beta \right \| ^2
    \end{equation}
    
    To penalize an hectic function \cite{gamBook} introduce as penalty term $\lambda$. The penalty term $\lambda$ weights the second derivation of the estimated function $f()$. Broadly speaking the second derivative of a function represents the acceleration or the mentioned "wigglyness" of function. With the penalty term the new subject of estimation becomes:
    
    \begin{equation}
        \left \| y- X\beta \right \| ^2 + \lambda\int_{0}^{1} (f^{''}(x))^2) dx
    \end{equation}
    
    The trade-off between fitting all x values and a smooth function is controlled by the penalty term $\lambda$. Since the penalty term weights the smoothness criteria it is also called smoothing parameter. The choice of $\lambda$ is crucial to the resulting function. While a $\lambda$ of 0 creates a function that will directly pass each x data point. A high lambda value will over penalize each acceleration of the function $f$ and generate a straight line. Since the estimated function $f$ is linear in $\beta$ the subject of estimation can be rewritten as:
    
    \begin{equation}
        \left \| y- X\beta \right \| ^2 + \lambda \beta^T S \beta
    \end{equation}
    
    Tha matrix $S$ is a penalty matrix that is specific to the chosen basis. The penalty matrix is a diagonal matrix with the penalty values adjacent to the to be penalized term. The problem of estimating the penalized regression spline is to minimize the just stated equation with respect to $\beta$ and to estimate $\lambda$. Estimating $\lambda$ will be discussed in the next section. For computationally stable fitting of regression splines the above formula can be rewritten as the following:
    
    \begin{equation}
        \label{penalty}
        \left \| y- X\beta \right \| ^2 + \lambda \beta^T S \beta = \left\|  \begin{bmatrix} y\\ 0
\end{bmatrix} - \begin{bmatrix} X\\ \sqrt{\lambda} B \end{bmatrix} \beta \right\| ^2
    \end{equation}
    
    \cite{gamBook} states the the penalty matrix $S$ for a given basis can be written as its square root $B$. Any symetric matrix can be decomposed into the following form. $B = \sqrt{S}$. The model matrix $X$ has been augmented with the square root of the penalty matrix S time the square root of $\lambda$. Since the X matrix gets augmented the vector of $y$ values needs to be agumented as well. Since we still are using linear models the number of elements in the $y$ vector must match the number of rows in the augmented $X$ matrix. A simple square root of a matrix can be written as
    
    <<echo=FALSE, cache=TRUE>>=
     mat.sqrt<-function(S) # A simple matrix square root 
     {  d<-eigen(S,symmetric=TRUE)
        rS<-d$vectors%*%diag(d$values^0.5)%*%t(d$vectors)
     }
    @
    
    The penalty matrix is specific to each basis function. For the author's choice of the rk basis the matrix S is can be created by forming outer knot product with the basis rk. The resulting matrix $S$ is the specific penalty matrix for a vectors of knots and a basis.
     
    <<>>=
    spl.S<-function(xk)
    # set up the penalized regression spline penalty matrix,
    # given knot sequence xk
    {   q<-length(xk)+2
        S<-matrix(0,q,q)                # initialize matrix to 0
        S[3:q,3:q]<-outer(xk,xk,FUN=rk) # fill in non-zero part
        S 
    }
    @
    
    With function for the model matrix X, the penalty matrix S and the ability to take a square root of a matrix a penalized cubic spline can be found by constructing the full matrix according to \ref{penalty}. 
    <<>>=
    prs.fit<-function(y,x,x_knots,lambda)
    # function to fit penalized regression spline to x,y data, 
    # with knots x_knots, given smoothing parameter, lambda.
    {   q<-length(x_knots)+2                # dimension of basis 
        n<-length(x)                        # number of data
        # create augmented model matrix
        Xa <- rbind(spl.X(x,x_knots),mat.sqrt(spl.S(x_knots))*sqrt(lambda)) 
        y[(n+1):(n+q)]<-0               # augment the data vector
        lm(y ~ Xa-1)                    # fit penalized regression spline with a linear model
    }
    @
    
    The prs.Fit functiion estimated a penalized regression spline for a given vector $y$ of response variables, a vector $x$ of predictor variables, a choice of knots $x_knots$ and a given smoothing parameter $\lambda$. The method then forms $X$, $B$, $y$ and $\sqrt{\lambda}$ according to \ref{penalty}. The formed system of equation gets solved via the linear model. The resulting estimated model contains the estimated coefficients. The proposed function can be used to estimated a penalized regression spline for the wear data set.
    
    <<>>=
    x_knots <-1:7/8 # choose some knots 
    lambda <- 0.0001
    mod.2 <- prs.fit(wear,x,x_knots, lambda) # fit pen. reg. spline 
    Xp<-spl.X(xp,x_knots) # matrix to map params to fitted values at xp 
    plot(x,wear);lines(xp,Xp%*%coef(mod.2)) # plot data & spl. fit
    @
    
    From the function prs.fit and \ref{penalty} it should be become evident that the choice of $\lambda$ has a significant influence on the shape of $f$. The figure below illustrates the influence of $\lambda$ on the shape of $f$
    
    
    \begin{figure}["h"]
            \centering
            \includegraphics[width=\textwidth]{lambda}
            \caption{\cite{gamBook} states the influence of $\lambda$ on the resulting function $f$.}
            \label{fig:lambda}
        \end{figure}
    
    \cite{gamBook} states that influence of $\lambda$ is significant enough that it should be estimated individually. How to estimate $\lambda$ is discussed in the next section.
    
    \subsubsection{Smoothing Parameter Estimation}
    The previous section and figure \ref{fig:lambda} in particular illustrated the importance of the smoothing parameter $\lambda$. A high value for $\lambda$ will cause an over smoothing while a low value will under smooth the data. Too high or too low, a bad choice for $\lambda$ will result in a spline $\widehat{f}$ that is far from the original unknown function $f$. An ideal $\lambda$ would have a small distance between $f$ and $\widehat{f}$ A measure $M$ is defined to capture this notion.
    
    \begin{equation}
        M = \frac{1}{n} \sum_{i=1}^{n} (\widehat{f}_i - f_i)^2
    \end{equation}
    
    \cite{gamBook} uses the following notation for the following equations. $\widehat{f}_i \equiv \widehat{f}(x_i)$ and  $f_i \equiv f(x_i)$. A suitable criterion to estimate $\lambda$ is thus to minimize $M$. However, since the true function $f$ is known $M$ cannot be estimated but it is possible to estimate the squared error $E(M)+\sigma^2$. \cite{gamBook} proposes to find $\lambda$ through cross validation. Given $\widehat{f}^{-i}$ is the estimated function on all data except $y_i$ a ordinary cross validation can given as:
    
    \begin{equation}
        V_o = \frac{1}{n} \sum_{i=1}^{n} (\widehat{f}^{-i} - y_i)^2
    \end{equation}
    
    This score results from leaving out each datum in turn, fitting the model to the remaining data and calculating the squared difference between the missing datum and its predicted value: these squared differences are then averaged over all the data. It should be obvious that the process of refitting the function for each $y$ value is $O(n^2)$. \cite{gamBook} states that the ordinary cross validation score can be defined in terms of the influence matrix.
    
    \begin{equation}
        V_o = \frac{1}{n} \sum_{i=1}^{n} (y_i -\widehat{f}_i)^2 /(1-A_{ii})^2
    \end{equation}
    
    The author thus proposes the generalized cross validation score (gcv) to avoid the computational over head. Computing the cross validation score through the hat matrix allows to validate without forming each possible function for each $y$ value. The computational most stable version of the gcv score replaces the weights $1-A_{ii}$ by the mean weight $tr(I-A)/n$. For a detailed discussion of the gcv see \cite{wahba}. 
    
    \begin{equation}
        V_g = \frac{n \sum_{i=1}^{n} (y_i - \widehat{f}_i )^2}{tr(I-A)^2} 
    \end{equation}
    
    This section will use this definition to compute the gcv score for a given function and the given hat matrix. With this the definition a simple loop can be written to find the best possible $\lambda$. The model producing the smallest gcv score generates the smoothest possible cubic spline for the given data set.
    <<>>=
    lambda<-1e-8
    n<-length(wear)
    V<-0
    for (i in 1:60) # loop through smoothing parameters 
        { 
            mod<-prs.fit(wear,x,xk,lambda)          # fit model, given lambda
            trA<-sum(influence(mod)$hat[1:n])       # find tr(A) 
            rss<-sum((wear-fitted(mod)[1:n])^2)     # residual sum of squares 
            V[i]<-n*rss/(n-trA)^2                   # obtain GCV score 
            lambda<-lambda*1.5                      # increase lambda
        }
    plot(1:60,V,type="l",main="GCV score",xlab="i")     # plot score
    i<-(1:60)[V==min(V)]                             # extract index of min(V) 
    mod.3<-prs.fit(wear,x,xk,1.5^(i-1)*1e-8) # fit optimal model 
    Xp<-spl.X(xp,xk) # .... and plot it
    plot(x,wear)
    lines(xp,Xp%*%coef(mod.3))
    @
    
    This section introduced the gcv score as a measure to evaluate the fit of a penalized regression spline. The gcv score is the central metric in evaluating choices for $\lambda$ and will be used the the following sections to evaluate generalized additive models.
    
    \subsection{Additive Model}
    The discussed methods allow to fit a regression spline to a single variable. Almost all relevant model have more than one variable. The additive model allows to model the response as the sum of several predictors. \cite{gamBook} formally describes the additive model as:
    
    \begin{equation}
        y_i = f_1(x_i) + f_2(z_i) ... f_n(w_i)  + \epsilon_i
    \end{equation}
    
    The functions $f_j$ are smooth functions estimated with the methods introduced in the previous sections. The error term $\epsilon$ is distributed according to an independent identically distributed $N(0,\sigma^2)$. Modelling $y_i$ as the sum of individual smoothing functions rather than a single function of all terms imposes a very strong condition. $f_1(x) + f_2(z)$ is a special case of the of the general smooth function of both predictors $f(x,z)$. The benefit of modeling each function individually is that each predictor maintains the interpretability of the linear model. Estimating $f(x,z)$ would provide superior flexibility but drastically decreased interpretable. The individual smooth function are a major benefit of the additive model. Fitting additive model with by the methods used for a single regression spline amounts to rewriting the penalty term. Instead of penalizing a single spline both splines no need to be penalized. The unknown parameter vector $\beta$ for the additve model can be estimated by the minimization of the penalized least squares objective.
    
    \begin{equation}
        \left \| y- X\beta \right \| ^2 + \lambda_1 \beta^T S_1 \beta + \lambda_2 \beta^T S_2 \beta
    \end{equation}
    
    Each smoothing function gets estimated with an individual penalty matrix $S_i$ and a smoothing parameter $\lambda_i$. The additive nature of the model allows to write the combined penalty as $S \equiv \lambda_1 S_1 + \lambda_2 S_3$. The ability to rewrite the penalty matrix as the addition of the individual penalty matrices and smoothing parameter allows to rewrite the least squares objective for computation.
    
    \begin{equation}
        \label{penalty}
        \left \| y- X\beta \right \| ^2 +  \beta^T S \beta = \left\|  \begin{bmatrix} y\\ 0
        \end{bmatrix} - \begin{bmatrix} X\\  B \end{bmatrix} \beta \right\| ^2
    \end{equation}
    
    Similar to the single smoothing function the $B$ is any square root matrix such that $B^TB=S$. Summing the different $\lambda$ and $S$ terms in to the square root matrix allow this model to be estimated with the standard linear model. To illustrate an additive model \cite{gamBook} defines an additive model in two steps. One method to set up the model and the penalty matrices. The second the estimate the models with  smoothing parameters. 
     <<>>=
    am.setup<-function(x,z,q=10)
        # Get X, S_1 and S_2 for a simple 2 term AM 
        { 
        # generate equidistant knots
        xk <- quantile(unique(x),1:(q-2)/(q-1))
        zk <- quantile(unique(z),1:(q-2)/(q-1))
        # Generate two individual, non-overlaping penalty matricies
        S <- list()
        S[[1]] <- S[[2]] <- matrix(0,2*q-1,2*q-1) 
        S[[1]][2:q,2:q] <- spl.S(xk)[-1,-1] 
        S[[2]][(q+1):(2*q-1),(q+1):(2*q-1)] <- spl.S(zk)[-1,-1] #     
        # Set up the model matrix X
            n<-length(x)
            X<-matrix(1,n,2*q-1)
            X[,2:q]<-spl.X(x,xk)[,-1]               # 1st smooth
            X[,(q+1):(2*q-1)]<-spl.X(z,zk)[,-1]     # 2nd smooth
            list(X=X,S=S)
        }

    @ 
   
    The function am.setup generates the model matrix and the penalty matrices for two predictor variables and a fixed set of equidistant knots. The knots are an arbitrary choice and only for illustrative purposes. A key aspect is that the function sets up two penalty matrices. To maintain the additive property each penalty matrix gets placed in full penalty matrix that is zeroes outside of the individual penalty matrix. The full penalty matrix has the size of n rows and number of parameters times penalty matrices. For example the first penalty matrix is covers all rows of for the first number of parameters columns and is zero otherwise. The second penalty matrix is 0 for the previous matrix was defined. Setting the penalty matrix up this way allows to estimate the smoothing parameter individually in the next function. The constructed model matrix $X$ and the list of penalty matrices are now sufficient to estimate an additive model.
   
    <<>>=
    fit.am<-function(y,X,S,sp)
        # function to fit simple 2 term additive model 
        { # generate the full penalty matrix and take square root
        rS <- mat.sqrt(sp[1]*S[[1]]+sp[2]*S[[2]])
        q <- ncol(X) # number of params
        n <- nrow(X)
        X1 <- rbind(X,rS) 
        y1<-y;y1[(n+1):(n+q)]<-0 # augment data 
        b<-lm(y1~X1-1) # fit model 
        trA<-sum(influence(b)$hat[1:n]) # tr(A) 
       norm<-sum((y-fitted(b)[1:n])^2) # RSS 
        list(model=b,gcv=norm*n/(n-trA)^2,sp=sp)
    }
    @
   
    
    The function fit.am takes the previously generated model and penalty matrix a vector of $y$ values and a vector of smoothing parameter values and produces a list of the model and the gcv score. Each penalty matrix receives an individual smoothing parameter. Defining the full penalty matrix as zero outside of the individual penalty matrix allows to multiply the smoothing parameter and then add the individual matrices to form the full penalty matrix $rS$. The model matrix on top of the penalty matrix form the complete model matrix $X$. Since the model matrix has more rows that the data  vector $y$ the data vectors receives zeroes to match. The resulting model is then fit via the linear model from section one. After the model is fit the gcv score gets computed and gets stored.
    
    \subsection{Additive Model Example}
    The process of fitting a two term additive model can be best understood by using the  setup.am and fit.am functions on a data set. \cite{gamBook} uses the default R data set $trees$. This data set contains three variables: Volume, Girth and Height for 31 felled cherry trees. The author suggests the following model to illustrate an additive model
    \begin{equation}
        Volumne = f_1(Girth) + f_2(Height) + \epsilon_i
    \end{equation}
    
    This model can be estimated with the functions defined in the previous section. 
    
     <<>>=
    data(trees)
    rg <- range(trees$Girth)
    rh <- range(trees$Height)
    ## rescaling of parameter to onto [0,1]
    trees$Girth <- (trees$Girth - rg[1])/(rg[2]-rg[1]) 
    trees$Height <- (trees$Height - rh[1])/(rh[2]-rh[1])
    am0 <- am.setup(trees$Girth,trees$Height)
    @
    The model matrix and the penalty matricies get setup with the rescaled parameters of Girth and Height. The resulting list can then be used to estimate the full model but not the smoothing parameter. To estimate the smoothing parameter the fit.am functions gets called with several choices of $\lambda$. The $\lambda$ that generates the lowest gcv score is estimated by iteratively trying different $\lambda$ values.  
    
    <<>>=
    sp<-c(0,0) # initialize smoothing parameter (s.p.) array 
    for (i in 1:30) for (j in 1:30) # loop over s.p. grid
    { 
        sp[1]<-1e-5*2^(i-1);sp[2]<-1e-5*2^(j-1)     # s.p.s 
        b<-fit.am(trees$Volume,am0$X,am0$S,sp)      # fit using s.p.s.
        if (i+j==2) best<-b else                    # number of data
        if (b$gcv<best$gcv) best<-b                 # augmented X
    }
    best$sp # GCV best smoothing parameter 
    @
    
    The resulting smoothing parameter for Girth is fairly presumably allowing $f_1$ some curvature. Height has a very high smoothing parameter, this most likely results in a rather straight line.  he values of the smooths at the predictor variable values can be obtained quite easily by zeroing all model coefficients, except those corresponding to the term of interest, and using predict as the following code shows.

    
    <<echo=FALSE, cache=TRUE, fig.height=4, fig.width=4, fig.pos="t">>=
    
    # plot 
    plot(trees$Volume,fitted(best$model)[1:31],
        xlab="Fitted Volume",ylab="Actual Volume") 
    b<-best$model
    b$coefficients[1]<-0 # zero the intercept 
    b$coefficients[11:19]<-0 # zero the second smooth coefs 
    f0<-predict(b) # predict f_1 only, at data values 
    plot(trees$Girth,f0[1:31],xlab="Scaled Girth",
     ylab=expression(hat(f[1])))

    @
    
    The resulting plot confirms the observation that the Girth smooth has more curvature than the Height smooth. The middle figure is the estimate of the smooth function of Girth at the given Girth data. The right figure is the estimate of the smooth function of Height at the given Height data. 
   
    \subsection{Generalized Additive Model}
    Generalized additive models (GAMs) follow from additive models, as generalized linear models follow from linear models. Like the GLM the GAM predicts some known smooth monotonic function of the expected value of the response. The response following any exponential family distribution. For the sake of illustrating the similarities to a GLM the function fit.am can extended to account for a gamma error and log link function.  
    
    <<>>==
    fit.gamG<-function(y,X,S,sp)
        # function to fit simple 2 term generalized additive model 
        # Gamma errors and log link
        { # get sqrt of combined penalty matrix
            rS <- mat.sqrt(sp[1]*S[[1]]+sp[2]*S[[2]])
            q <- ncol(X) # number of parameterss
            n <- nrow(X) # number of data
            X1 <- rbind(X,rS) # augmented model matrix
            b <- rep(0,q);b[1] <- 1 # initialize parameters
            norm <- 0;old.norm <- 1 # initialize convergence control 
            while (abs(norm-old.norm)>1e-4*norm) # repeat unconverged
            { eta <- (X1%*%b)[1:n] # generate pseudo data by computing expected value of current beta choice
                mu <- exp(eta) # log link, exp of pseudo data
                z <- (y-mu)/mu + eta  # compute pseudo data
                z[(n+1):(n+q)] <- 0 # agument pseudo data
                m <- lm(z~X1-1) # solve the linear model with pseudo data
            }
        list(model=m,gcv=norm*n/(n-trA)^2,sp=sp)
        }
    @
    
    The function fit.gamG takes a model matrix X and a list of penalty matricies S. Both can be generated by the am.setup function. The generalized additive model seeks the best smoothing parameter by iteratively searching for a value until convergence. Each iteration step forms a vector of pseudo data. Pseudo data is the estimated values trained data. If the resulting gcv score of the current pseudo data does not change the model is converged. The fitting process is called penalized iterative reweighted least squares and formally described in the next section.
    
    
    
    
    \subsubsection{Penalized Iterative Reweighted Least Squares}
    While the GLM is fitted by the iterative reweighted least squares GAMs are fitted by the penalized reweighted least squares. The major difference being that the objective is contains the penalty matrix. To fit a generalized additive model the following penalized iteratively re-weighted least squares (P-IRLS) scheme is iterated to convergence.
    
    \begin{enumerate}
    \item Given the current parameter estimated $\beta^{[k]}$ and the estimated mean response vector $\mu^{[k]}$ compute 
    \begin{equation}
        w \propto \frac{1}{V(\mu_i^{[k]}) g^{'}(\mu_i^{[k]})}
    \end{equation}
        
    With the pseudo data vector $z_i= g(\mu_i^{[k]})(y_i-\mu_i^{[k]}) + X_i \beta^{[k]}$ and the $var(Y_i) = V(\mu^{[k]}) \theta$ as described in the IRLS section.
    
    \item Minimize the resulting least squares objective 
    
    \begin{equation}
    \left \|  \begin{bmatrix} \sqrt{W} & 0 \\ 0 & I \end{bmatrix} \left ( \begin{bmatrix} z\\0 \end{bmatrix} -  \begin{bmatrix} X \\ B \end{bmatrix} \right  \beta)\right \| ^2 \end{equation}
    
    Minimizing the least squares objective w.r.t. $\beta$ to obtain $\beta^{[k+1]}$. B is again any square root matrix such that $B^TB = \lambda_1 S_1 +\lambda_2 S_2$
    
    \end{enumrate}

    The P-IRLS accounts for the penalty matrix required to fit additive models with smoothing functions.
    
    \subsubsection{Generalized Additive Model Example}
    Fitting a GAM with the fit.gamG function we can see a slightly better fit than th
    
    
    
    \section{Apache Spark}
        \subsection{General In Memory Cluster Computing Framework}
        \subsection{The Language Scala and Breeze}
        \subsection{Machine Learning Pipeline}
        \subsubsection{Transformer}
        \subsubsection{Estimator}
        \subsection{Implementing GAM in Apache Spark}
        \subsection{R vs Spark vs SparkR}
        \subsubsection{Architecture}
        
        

    \section{Experiment: Fitting GAMs for large data sets}
    \subsection{Fitting a large GAM with Airbnb data}
        \subsubsection{Why GAMs for Airbnb}
        \subsubsection{Fitting seasonality with GAMs}
    \subsection{How does the performance compare to R?}
        \subsubsection{Memory}
        \subsubsection{CPU}
    \subsection{How does the performance scale?}
        \subsubsection{Memory}
        \subsubsection{CPU}
    
    \section{Conclusion}
    \subsection{Future Work}
    
    
    
    \section{Matrix Algebra Appendix} \label{end}
        \subsection{QR Decomposition}
        
    


    \newpage

    \bibliography{thesis}    % reference to thesis.bib

    \newpage

\end{document}