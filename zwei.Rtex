\documentclass{article}

% Latex Tutorial beginners: http://www.latex-tutorial.com/tutorials/beginners/
\newcommand{\comment}[1]{}

\newcommand{\note}[1]{{\tiny (#1)}}

% Packages explained - Adding more functions to LATEX http://www.latex-tutorial.com/tutorials/beginners/lesson-3/

\usepackage{booktabs}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{textgreek}
\usepackage{listings}
\usepackage{graphicx}
\usepackage[]{babel}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.3in]{geometry}

% f√ºgt die Literatur auch zum Inhaltsverzeichnis hinzu (ist so nicht default)
\usepackage[nottoc,notlot,notlof,numbib]{tocbibind}

% Adding a bibliography http://www.latex-tutorial.com/tutorials/beginners/lesson-7/
\bibliographystyle{apalike}

\graphicspath{ {charts/} }
% Nested numbered enumerations, eg. 1.1, 1.2: http://tex.stackexchange.com/questions/78842/nested-enumeration-numbering
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}


\title{
    Fitting Generalized Additive Models for\\ very large data sets with Apache Spark \\ or \\ 
    Fitting Generalized Additive Models with Apache Spark\\ or \\
    Fitting Generalized Addittve Models for very large data sets
    \\[7pt]
    \large Chapter Excerpt
}
\date{Future}
\author{Kai Thomas Brusch}

\begin{document}

    \pagenumbering{arabic}

    \maketitle

    \paragraph{Summary}

    This document contains three introductory chapters for my bachelor thesis titled "Fitting General Additive Models for very large datasets with Apache Spark".

    \paragraph{Contact} \texttt{kai.brusch@gmail.com}

    \paragraph{Location} Hochschule f\"ur Angewandte Wissenschaften Hamburg
    \paragraph{Department} Dept. Informatik
    \paragraph{Examiner} Prof. Dr. Michael K\"ohler-Bu\ss{}meier
    \paragraph{Second examiner} Dipl.-Mathematiker Univ. Markus Schmaus

    \newpage

    \tableofcontents

    \newpage

    
        \section{Statistical Learning}
    
     \cite{hastie} introduces statistical learning as a set of tools for modeling and understanding complex data sets. Recent developments in statistics and computer science have created an interesting field that empowers modern statistical with powerful computational methods. Statistical learning provides a powerful framework to ask and answer interesting questions with data. This thesis will focus on the generalized additive model, a method of the supervised learning branch of statistical learning. Supervised learning methods build a statistical model for predictions and estimations of an output variable based on one or many input variables. Statistical learning methods are essential for modern scientific research and have empowered many other branches of quantitative sciences. The professional world has been raving about the potential of statistical learning giving birth terms such as: Data Science and Big Data.
    
        \subsection{Why statistical learning?}
    
    Ideas are sometimes best conveyed through example. \cite{hasite} illustrated the benefits of statistical learning on a simple yet illustrative example. In the example we assume the be working for as statistical consultant in charge of advising a client on how to improve the sales of a given product. The client provides us with a data set containing the sales of a given product across several markets along with the budged for the product in each market in three different media outlets: TV, radio and newspaper. The client is not able to directly impact the amount of sales but he may to change amount spend on each media outlet. If we can understand the number of items sold as a function of spend on media we can suggest our client to adjust spending to increase units sold. In \ref{fig:sales} the sales are plotted on the Y axis and the budget for a media outlet on the X axis. \cite{hastie} uses this example to introduce terminology as well as the framework. The media budgets are the input variable and sales are the output variables. The input variables are commonly denoted with $X_i$, in example $X_1$ is the TV spend, $X_2$ the radio spend and $X_3$ the newspaper spend. The name for input variables has become inconsistent as the the terms predictor, independent variables and features can be used interchangeably. The output variable, in this example the sales, is also called the response or dependant variable and denoted with Y.
    
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{sales}
            \caption{\cite{hastie} illustrates the units sold as a function of spend on each of the advertisement channels. The blue line is a linear model describing the underlying relationship between units sold and spend on media outlet.}
            \label{fig:sales}
        \end{figure}
        
    This example illustrates the notion behind statistical learning but fails to capture it's broad applicability. To generalize this example the essential question of statistical learning needs to be generalized. Supposed a data set contains observations of a quantitative response $Y$ and several predictors $X_1, X_2 , ... X_i$. Statistical learning is the practice of looking for a relationship $f()$ that describes the relationship in the following form:
    \begin{equation}
        Y = f(X) + \epsilon
    \end{equation}
    The function $f()$ is a fixed but unknown function of the input variables $X_1, ... X_p$. $\epsilon$ is random, identical, independent distributed error term. Another way of framing this is to see the function $f()$ as the systematic information in the observations. All statistical learning methods are concerned with finding an optimal $f()$. This thesis will discuss three, related methods of finding $f()$. Each method with different idea on how to find $f()$ and with a different interpretation of optimal.


    \subsection{Why Estimate f()?}
    Framing the question of relationship between dependent and independent variable as the problem to find a function $f()$ has benefits. \cite{hastie} argues two essential reasons for estimating $f()$: inference and prediction. Inference is concerned with understanding a system while prediction wants to use the estimated function to predict $Y$ on different values for $X$.
    
    Once a function $f()$ has been estimated on a given data set we can use that function to predict the value of Y for different $X_1 , ... ,X_p$. Predicting the value of $Y$ new $X$ values can be simple when the error term averages to zero. The value of Y can computed with the following equation:
    
    \begin{equation}
        \widehat{Y} = \widehat{f}(X)
    \end{equation}
    
    The wide hat notation stands for an approximation of an unknown. $\widehat{f}$ is the estimated function for the true, unknown relationship $f()$ and $\widehat{Y}$ is an estimation of $Y$. The accuracy of our predicted $\widehat{Y}$ depends on two quantities: the reducible error and the irreducible error. The reducible error yields from the approximating nature of $\widehat{f()}$. The choice of $\widehat{f}$ causes the reducible error. A different approximation can change the reducible error. However, some of the error will always be unexplained and inherent in the measurements taken. This implicit error in our measurement can not be reduced and is thus called the irreducible error. \cite{hastie} formally describes the two types of error as
    \begin{equation}
        E(Y-\widehat{Y}) = E[f(X)+\epsilon - \widehat{f}(X)]^2 = [f(X)-\widehat{f}(X)] + Var(\epsilon)
    \end{equation}
    
    With $Var(\epsilon)$ being the irreducible error and $[f(X)-\widehat{f}(X)]$ the reducible part. Estimating $f$ to reduce the reducible error stands at the heart of every method introduced in this thesis. However, the irreducible error can not be neglected from discussion. The irreducible error is only assumed to be zero mean sum on the easiest of models, an important feature of models introduced later in this thesis is the assumption that the irreducible is in fact not zero sum average. 

    
    While prediction is focused on looking forward, inference is somewhat focused on looking backwards. Inference is the analytical perspective of statistical learning. Inference focuses on analysing the estimated $\widehat{f}$. Examining the respective contribution of each $X_1, ... , X_p$ to $Y$ can give qualitative insight into finding the best predictors $Y$ our of many. Returning to the previous example of budget spend on a media outlet. In \ref{fig:sales} the number of sales responds differently to additional spend for each media category. Comparing the different $X$s we see that TV is by far the most efficient place to spend the budget. This simple analysis illustrates the power of inference to understand the underlying system. Understanding the basic nature of $X$'s influence on $Y$ is a key aspect of inference. Does $Y$ in- or decrease with a change in $X$ and if so by how much.  Inference further examines the fit of the function, \ref{fig:sales} assumes a linear relationship between each $X$ and $Y$, while this tread works well for TV it fails to capture the nature of the Newspaper category. A linear relationship between sales and Newspaper seems unreasonable after examination.   
    

     
    \subsection{How to estimate f()?}
    There are many approaches to estimate an unknown function given data points. \cite{hastie} states that all share certain characteristics. First all methods need training set of data. This training set contains tuples of dependant and independent variables. This set of tuples is used as input for an algorithm that approximates function $\widehat{f}()$. Methods for approximating $\widehat{f}()$ can broadly be categories into two categories: parametric and non-parametric.
    
    Parametric models assume a specific shape of the true underlying function $f()$. This assumptions manifests itself in the two step approach to to parametric models. First, make an assumption about the functional form or shape of $f$. The simples assumption and subject of the next chapter is the linear model. A linear assumption results in the following form:
    
    \begin{equation}
        f(X) = \beta_0 + \beta_1 X_1 + ... + \beta_p X_p
    \end{equation}
    
    The function f is linear in the influence of the parameter $\beta$. The linear assumption poses tight restrictions of the shape $f$ but also on space of potential functions. While a non-parametric function implies a search in an arbitrary p-dimensional function space the linear assumption limits the search to p+1 coefficients, $\beta_0,\beta_1, ... ,\beta_p$ in this example. 
    
    After deciding on shape for $f()$ the function needs to be estimated. The process of actually finding $f$ is also referred to as training or fitting a function. In this example this means fitting means to find the coefficients $\beta_0,\beta_1, ... ,\beta_p$. Thus the full problem of fitting a linear model becomes:
    
    \begin{equation}
        Y \approx \beta_0 + \beta_1 X_1 + ... + \beta_p X_p
    \end{equation}
    
    
    The shape decided in step one does not imply a method of actually finding the coefficients though there are best practices associated with each. The linear assumption in this example is usually fit with a least squares method, which can be computer with different methods. The choice of a parametric form solely dictated the nature and number of the parameters involved, thus the name: parametric.
    
    Choosing a functional shape for $f()$ drastically simplifies the fitting process by predetermining the amount of coefficients to be estimated. However, the chosen shape may enforce conditions that are very different to the original function $f$. A poor choice of the parametric form will directly translate into a poor estimate. This problem can be addressed with more flexible methods that allow for a more parameters. The higher number of parameters has to be carefully weighed with a phenomena called over fitting. Over fitting is the result of giving a functional form too many parameters, causing the approximation to follow the error too closely. A major benefit of parametric methods is the ability to interpret the results. Assuming a functional shape usually yields from information about the function that is not in the data itself. Accounting for outside information though a choice of parametric shape allows for an easy interpret ability. The choice of form enforces a certain model of the data I want to examine, the result will be within the shape and thus are interpretable through the model.
    
    \cite{hastie} describes the non-parametric approach as the complement to the parametric methods. Non-parametric methods do not assume the functional form of $f$ before estimating it. This brings a major advantage over parametric methods since they allows $f$ to follow the data as closely as possible without dealing with a determined shape. Allowing $f$ to take any arbitrary form is a large advantage and offers much more flexibility then parametric methods. While non-parametric methods avoid the pitfall of a bad functional shape they have to search the whole possible space of f. Searching an arbitrary functional space is very time and space intensive. Non-parametric methods provide much more flexibility than parametric methods and are usually preferred for prediction. Their arbitrary shape of the results makes the interpretation of the result harder to understand and are a clear disadvantages over parametric methods.
    
    The method central to this thesis follows a progression through the parametric, non-parametric framework. The simple linear model will be the first model introduced and is a parametric method. The linear model assumes a linear relationship between $X$ and $Y$ which makes it the easiest to estimate and interpret. Enforcing very strict limitations on functional form, only allowing a linear relationship with a normal distributed response variable and a zero mean error term. The generalized linear model extends the linear model by easing the restriction on the response variable and allowing for a varying error term. Even though the restrictions on the functional shape are eased in the generalized linear model they are still a parametric methods with all the advantages and disadvantages. Generalized additive models stand at the heart of the thesis and present an interesting extension of the generalized linear model. Generalized additive models allow for a non-parametric method called smoothing splines to be used as part of generalized linear models. Splines are a highly flexible non-parametric method that allow to fit arbitrary functions. Generalized additive models offer much greater flexibility than generalized linear models but rely on similar estimation technique and theory. This progression with an implementation of generalized additive models in a cluster environment will be the central piece of this thesis.
    
    
    \subsection{Statistical Computing Environment: R}
        
    R is a free software environment for statistical computing and graphics. R is designed to express statistical models and comes with an development environment that is build to illustrate and discover data. Academia has recognized R as it's language and continuously published learning material and research alongside R code. \cite{hastie} and \cite{gamBook} employ R to illustrate statistical concepts in a computational environment. I will do the same and use R to express statistical concepts and example throughout this thesis. I will now introduce some syntax for matrix manipulation in R:
    
  <<chunk1>>=
    # A vector is the essential data type in R
    y = c(1,2,3,4,5)
    
    # Indexes start at 1
    y[1]
    
    # Create a matrix with n rows m columns filled with random values
    n = 5
    m = 3
    X = matrix(runif(15),n,m)
    
    # A linear model to explain y in terms of X
    mod <- lm(y ~ X)
    
    # Return the coefficients of the model
    coef(mod)
    @
        
    
    
    \section{Linear Models}
       I will examine the linear and generalized linear model in this section. The linear model is the an elementary member of statistical models and enforces a strict parametric form on the estimated function. Though linear models have limited real world applications they introduce many relevant ideas and concept for the following methods. The generalized linear model is the generalized version of the linear model and relaxes some restrictions set by the linear model. Both models are essential for understanding the generalized additive model which is the main subject of this thesis. Generalized additive models are an extension to the generalized linear model. For both models I will first introduce the relevant theory for the estimated function then explain how this function is estimated and conclude with an model example. 
        
        \subsection{Simple Linear Model}
        
        The linear model, also linear regression, is a simple but interesting method central to many other more sophisticated methods. The simple linear model is the simplest linear model. A simple linear model explains a normal distributed response variable to be explained in terms of one explanatory variable. \cite{gamBook} describes the theory for a linear model with one explanatory variables as following: Given $n$ observations of $x_i$ and $y_i$. Where $y_i$ is the observations of a random variable $Y_i$ with expectation $\mu_i \equiv E(Y_i)$. A linear model assumes a model of the following parametric form:
        
        \begin{equation}
            Y_i = x_i \beta + \epsilon_i \ where \mu_i = x_i \beta
        \end{equation}
      
        With $\beta$ being an unknown parameter that is to be estimated and $\epsilon_i$ error term for that row. A linear model assumes that $\epsilon_i$ are mutually independent zero mean random variables with the same variance $\sigma^2$. The linear model explains the response variable $Y$ in terms of a predictor variable $x$  multiplied by an estimated coefficient $\beta$ plus a random error term $\epsilon$. 
        
        \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{simple_linear_model}
            \caption{\cite{gamBook} visually described all relevant variables and their relationship}
            \label{fig:simple_linear_model}
        \end{figure}
        
        
        $y_i$ and $x_i$ are known but $\beta$ is unknown and thus needs to be estimated. The simple model linear model gives one approach to estimate $\beta$ from the given data $x_i$,$y_i$. The simple linear model seeks to find the $\beta$ that minimizes the squared difference between $y_i$ and $x_i \beta$. Formalizing this notion amounts to defining the least squares measure  $S$ as:
        
        \begin{equation} 
        \mathbf{S} =  \sum_{i=1}^{n}(y\textsubscript{i} - x\textsubscript{i}\beta)^{2} = \sum_{i=1}^{n}(y\textsubscript{i} - \mu\textsubscript{i})^{2} 
        \end{equation}
        Per this definition a good choice of $\beta$ minimizes the difference between $y_i$ and $\mu_i$. As $S$ converges $0$ the better of an estimated $\beta$  becomes . The problem of least square estimation becomes minimizing $S$ with respect to $\beta$. To minimize S, differentiate with respect to (w.r.t.) $\beta$.
        
        \begin{equation} \label{partialDer} \frac{\partial S}{\partial \beta} = - \sum_{i=1}^{n} 2x_i(y_i-x_i\beta) \end{equation}

       By setting the result to $0$ the best estimate for $\beta$ can be found. The best estimate for $\beta$ is called $\widehat{\beta}$.  After setting the equation to zero and rearranging term to express $\widehat{\beta}$ in terms of $x_i$ and $y_i$ the full problem of estimating $beta$ can be stated as:

        \begin{equation}  - \sum_{i=1}^{n} 2x_i(y_i-x_i\widehat{\beta}) = 0  \end{equation}

        \begin{equation} - \sum_{i=1}^{n} x_i y_i-\widehat{\beta} \sum_{i=1}^{n} x_i^2 = 0  \end{equation}

        \begin{equation} \widehat{\beta} = \sum_{i=1}^{n} x_i y_i /\sum_{i=1}^{n} x_i^2 \end{equation}


        Minimizing $S$ w.r.t. $\beta$ to compute $\widehat{\beta}$ is a reasonable approach when dealing with one explanatory variable. The Gauss-Markov theorem shows that estimating $\beta$ with $\widehat{\beta}$ is best linear unbiased estimator. However, most relevant linear models involve more explanatory variables than one. The question for finding must be account for multiple explanatory variables but the measure of fit $S$ remains.
      
        \subsection{Linear Model}
         \cite{gamBook} introduces the linear model as the generalization of the simple linear model, allowing for the response variable to be explained with multiple predictor variables. Though the linear model generalizes the simple linear model it still assumes a normal distributed $Y$
        \subsubsection{Linear Model in the vector-matrix form}
        Allowing for several predictor variables amounts to rewriting the simple linear model in terms of vectors and matrices. A major benefit of using the matrix-vector forms is that the problem of finding $\beta$ becomes the problem of solving a overdetermined system of equations. Again, given n observations of $y_i$ and $x_i$ plus some additive constant. Explicitly writing each $\mu_i = x_i \beta_i$ illustrates the shape of the system of equations.
        \begin{equation}
        \begin{align*} 
        \begin{matrix}
            \mu_1 = \beta_0 + x_1 \beta_1 \\
            \mu_2 = \beta_0 + x_2 \beta_1 \\
            ... \\
            \mu_n = \beta_0 + x_n \beta_1 \\
        \end{matrix}
        \end{align*}     
        \end{equation}
        
        For the linear model this system of equations must now be rewritten in matrix form resulting in the matrix-vector form.  The matrix-vector form for the simple linear model takes the following shape. 
        \begin{equation}
       \left[ \begin{array}{c} \mu_1 \\ \mu_2 \\ \mu_3 \\ ... \\  \mu_n\end{array} \right] = \begin{bmatrix} 1 & x_1 \\ 1 & x_2 \\ 1 & x_3 \\ ... \\1 & x_n   \end{bmatrix}  \left[ \begin{array}{c} \beta_0 \\ \beta_1 \end{array} \right]   
        \end{equation}
        
        Now, adding predictor variables and generalizing the simple linear model amounts to appending a predictor variable vector to the matrix and introducing a new coefficient to the coefficient vector.
        
        \begin{equation}
           \left[ \begin{array}{c} \mu_1 \\ \mu_2 \\ \mu_3 \\ ... \\  \mu_n\end{array} \right] = \begin{bmatrix} 1 & x_1 & x_2\\ 1 & x_2 & x_2 \\ 1 & x_3 & x_3 \\ ... \\1 & x_n & x_n   \end{bmatrix}  \left[ \begin{array}{c} \beta_0 \\ \beta_1 \\ \beta_2 \end{array} \right]   
        \end{equation}
    
        The matrix-vector form yields the general form the linear model $\mu = X\beta$. This means that the value vector $\mu$ is given by the model matrix $X$ times the unknown parameter vector $\beta$. The model matrix can account for an arbitrary amount of predictor variables by adding a column for each predictor. The nature of the predictor variable: continuous or discrete plays an important role in the construction of the model matrix. Dummy and factor encoding are the two approaches when dealing with factor variables. For an thorough text on factor encoding in linear models see \cite{regression}. The model matrix is also known as design matrix and the terms are used interchangeably. $\mu = X\beta$ is the canonical form of a linear model. The method for estimating $\beta$ for one predictor do not work for the matrix-vector form and need to adjusted for estimating the unknown vector $\beta$. 
        
        \subsubsection{Estimating Linear Models}
        \cite{gamBook} states estimating the vector of unknown parameters as finding the least squares solution to $\beta$. The computationally stablest and fastest method involves QR decomposing the model matrix to express $\widehat{\beta}$ in terms of the upper triangular matrix. This method is widely used in statistical packages and relevant for the numerically optimized version of  generalized additive model. This approach involves stats with the linear model in the full matrix-vector form:
         
         \begin{equation}
             \mu = X \beta, y \approx N(\mu, I_n, \sigma^2)
         \end{equation}
        
        The model matrix $X$ is a matrix with n rows and p columns. With n being the number of observations and p is a product of predictors and their encoding. The least squares estimation for $\beta$ is the best linear unbiased estimator with least variance, \cite{gamBook} provides a full proof. Estimating $\beta$ relies on the Euclidean length of a vector. The Euclidean length of a vector is the sum squared of its elements. For a vector v in n dimensional space the Euclidean length is defined as:
        
        \begin{equation}
            \left \| v \right \|^2 \equiv v^T v \equiv \sum_{n}^{i=1} v^2
        \end{equation}
        
        An essential fact about the Euclidean length is that the rotation a matrix does not change its length. The practical method for fitting least squares heavily leverages this fact. This also holds for the rotation of $y-X\beta$. Bootstrapping the Euclidean length to define our measure $S$ in the matrix-vector framework yields
        
        \begin{equation}
           \label{S} S =  \left \| y - \mu \right \|^2 = \| y = X\beta \|^2 
        \end{equation}
        
        \cite{gamBook} describes QR decomposition of a matrix as the essential method for estimating $\beta$ and also an essential method for finding generalized additive models. Any real matrix X can always decomposed into an orthogonal matrix and triangular matrix.
        
        \begin{equation}
        X = Q\begin{bmatrix}R \\ 0 \end{bmatrix} = Q_f R
        \end{equation}
        
        $R$ is the upper triangular matrix with p rows and p columns and $Q$ is an orthogonal matrix with n rows and n columns of which the first p columns form $Q_f$. By definition does multiplying a vector with an orthogonal matrix not change the their length. The matrix algebra appendix provide more detail on these concepts. The QR decomposition decomposes a matrix into two district matrices that have different properties but maintain their length. Multiplying an orthogonal matrix with $S$ does not change the length. Applying the QR decomposition of the model matrix in \ref{S} yields in:
        
        \begin{equation}
             \left \| y - X\beta^2\right \|  =  \left \| Q^Ty - Q^T X\beta^2\right  \| =  \| Q^T-y \begin{bmatrix} R \\ 0 \end{bmatrix} \beta \|^2
        \end{equation}
        
        Only the orthogonal matrix of the QR decomposed model matrix gets multiplies to the the response variable vector and the original model matrix. Multiplying $Q^T$ with the response vector can be stated as $Q^T y = \begin{bmatrix} f \\ r \end{bmatrix}$. Where $f$ is vector of p dimensions and hence r is a vector of  $n-p$ dimension.
        
        \begin{equation}
            \left \| y - X\beta\right \|^2  =  \left \| \begin{bmatrix} f \\ r \end{bmatrix} - \begin{bmatrix} R \\ 0 \end{bmatrix}\beta\right \|^2 =  \| f-R \beta \|^2 + \|r \|^2
        \end{equation}
        
        This complicated form exposes the residual error $r$ as independent of $\beta$. $\| f - R \beta \|^2$ can be reduced to zero by choosing $\beta$ so that $R\beta$ equals $f$, thus the estimator $\widehat{\beta}$ can be stated as 
        
        \begin{equation}
            \widehat{\beta} = R^{-1} f
        \end{equation}
        
        The reducible error, also called residual error is the difference between the model matrix multiplies with the estimated $\beta$ minus the value for y. $\|r\|^2 = \|y-X \widehat{\beta\x}\|^2$. 
        
    \subsubsection{Influence Matrix}
    The hat or influence matrix is the projection matrix that maps the $y$ vector into the column space of $X$. Given a design matrix $X$ with n rows and p columns and $n > p$. The $y$ vector cannot lie in the column space of X. The hat matrix is the projection of $y$ into the column space of $X$. Since the projection is an approximation the matrix is called hat matrix. The hat matrix is required for the prediction piece of linear models and plays an important role for generalized additive models. \cite{gamBook} introduces the influence matrix as the matrix that yields the predicted fitted value vector $\widehat{\mu}$. To obtain $\widehat{\mu} $the hat matrix is post multiplied with the data vector $y$. The influence matrix is required to compute the estimated value vector. The first p columns of Q make up the matrix $Q_f$. $f$ is then defined as $f = Q_f y$ which implies that 
    
    \begin{equation}
        \widehat{\beta} = R^{-1} Q_f^T y 
    \end{equation}
    
    Given that $\widehat{\mu} = X \widehat{\beta}$ and $X = Q_fR$ the fitted value vector can be written as 
    \begin{equation}
        \widehat{\mu} = Q_f RR^-1 y = Q_fQ_f^T_f y  
    \end{equation}
    Since the upper triangular matrix $R$ is multiplied its inverse we can factor them out resulting with the hat matrix $A \equiv Q_f Q_f^T$ that fulfills $\widehat{\mu} = Ay$.  The hat matrix will essential in cross validating the estimated $\widehat{y}$ values. Generalized additive models will iteratively minimize the distance difference between the $\widehat{y}$ and $y$ . Details will be given in later sections.
    
    \subsubsection{Simple linear model example}
        
        
    \subsection{Generalized Linear Model}
    The generalized linear model (GLMs) is an extension to the general linear model, allowing the response variable $Y$ to be of any exponential family distributions. The exponential family of distributions contains many practical useful distributions including Poisson, bionomial, gamma and normal distributions. A formal description of it's basis structure can be given as:

        \begin{equation} g(\mu_i) = X_i \beta_i \end{equation}


   $X_i$ is the $i^th$ row of a model matrix X and $\beta$ is a vector of unknowns parameters with $\mu_i \equiv E(Y_i)$ and $Y_i$ is distributed according to some exponential family. Every exponential family distribution has a canonical link function $g()$. The link function allows to estimate the linear functions of the predictor variables by transforming the right side of the equation $f(x)$. The data are then fit in this transformed scale but the expected variance is calculated on the original scale of the predictor variables. The link functions is essential in modelling exponential functions in linear terms. Fitting several distributions with a single framework at a cost: While the least squares approach was sufficient for estimating $\beta$ for normal distributed data with zero mean error the generalized estimation method has to account for an arbitrary amount of distribution parameters. \cite{glm} introduces iterative reweighted least squares (IRLS) as a method to obtain maximum likelihood estimates (MLE) for a particular exponential family distribution. IRLS is the generalization of the least squares approach from normal distribution to any exponential family distribution. A practical feature of GLMs is that they can all be fitted using IRLS, independent of response variable distribution. 
 
    \subsubsection{Maximum-Likelihood Estimation}
    
    \cite{glm} describes the principle of maximum likelihood estimation as searching for probability distribution parameters that makes the observed data most likely. This results in a search for the parameter vector that maximizes the likelihood function $L(\beta|y)$. The parameter vector is found by searching the multi-dimensional parameter space.

    \cite{MLE} describes MLE as finding the parameters for a distribution that were most likely to produce the given data. Random variable are defined in terms of one or several parameters. A good choice of parameters results in a distribution that emulates the given data. The least squares linear model estimated the $\mu$ and $\sigma$ for $N( \mu ,\sigma^2)$ by minimizing $S$ is a special case of the MLE, only allowing for normal distributed data. \cite{glm} introduces a method to account estimate parameters for any exponential family distributions members. Exponential family distributions have arbitrary number of parameters. Examle for these parameters are the Poisson distribution  $P(\lambda)$  and gamma distribution $G(K,\theta)$. Maximum likelihood estimation provides a single framework to allow parameter estimation for any of the exponential family distributions.

    From a statistical analysis point of view the vector $y$ of observed data is a random sample from an unknown population. The goal of maximum likelihood estimation is to find the parameters of the given distribution that most likely have produced this sample. This process is described with a probability density function (PDF) $f()$ of observed data $y$ given a parameter $\beta$: $f(y|\beta)$. If individual observations, $y_i$‚Äôs, are statistically independent of one another, the PDF for the data $y$ given the parameter vector $\beta$ can be expressed as a multiplication of PDFs for individual observations.

    \begin{equation} f(y|\beta) = f((y_1, y_2,...,y_n) | (\beta_1,\beta_2, ... \beta_n )) = \prod_{i=1}^{n} f_i(y_i|\beta_i) ) \end{equation}

    Given a set of parameter values, the corresponding PDF will show that some data are more probable than other data.  However, the data is already given and we are searching for the parameters of the distribution that most likely produced the data. The casual term most likely is referring to maximizing likelihood. We thus inverse our function $f(y|\beta)$ to $L(\beta|y)$ to produce the likelihood of $y$ given the parameters $\beta$. Theoretically MLE need not exist nor be unique. But if they exist and are unique they can be estimated. For computational convenience, the MLE estimate is obtained by maximizing the log-likelihood function, $ln(L(\beta|y))$: This is because the two functions, $ln(L(\beta|y))$ and $L(w|y)$; are monotonically related to each other so the same MLE estimate is obtained by maximizing either one and the log-likelihood is preferred for obvious reasons. Assuming that the log-likelihood function, $ln(L(\beta|y))$ is differentiable, if $\beta$ exists, it must satisfy the following partial differential equation known as the likelihood equation:

    \begin{equation} \frac{\partial ln L(\beta|y)}{\partial \beta_i} = 0 \end{equation}

    This properties are given because the definition of maximum or minimum of a continuous differentiable function implies that its first derivatives vanish at such points. The likelihood equation represents a necessary condition for the existence of an MLE estimate. An additional condition must also be satisfied to ensure that $ln(L(\beta|y))$ is a maximum and not a minimum, since the first derivative cannot reveal this. The log likelihood must be convex near w. This is verified by calculating the second derivatives of the log-likelihoods and checking if they are negative.

    \begin{equation} \frac{\partial^2 ln L(\beta|y)}{\partial \beta^2_i} < 0 \end{equation}

    This form is only of theoretical value as practical model often involves many parameters and have highly non-linear PDFs. Thus the estimate must be sought numerically using nonlinear optimization methods. The basic idea for these methods is to find optimal parameters that maximize the log-likelihood focusing on smaller sub-sets of the multi-dimensional parameter space. This is the preferred because exhaustively searching the whole parameter space becomes intractable with an increasing amount of parameters. The practical method for MLE searches by trial and error over the course of a series of iterative steps. Each iteration changes the results from the previous iteration by a small value. The choice of the value is tailored to improve the log-likelihood. \cite{glm} introduces a metthod called iterative reweighted least squares and is subject in the next section.
        
        \subsubsection{Fitting Generalized Linear Models}
    
    \cite{gamBook} introduces iterative reweighted least squares (IRLS) as the practical method for fitting GLMs. The method method must find the distribution parameters and account for unequal variance. GLM models an n-vector of independent response variables $Y$, where $\mu$ is the expected value of $Y$ as:
    
    \begin{equation}
        g(\mu_i) = X_i \beta
    \end{equation}
    
    With $\mu \equiv E(Y)$ and the link transformation of $Y_i$ as $Y_i \approx f_\theta_i (y_i)$ where $f_\theta_i$ stands for the canonical link transformation of an exponential family distribution. The value of $mu_i$ is the to determined by finding $\beta$. With this in mind the probability mass function for any exponential family distribution can be written as:
    
    \begin{equation}
        f_\theta = exp[\{y\theta-b(\theta)\}/a(\phi)+c(y, \phi)]
    \end{equation}
    
    $a,b$ and $c$ are arbitrary functions, $\phi$ an arbitrary scale parameter and the $\theta$ the canonical link parameters. The interesting property of this form is that the general expression for mean and variance of exponential family distributions can be expressed in terms of $a,b$ and $\phi$. The mean and the variance are the required parameters to fit any exponential family distribution and are computed via deriving the log-likelihood. Since the natural logarithm is the inverse of the exponential function the log-likelihood has a familiar form.
    
    \begin{equation}
        l(\theta) = [y\theta - b(\theta)] / a(\phi) + c(y,\phi) 
    \end{equation}
    
    $E(Y)$ can now computing by differentiating $l$ w.r.t. $\theta$ and treating $l$ as a random variable. This amounts to replacing the particular value $y$ with the random variable $Y$.
    
    \begin{equation}
        \frac{\partial l }{\partial \theta} = [y-{b}'(\theta))]/a(\phi))    
    \end{equation}
    
    \begin{equation}
        E(\frac{\partial l }{\partial \theta}) = [E(Y)-{b}'(\theta))]/a(\phi))
    \end{equation}
    
    Given that $E(\partial l / \partial \theta) = 0$ the expected value of the random variable Y can be stated as:
    
    \begin{equation}
        E(Y) = {b}'(\theta)
    \end{equation}
    
    This results in a method to compute the mean of any exponential family random variable by the first derivative of b w.r.t $\theta$. This the direct link between the $\beta$ and the model parameter. Allowing the parameter $\beta$ to determine the mean of the response variable and consequentially the canonical parameter is the proper way to find that value. The variance of that random variable can be computed from the second derivative of the log-likelihood:
    
    \begin{equation}
        \frac{\partial^2 l }{\partial^2 \theta} = -{b}''(\theta)/a(\phi)
    \end{equation}
    
    \cite{gamBook} states that re-arranging this formula yields the equation for the variance of $Y$.
    
    \begin{equation}
        var(Y) = {b}''(\theta) a(\phi)
    \end{equation}
    
    A very important feature of GLMs is that the variance can vary. To account for varying variance the variance is divided by a weight $w$ that penalizes data points with high relative variance.
    
    \begin{equation}
        var(Y) = {b}''(\theta) a(\phi)/w
    \end{equation}
    
    For simplicity the function $V(\mu)$ is defined as $V(\mu) = {b}''(\theta)$ such that $var(Y) = V\(\mu)$.
    
    Given vector $y$ of an observation random variable $Y$ the maximum likelihood estimation of $\beta$ is possible due to the independence of all $Y_i$. The formal notation for the MLE of the likelihood of $\beta$, $L(\beta)$ becomes
    
    \begin{equation}
        L(\beta) \prod_{i=1}^{n} f_\theta_i (y_i)
    \end{equation}
    
    Given the previously stated the probability mass function for exponential family members the log likelihood $l()$ of $\beta$ can be written as:
    
    \begin{equation}
        l(\beta) = \sum_{i=1}^{n} log[f_\theta_i(y_i))] = \sum_{i=1}^{n} (y_i\theta_i-b_i(\theta_i)) /a_i(\phi)+c_i(y_i, \phi)
    \end{equation}
    
    The log function transforms the product to a sum. $\phi$ is assumed to be constant for all $i$ and \cite{gamBook} states that only choices for $\phi$ are relevant that can be stated as $a_i(\phi) = \phi / w_i$ with $w_i$ being a constant. This assumption allows us the rewrite the previous formula as
    
    \begin{equation}
        l(\beta) = \sum_{i=1}^{n}\ w_i(y_i\theta_i-b_i(\theta_i)\)/a_i(\phi)+c_i(y_i, \phi)
    \end{equation}
    
    The process of finding the parameter vector $\beta$ amounts to maximizing the log-likelihood by partially differentiating $l$ w.r.t. each element of $\beta$, setting the resulting expressions to zero and solving for $\beta$.
    
    
    \begin{equation}
    \frac{\partial l }{\partial \beta_j} = 
     \sum_{i=1}^{n} w_i \begin{pmatrix} y_i \frac{\partial \theta_i }{\partial \beta_j} - {b}'(\theta_i) \frac{\partial \theta_i }{\partial \beta_j} \end{pmatrix}
    \end{equation}
    
    \cite{gamBook} details that applying the chain rule,  $E(Y) = {b}'(\theta)$ and term substitution the following form can be produced:
    
    \begin{equation}
        \frac{\partial l }{\partial \beta_j} = \frac{1}{\phi} \sum_{i=1}^{n} \frac{[y_i - {b_i}'(\theta_i)]}{b''_i(\theta_i)/w_i} \frac{\partial \mu_i }{\partial \beta_j}
    \end{equation}
    
    Applying the framework developed above to determine variance and expected value the full form for the estimation of $\beta$ can be stated as:
    
    \begin{equation}
         \sum_{i=1}^{n} \frac{(y_i - \mu_i)^2}{V(\mu_i)} \frac{\partial \mu_i }{\partial \beta_j} = 0 \ \forall \ j.
    \end{equation}
    
    This matches exactly the same equation that would have to be solved to find $\beta$ by non-linear weighted least squares if the weights for $(V(\mu_i)$ were known in advance and were independent of $\beta$. In that scenario the least square objective would be
    
      \begin{equation}
          S = \sum_{i=1}^{n} \frac{(y_i - \mu_i)^2}{V(\mu_i)}
      \end{equation}
    
    In the formulation $\mu_i$ depends non-linearly on $\beta$ but the weights $V(\mu)$ are treated as fixed. To find the least squares estimated $\partial S / \partial \beta_j$ must equal to zero for all js. Formulating the search for $\beta$ as finding the optimal choice of distribution parameters of $Y$ invites an iterative approach that chooses some parameter values and e .
    
    \subsubsection{Iterative Reweighted Least Squares}
    \cite{gamBook} describes this process formally to introduce the iterative reweighted least squares algorithm. Let $\widehat{\beta}^{[k]}$ be the estimated parameter vector at the $k^{th}$ iteration. $\eta^{[k]}$ is a vector with the elements $\eta^{[k]}=X_i \widehat{\beta}^{[k]}$ and $\mu_{i}^{[k]}}$ is defined as inverse of the link function. $\mu_{i}^{[k]} = g^{-1}(\eta_{i}^{[k]}$. Given these definitions the algorithm can be stated.
    
    \begin{enumerate}
   \item Compute the variance weighted variance $V(\mu_{i}^{[k]})$ terms implied by the current estimate for $\widehat{\beta}^{[k]}$
   \item Use these estimates and apply the method describe to minimize the least square objective with respect to $\beta$ to obtain $\widehat{\beta}^{[k]}$
   \item Set k to k+1
   \end{enumerate}
    
    To come to a computational formulation the least square objective for IRLS can be written as 
    
    \begin{equation}
        S = \left \| \sqrt{W}^{[k]}  \left ( z^{[k]} - X\beta \right )  \right \|^2
    \end{equation}
    
    Where $z^{[k]$ is so called "pseudodata" and $W}^{[k]]$  the diagonal weight matrix, each defined as
    
    \begin{equation}
        z_{i}^{[k]} = {g}'(\mu^{[k]})(y_i - \mu_i^{[k]} + \eta_i^{[k]}
    \end{equation}
    
    \begin{equation}
        W_{ii}^{[k]} = \frac{1} { V(\mu_{i}^{[k]}) \mu_{i}^{[k]}}
    \end{equation}

    With these definition we can finally write the full form of the practically used IRLS.
    \begin{enumerate}
        \item Use current $\eta^{[k]}$ and $\mu^{[k]}$ to calculate calculate pseudo data $z^{[k]}$ and iterative weights for the weights matrix $W^{[k]}$
        \item Minimize the least squares objective $\left \| \sqrt{W}^{[k]}  \left ( z^{[k]} - X\beta \right )  \right \|^2$ w.r.t. $\beta$ to obtain  $\widehat{\beta}^{[k+1]}$ and the resulting $\eta^{[k+1]}=X_i \widehat{\beta}^{[k+1]}$ and $\mu_{i}^{[k+1]}}$
        \item Set k to k + 1 
    \end{enumerate}
        
        The proposed method will converge on the optimal parameter vector $\widehat{\beta}$
        \subsubsection{Generalized Linear Model Example}
    
    \section{Generalized Additive Models}
    Generalized additive model (GAM) allow predictor variables of GLM to be estimated with non-parametric methods. GAM has the interpretability advantages of GLMs where the contribution of each predictor variable to the function is clearly encoded. However, it has substantially more flexibility because the relationships between predictor and response variable are not assumed to be linear. GAMs assume the relationship between predictor in response variable to be the sum of functions. These functions are regularized by enforcing a smooth patter. These smooth functions can be estimated with a scatter plot smoother. \cite{gam} introduced GAMs and described many approaches to scatter plot smoothing. For practicability and applicability this thesis will focus on regression splines, especially cubic splines and cyclic cubic splines. \cite{gamBook} formally describe GAMs as:
    
    \begin{equation}
        g(\mu_i) = X_i^* \theta + f_1(x_1i) + f_2(x_2i) + ... + f_3(x_3i)
    \end{equation}
    
    GAM inherit the link function $g()$, parameter vector $\theta$ and the model matrix $X_i^*$ from the GLM. Following the GLM definition $\mu \equiv E(Y_i)$ and $Y$ is some exponential family distribution. $X_i^*$ is the ith row of the model matrix. GAM introduce smooth functions $f_j$ over the the predictor variables $x_k$. Specifying a model in terms of an non-parametric, smooth functions allow $x_k$ to have an arbitrary pattern. Allowing $f_j(x_k)$ to follow any shape can give insight into response variable behavior that the parametric form GLMs fail to capture. The gained flexibility comes with the question of how to find these smooth functions.
    
    \subsection{Smooth functions}
    Smooth functions are any function that explains $x_i$ in terms of $y_i$ with some error $\epsilon$. The error term  is a random variable that is independent, identically distributed with $N(0, \sigma^2)$. This is identical to the definition of non-parametric methods from the first section of this thesis. Formally we are searching for function $f()$ that satisfies:
    
    \begin{equation}
        y_i = f(x_i) + \epsilon_i
    \end{equation}
    
    Bootstraping the methods used for fitting a simple linear model we assume the the function $f$ to be linear in $x_i$. This assumption is guarantees that the function can be found with linear parametric methods. \cite{gamBook} proposes to choose a basis function to represent function $f$. The basis function allows to represent the function $f$ as a combination of a basis function and a parameter vector $\beta$. The function $f$ can thus be written as:
    
    \begin{equation}
        f(x) = \sum_{i=1}^{q} b_i(x)\beta_i
    \end{equation}

    The function is $f$ is represented as the sum of basis functions times the parameter vector. The sum goes up the basis dimension which is determined by the basis. The influence of the $\beta$ is linear combination, satisfying the linearity condition.\cite{hastie} states that the basis over the entire range of the data would amount to a polynomial regression. Polynomial regression can be useful but suffer from instability at the edges and insufficient for interpolation. The suitable alternative is splines estimation. Splines divide the the unknown function into sections. Each section is then fit with an individual polynomial. Each piecewise polynomial is required to be continuous at the intersection with the adjacent piecewise polynomial. Basis splines find the piecewise polynomial through a linear combination of function basis and parameters $\beta$. This approach gives enough flexibility while satisfying the linearity constraint.
    
    \subsection{Basis Splines}
    Splines constitute an ideal method for the nonparametric estimation of any unknown function. Basis splines (B-Splines) are of particular interest since they are very easy to set up satisfy in the linearity condition. Their linear nature allows to fit them via the simple model from sections 1. B-Splines consists of two parts: the knots and the functions. A B-spline of order n is a piecewise polynomial function of degree $< n$ in a variable x. Each polynomial is of degree $<n$ \cite{splines} states that B-splines are defined by their order m and number of interior knots N. There are two endpoints which are themselves knots so the total number of knots will be $N + 2$ . The degree of the B-spline polynomial will be the spline order m minus one. The degree n of a B-Splines is thus $m-1$. The knots are formally described as a vector in non-descending order with N interior knots and knots at the endpoints.
    
    \begin{equation}
        t_0 \leq  t_1 \leq ..  t_N \leq t_N+1
    \end{equation}
    
    The location and the number of knots are a matter of design and impact the resulting basis function. The B-Spline function is unique for any given sequence of knots. The B-Spline function is the $b()$ of the previous section. \cite{hastie01statisticallearning} states that for the basis function given a sequence of knots and x values can be given in the Haar form. The Haar form allows for a short, recursive definition of the basis function. 
    
    \begin{equation}
        b_{i,1}(x) := \begin{cases}
 & 1 \text{ if } t_i \leq x \leq t_i+1 \\ 
    & 0 \text{ otherwise}\\ 
        \end{cases}
    \end{equation}
    
    \begin{equation}
           b_{i,k}(x) := \frac{x-t_i}{t_{i+k-1} - t_i} b_{i,k-1}(x) + \frac{t_{i+k}-x}{t_{i+k}-t_{i+1}}b_{i+1,k-1}(x)
           \label{haar}
    \end{equation}
            
    The B-Spline basis is strictly local function. For a given basis order of m+2 the basis function is only non-zero between the m+3 knots. Each basis function is only non-zero over the intervals between m+3 adjacent knots. \cite{regressionspline} provides an insightful example of a B-Spline. B-Splines cover a broad range of basis functions and offer flexibility. Specifying an order for the basis am
            
            
    \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{bsplines}
            \caption{\cite{regressionspline} gives a visual example of a basis spline.}
            \label{fig:bsplines}
    \end{figure}
    
    The figure on the left is the basis for the given x values and the amended knots. The figure on the right is the resulting B-Spline.
    
    \subsubsection{Fitting Cubic Splines}
    A cubic basis is a reasonable choice for a B-Spline. Cubic splines B-Splines of order 4 with the additional condition that the first and second derivative are equal at the knot location. \cite{gamBook} illustrates the process of fitting a cubic spline to a data set with the predictor variable $x$ and response $y$. For this example the author chooses a specific cubic splines basis called rk. The function rk defines a cubic cyclic basis given a vector of values x and the relevant knots xk. The basis a possible cubic spline choice and introduced by \cite{gamBook}. With the function rk the model matrix to find a cubic spline can be written.  
    The function spl.X sets up the model matrix to fit a cubic spline. The function takes the data vector x and knot vector xk to produce the full model matrix. 
    <<echo=FALSE, cache=TRUE>>=
    rk<-function(x,z) # R(x,z) for cubic spline on [0,1] 
    { ((z-0.5)^2-1/12)*((x-0.5)^2-1/12)/4-
    ((abs(x-z)-0.5)^4-(abs(x-z)-0.5)^2/2+7/240)/24
    }
    @
     <<>>=
     spl.X<-function(x,xk)
            # set up model matrix for cubic penalized regression spline    
        {   q<-length(xk)+2             # number of parameters
            n<-length(x)                # number of data
            X<-matrix(1,n,q)            # initialized model matrix
            X[,2]<-x                    # set second column to x
            X[,3:q]<-outer(x,xk,FUN=rk) # and remaining to R(x,xk) X
        }
     @
     
     The rows of the model matrix are determined by the number of elements in the data vector x. The first two columns of the model matrix are for encoding and of less interest for the thesis. The remaining elements of the model matrix are computing through the basis function rk. Given that the splines were designed to be linear in the unknown parameter $\beta$ the splines can be estimated with the methods introduced in section 1. The ability to fit an arbitrary function by defining the basis and solving the resulting matrix with a linear method is a key aspect of regression splines. To fit the spline we estimate the coefficients from the model matrix with a linear model. The exact fitting happens via the QR Decomposition described in section 1. The estimated coefficients get multiplied by the same design matrix that produced the coefficients to estimates fitted values. This process essentially fits the function to the data that was used to estimate the coefficients. 
     
     <<>>=
        
        size<-c(1.42,1.58,1.78,1.99,1.99,1.99,2.13,2.13,2.13, 2.32,2.32,2.32,2.32,2.32,2.43,2.43,2.78,2.98,2.98) 
        wear<-c(4.0,4.2,2.5,2.6,2.8,2.4,3.2,2.4,2.6,4.8,2.9, 3.8,3.0,2.7,3.1,3.3,3.0,2.8,1.7) 
        x<-size-min(size)
        x<-x/max(x)
        plot(x,wear,xlab="Scaled engine size",ylab="Wear index")
        xk<-1:4/5 # choose some knots 
        X<-spl.X(x,xk) # generate model matrix 
        mod.1<-lm(wear~X-1) # fit model with out the first column
        xp<-0:100/100 # x values for prediction 
        Xp<-spl.X(xp,xk) # prediction matrix 
        lines(xp,Xp%*%coef(mod.1)) # plot fitted spline
     @
    
    The resulting plot gives a good idea of the estimated method $f()$. The choice of basis dimensions, $q = knots+2$ was arbitrary but highly influential for the function $f()$. The next section introduces theory to justify the choice of basis dimension and a way of penalizing over fit of the data.
    \subsubsection{Penalized Cubic Splines}
    
    Smoothing cubic splines seek to find the smoothest interpolating spline by penalizing the 'wigglynes". \cite{gamBook} argues that the choice of spline basis order does not suffice to control the smoothness of the resulting spline. The author goes on to introduce the penalized regression spline, specifically the penalized cubic spline. Penalized cubic splines introduce a smoothing parameter that penalizes the over-fitting of data. Over-fitting has been stated as a major weakness of non-parametric methods in chapter one, the penalized regression spline is a compromise. As stated the subject of fitting a function is to find a parameter vector $\beta$ that minimizes the following equation.
    
    \begin{equation}
        \left \| y- X\beta \right \| ^2
    \end{equation}
    
    To penalize an hectic function \cite{gamBook} introduce as penalty term $\lambda$. The penalty term $\lambda$ weights the second derivation of the estimated function $f()$. Broadly speaking the second derivative of a function represents the acceleration or the mentioned "wigglyness" of function. With the penalty term the new subject of estimation becomes:
    
    \begin{equation}
        \left \| y- X\beta \right \| ^2 + \lambda\int_{0}^{1} (f^{''}(x))^2) dx
    \end{equation}
    
    The trade-off between fitting all x values and a smooth function is controlled by the penalty term $\lambda$. Since the penalty term weights the smoothness criteria it is also called smoothing parameter. The choice of $\lambda$ is crucial to the resulting function. While a $\lambda$ of 0 creates a function that will directly pass each x data point. A high lambda value will over penalize each acceleration of the function $f$ and generate a straight line. Since the estimated function $f$ is linear in $\beta$ the subject of estimation can be rewritten as:
    
    \begin{equation}
        \left \| y- X\beta \right \| ^2 + \lambda \beta^T S \beta
    \end{equation}
    
    Tha matrix $S$ is a penalty matrix that is specific to the chosen basis. The penalty matrix is a diagonal matrix with the penalty values adjacent to the to be penalized term. The problem of estimating the penalized regression spline is to minimize the just stated equation with respect to $\beta$ and to estimate $\lambda$. Estimating $\lambda$ will be discussed in the next section. For computationally stable fitting of regression splines the above formula can be rewritten as the following:
    
    \begin{equation}
        \label{penalty}
        \left \| y- X\beta \right \| ^2 + \lambda \beta^T S \beta = \left\|  \begin{bmatrix} y\\ 0
\end{bmatrix} - \begin{bmatrix} X\\ \sqrt{\lambda} B \end{bmatrix} \beta \right\| ^2
    \end{equation}
    
    \cite{gamBook} states the the penalty matrix $S$ for a given basis can be written as its square root $B$. Any symetric matrix can be decomposed into the following form. $B = \sqrt{S}$. The model matrix $X$ has been augmented with the square root of the penalty matrix S time the square root of $\lambda$. Since the X matrix gets augmented the vector of $y$ values needs to be agumented as well. Since we still are using linear models the number of elements in the $y$ vector must match the number of rows in the augmented $X$ matrix. A simple square root of a matrix can be written as
    
    <<echo=FALSE, cache=TRUE>>=
     mat.sqrt<-function(S) # A simple matrix square root 
     {  d<-eigen(S,symmetric=TRUE)
        rS<-d$vectors%*%diag(d$values^0.5)%*%t(d$vectors)
     }
    @
    
    The penalty matrix is specific to each basis function. For the author's choice of the rk basis the matrix S is can be created by forming outer knot product with the basis rk. The resulting matrix $S$ is the specific penalty matrix for a vectors of knots and a basis.
     
    <<>>=
    spl.S<-function(xk)
    # set up the penalized regression spline penalty matrix,
    # given knot sequence xk
    {   q<-length(xk)+2
        S<-matrix(0,q,q)                # initialize matrix to 0
        S[3:q,3:q]<-outer(xk,xk,FUN=rk) # fill in non-zero part
        S 
    }
    @
    
    With function for the model matrix X, the penalty matrix S and the ability to take a square root of a matrix a penalized cubic spline can be found by constructing the full matrix according to \ref{penalty}. 
    <<>>=
    prs.fit<-function(y,x,x_knots,lambda)
    # function to fit penalized regression spline to x,y data, 
    # with knots x_knots, given smoothing parameter, lambda.
    {   q<-length(x_knots)+2                # dimension of basis 
        n<-length(x)                        # number of data
        # create augmented model matrix
        Xa <- rbind(spl.X(x,x_knots),mat.sqrt(spl.S(x_knots))*sqrt(lambda)) 
        y[(n+1):(n+q)]<-0               # augment the data vector
        lm(y ~ Xa-1)                    # fit penalized regression spline with a linear model
    }
    @
    
    The prs.Fit functiion estimated a penalized regression spline for a given vector $y$ of response variables, a vector $x$ of predictor variables, a choice of knots $x_knots$ and a given smoothing parameter $\lambda$. The method then forms $X$, $B$, $y$ and $\sqrt{\lambda}$ according to \ref{penalty}. The formed system of equation gets solved via the linear model. The resulting estimated model contains the estimated coefficients. The proposed function can be used to estimated a penalized regression spline for the wear data set.
    
    <<>>=
    x_knots <-1:7/8 # choose some knots 
    lambda <- 0.0001
    mod.2 <- prs.fit(wear,x,x_knots, lambda) # fit pen. reg. spline 
    Xp<-spl.X(xp,x_knots) # matrix to map params to fitted values at xp 
    plot(x,wear);lines(xp,Xp%*%coef(mod.2)) # plot data & spl. fit
    @
    
    From the function prs.fit and \ref{penalty} it should be become evident that the choice of $\lambda$ has a significant influence on the shape of $f$. The figure below illustrates the influence of $\lambda$ on the shape of $f$
    
    
    \begin{figure}
            \centering
            \includegraphics[width=\textwidth]{lambda}
            \caption{\cite{gamBook} states the influence of $\lambda$ on the resulting function $f$.}
            \label{fig:lambda}
        \end{figure}
    
    \cite{gamBook} states that influence of $\lambda$ is significant enough that it should be estimated individually. How to estimate $\lambda$ is discussed in the next section.
    
    \subsubsection{Smoothing Parameter Estimation}
    The previous section and figure \ref{fig:lambda} in particular illustrated the importance of the smoothing parameter $\lambda$. A high value for $\lambda$ will cause an over smoothing while a low value will under smooth the data. Too high or too low, a bad choice for $\lambda$ will result in a spline $\widehat{f}$ that is far from the original unknown function $f$. An ideal $\lambda$ would have a small distance between $f$ and $\widehat{f}$ A measure $M$ is defined to capture this notion.
    
    \begin{equation}
        M = \frac{1}{n} \sum_{i=1}^{n} (\widehat{f}_i - f_i)^2
    \end{equation}
    
    \cite{gamBook} uses the following notation for the following equations. $\widehat{f}_i \equiv \widehat{f}(x_i)$ and  $f_i \equiv f(x_i)$. A suitable criterion to estimate $\lambda$ is thus to minimize $M$. However, since the true function $f$ is known $M$ cannot be estimated but it is possible to estimate the squared error $E(M)+\sigma^2$. \cite{gamBook} proposes to find $\lambda$ through cross validation. Given $\widehat{f}^{-i}$ is the estimated function on all data except $y_i$ a ordinary cross validation can given as:
    
    \begin{equation}
        V_o = \frac{1}{n} \sum_{i=1}^{n} (\widehat{f}^{-i} - y_i)^2
    \end{equation}
    
    This score results from leaving out each datum in turn, fitting the model to the remaining data and calculating the squared difference between the missing datum and its predicted value: these squared differences are then averaged over all the data. It should be obvious that the process of refitting the function for each $y$ value is $O(n^2)$. \cite{gamBook} states that the ordinary cross validation score can be defined in terms of the influence matrix.
    
    \begin{equation}
        V_o = \frac{1}{n} \sum_{i=1}^{n} (y_i -\widehat{f}_i)^2 /(1-A_{ii})^2
    \end{equation}
    
    The author thus proposes the generalized cross validation score (gcv) to avoid the computational over head. Computing the cross validation score through the hat matrix allows to validate without forming each possible function for each $y$ value. The computational most stable version of the gcv score replaces the weights $1-A_{ii}$ by the mean weight $tr(I-A)/n$. For a detailed discussion of the gcv see \cite{wahba}. 
    
    \begin{equation}
        V_g = \frac{n \sum_{i=1}^{n} (y_i - \widehat{f}_i )^2}{tr(I-A)^2} 
    \end{equation}
    
    This section will use this definition to compute the gcv score for a given function and the given hat matrix. With this the definition a simple loop can be written to find the best possible $\lambda$. The model producing the smallest gcv score generates the smoothest possible cubic spline for the given data set.
    <<>>=
    lambda<-1e-8
    n<-length(wear)
    V<-0
    for (i in 1:60) # loop through smoothing parameters 
        { 
            mod<-prs.fit(wear,x,xk,lambda)          # fit model, given lambda
            trA<-sum(influence(mod)$hat[1:n])       # find tr(A) 
            rss<-sum((wear-fitted(mod)[1:n])^2)     # residual sum of squares 
            V[i]<-n*rss/(n-trA)^2                   # obtain GCV score 
            lambda<-lambda*1.5                      # increase lambda
        }
    plot(1:60,V,type="l",main="GCV score",xlab="i")     # plot score
    i<-(1:60)[V==min(V)]                             # extract index of min(V) 
    mod.3<-prs.fit(wear,x,xk,1.5^(i-1)*1e-8) # fit optimal model 
    Xp<-spl.X(xp,xk) # .... and plot it
    plot(x,wear)
    lines(xp,Xp%*%coef(mod.3))
    @
    
    This section introduced the gcv score as a measure to evaluate the fit of a penalized regression spline. The gcv score is the central metric in evaluating choices for $\lambda$ and will be used the the following sections to evaluate generalized additive models.
    
    \subsection{Additive Model}
    The discussed methods allow to fit a regression spline to a single variable. Almost all relevant model have more than one variable. The additive model allows to model the response as the sum of several predictors. \cite{gamBook} formally describes the additive model as:
    
    \begin{equation}
        y_i = f_1(x_i) + f_2(z_i) ... f_n(w_i)  + \epsilon_i
    \end{equation}
    
    The functions $f_j$ are smooth functions estimated with the methods introduced in the previous sections. The error term $\epsilon$ is distributed according to an independent identically distributed $N(0,\sigma^2)$. Modelling $y_i$ as the sum of individual smoothing functions rather than a single function of all terms imposes a very strong condition. $f_1(x) + f_2(z)$ is a special case of the of the general smooth function of both predictors $f(x,z)$. The benefit of modeling each function individually is that each predictor maintains the interpretability of the linear model. Estimating $f(x,z)$ would provide superior flexibility but drastically decreased interpretable. The individual smooth function are a major benefit of the additive model. Fitting additive model with by the methods used for a single regression spline amounts to rewriting the penalty term. Instead of penalizing a single spline both splines no need to be penalized. The unknown parameter vector $\beta$ for the additve model can be estimated by the minimization of the penalized least squares objective.
    
    \begin{equation}
        \left \| y- X\beta \right \| ^2 + \lambda_1 \beta^T S_1 \beta + \lambda_2 \beta^T S_2 \beta
    \end{equation}
    
    Each smoothing function gets estimated with an individual penalty matrix $S_i$ and a smoothing parameter $\lambda_i$. The additive nature of the model allows to write the combined penalty as $S \equiv \lambda_1 S_1 + \lambda_2 S_3$. The ability to rewrite the penalty matrix as the addition of the individual penalty matrices and smoothing parameter allows to rewrite the least squares objective for computation.
    
    \begin{equation}
        \label{penalty}
        \left \| y- X\beta \right \| ^2 +  \beta^T S \beta = \left\|  \begin{bmatrix} y\\ 0
        \end{bmatrix} - \begin{bmatrix} X\\  B \end{bmatrix} \beta \right\| ^2
    \end{equation}
    
    Similar to the single smoothing function the $B$ is any square root matrix such that $B^TB=S$. Summing the different $\lambda$ and $S$ terms in to the square root matrix allow this model to be estimated with the standard linear model. To illustrate an additive model \cite{gamBook} defines an additive model in two steps. One method to set up the model and the penalty matrices. The second the estimate the models with  smoothing parameters. 
     <<>>=
    am.setup<-function(x,z,q=10)
        # Get X, S_1 and S_2 for a simple 2 term AM 
        { 
        # generate equidistant knots
        xk <- quantile(unique(x),1:(q-2)/(q-1))
        zk <- quantile(unique(z),1:(q-2)/(q-1))
        # Generate two individual, non-overlaping penalty matricies
        S <- list()
        S[[1]] <- S[[2]] <- matrix(0,2*q-1,2*q-1) 
        S[[1]][2:q,2:q] <- spl.S(xk)[-1,-1] 
        S[[2]][(q+1):(2*q-1),(q+1):(2*q-1)] <- spl.S(zk)[-1,-1] #     
        # Set up the model matrix X
            n<-length(x)
            X<-matrix(1,n,2*q-1)
            X[,2:q]<-spl.X(x,xk)[,-1]               # 1st smooth
            X[,(q+1):(2*q-1)]<-spl.X(z,zk)[,-1]     # 2nd smooth
            list(X=X,S=S)
        }

    @ 
   
    The function am.setup generates the model matrix and the penalty matrices for two predictor variables and a fixed set of equidistant knots. The knots are an arbitrary choice and only for illustrative purposes. A key aspect is that the function sets up two penalty matrices. To maintain the additive property each penalty matrix gets placed in full penalty matrix that is zeroes outside of the individual penalty matrix. The full penalty matrix has the size of n rows and number of parameters times penalty matrices. For example the first penalty matrix is covers all rows of for the first number of parameters columns and is zero otherwise. The second penalty matrix is 0 for the previous matrix was defined. Setting the penalty matrix up this way allows to estimate the smoothing parameter individually in the next function. The constructed model matrix $X$ and the list of penalty matrices are now sufficient to estimate an additive model.
   
    <<>>=
    fit.am<-function(y,X,S,sp)
        # function to fit simple 2 term additive model 
        { # generate the full penalty matrix and take square root
        rS <- mat.sqrt(sp[1]*S[[1]]+sp[2]*S[[2]])
        q <- ncol(X) # number of params
        n <- nrow(X)
        X1 <- rbind(X,rS) 
        y1<-y;y1[(n+1):(n+q)]<-0 # augment data 
        b<-lm(y1~X1-1) # fit model 
        trA<-sum(influence(b)$hat[1:n]) # tr(A) 
        norm<-sum((y-fitted(b)[1:n])^2) # RSS 
        list(model=b,gcv=norm*n/(n-trA)^2,sp=sp)
    }
    @
   
    
    The function fit.am takes the previously generated model and penalty matrix a vector of $y$ values and a vector of smoothing parameter values and produces a list of the model and the gcv score. Each penalty matrix receives an individual smoothing parameter. Defining the full penalty matrix as zero outside of the individual penalty matrix allows to multiply the smoothing parameter and then add the individual matrices to form the full penalty matrix $rS$. The model matrix on top of the penalty matrix form the complete model matrix $X$. Since the model matrix has more rows that the data  vector $y$ the data vectors receives zeroes to match. The resulting model is then fit via the linear model from section one. After the model is fit the gcv score gets computed and gets stored.
    
    \subsection{Additive Model Example}
    The process of fitting a two term additive model can be best understood by using the  setup.am and fit.am functions on a data set. \cite{gamBook} uses the default R data set $trees$. This data set contains three variables: Volume, Girth and Height for 31 felled cherry trees. The author suggests the following model to illustrate an additive model
    \begin{equation}
        Volumne = f_1(Girth) + f_2(Height) + \epsilon_i
    \end{equation}
    
    This model can be estimated with the functions defined in the previous section. 
    
     <<>>=
    data(trees)
    rg <- range(trees$Girth)
    rh <- range(trees$Height)
    # rescaling of parameter to onto [0,1]
    trees$Girth <- (trees$Girth - rg[1])/(rg[2]-rg[1]) 
    trees$Height <- (trees$Height - rh[1])/(rh[2]-rh[1])
    am0 <- am.setup(trees$Girth,trees$Height)
    @
    The model matrix and the penalty matricies get setup with the rescaled parameters of Girth and Height. The resulting list can then be used to estimate the full model but not the smoothing parameter. To estimate the smoothing parameter the fit.am functions gets called with several choices of $\lambda$. The $\lambda$ that generates the lowest gcv score is estimated by iteratively trying different $\lambda$ values.  
    
    <<>>=
    sp<-c(0,0) # initialize smoothing parameter (s.p.) array 
    for (i in 1:30) for (j in 1:30) # loop over s.p. grid
    { 
        sp[1]<-1e-5*2^(i-1);sp[2]<-1e-5*2^(j-1)     # s.p.s 
        b<-fit.am(trees$Volume,am0$X,am0$S,sp)      # fit using s.p.s.
        if (i+j==2) best<-b else                    # number of data
        if (b$gcv<best$gcv) best<-b                 # augmented X
    }
    best$sp # GCV best smoothing parameter 
    @
    
    The resulting smoothing parameter for Girth is fairly presumably allowing $f_1$ some curvature. Height has a very high smoothing parameter, this most likely results in a rather straight line.  he values of the smooths at the predictor variable values can be obtained quite easily by zeroing all model coefficients, except those corresponding to the term of interest, and using predict as the following code shows.

    
    <<echo=FALSE, cache=TRUE>>=
    
    # plot 
    plot(trees$Volume,fitted(best$model)[1:31],
        xlab="Fitted Volume",ylab="Actual Volume") 
    b<-best$model
    b$coefficients[1]<-0 # zero the intercept 
    b$coefficients[11:19]<-0 # zero the second smooth coefs 
    f0<-predict(b) # predict f_1 only, at data values 
    plot(trees$Girth,f0[1:31],xlab="Scaled Girth",
     ylab=expression(hat(f[1])))

    @
    
    The resulting plot confirms the observation that the Girth smooth has more curvature than the Height smooth. The middle figure is the estimate of the smooth function of Girth at the given Girth data. The right figure is the estimate of the smooth function of Height at the given Height data. 
   
    \subsection{Generalized Additive Model}
    Generalized additive models (GAMs) follow from additive models, as generalized linear models follow from linear models. Like the GLM the GAM predicts some known smooth monotonic function of the expected value of the response. The response following any exponential family distribution. For the sake of illustrating the similarities to a GLM the function fit.am can extended to account for a gamma error and log link function.  
    
    <<>>==
    fit.gamG<-function(y,X,S,sp)
        # function to fit simple 2 term generalized additive model 
        # Gamma errors and log link
        { # get sqrt of combined penalty matrix
            rS <- mat.sqrt(sp[1]*S[[1]]+sp[2]*S[[2]])
            q <- ncol(X) # number of parameterss
            n <- nrow(X) # number of data
            X1 <- rbind(X,rS) # augmented model matrix
            b <- rep(0,q);b[1] <- 1 # initialize parameters
            norm <- 0;old.norm <- 1 # initialize convergence control 
            while (abs(norm-old.norm)>1e-4*norm) # repeat unconverged
            { eta <- (X1%*%b)[1:n] # generate pseudo data by computing expected value of current beta choice
                mu <- exp(eta) # log link, exp of pseudo data
                z <- (y-mu)/mu + eta  # compute pseudo data
                z[(n+1):(n+q)] <- 0 # agument pseudo data
                m <- lm(z~X1-1) # solve the linear model with pseudo data
            }
        list(model=m,gcv=norm*n/(n-trA)^2,sp=sp)
        }
    @
    
    The function fit.gamG takes a model matrix X and a list of penalty matricies S. Both can be generated by the am.setup function. The generalized additive model seeks the best smoothing parameter by iteratively searching for a value until convergence. Each iteration step forms a vector of pseudo data. Pseudo data is the estimated values trained data. If the resulting gcv score of the current pseudo data does not change the model is converged. The fitting process is called penalized iterative reweighted least squares and formally described in the next section.
    
    
    
    
    \subsubsection{Penalized Iterative Reweighted Least Squares}
    While the GLM is fitted by the iterative reweighted least squares GAMs are fitted by the penalized reweighted least squares. The major difference being that the objective is contains the penalty matrix. To fit a generalized additive model the following penalized iteratively re-weighted least squares (P-IRLS) scheme is iterated to convergence.
    
    \begin{enumerate}
    \item Given the current parameter estimated $\beta^{[k]}$ and the estimated mean response vector $\mu^{[k]}$ compute 
    \begin{equation}
        w \propto \frac{1}{V(\mu_i^{[k]}) g^{'}(\mu_i^{[k]})}
    \end{equation}
        
    With the pseudo data vector $z_i= g(\mu_i^{[k]})(y_i-\mu_i^{[k]}) + X_i \beta^{[k]}$ and the $var(Y_i) = V(\mu^{[k]}) \theta$ as described in the IRLS section.
    
    \item Minimize the resulting least squares objective 
    
    \begin{equation}
    \left \|  \begin{bmatrix} \sqrt{W} & 0 \\ 0 & I \end{bmatrix} \left ( \begin{bmatrix} z\\0 \end{bmatrix} -  \begin{bmatrix} X \\ B \end{bmatrix} \right  \beta)\right \| ^2 \end{equation}
    
    Minimizing the least squares objective w.r.t. $\beta$ to obtain $\beta^{[k+1]}$. B is again any square root matrix such that $B^TB = \lambda_1 S_1 +\lambda_2 S_2$
    
    \end{enumrate}

    The P-IRLS accounts for the penalty matrix required to fit additive models with smoothing functions.
    
    \subsubsection{Generalized Additive Model Example}
    Fitting a GAM with the fit.gamG function we can see a slightly better fit than th
    
    <<echo=FALSE, cache=TRUE>>=
    sp<-c(0,0) # initialize smoothing parameter (s.p.) array 
    for (i in 1:30) for (j in 1:30) # loop over s.p. grid
    { 
        sp[1]<-1e-5*2^(i-1);sp[2]<-1e-5*2^(j-1)     # s.p.s 
        b<-fit.gamG(trees$Volume,am0$X,am0$S,sp)      # fit using s.p.s.
        if (i+j==2) best<-b else                    # number of data
        if (b$gcv<best$gcv) best<-b                 # augmented X
    }
    best$sp # GCV best smoothing parameter 
    
    # plot 
    plot(trees$Volume,fitted(best$model)[1:31],
        xlab="Fitted Volume",ylab="Actual Volume") 
    b<-best$model
    b$coefficients[1]<-0 # zero the intercept 
    b$coefficients[11:19]<-0 # zero the second smooth coefs 
    f0<-predict(b) # predict f_1 only, at data values 
    plot(trees$Girth,f0[1:31],xlab="Scaled Girth",
     ylab=expression(hat(f[1])))

    @
    
    
    \section{Apache Spark}
        \subsection{General In Memory Cluster Computing Framework}
        \subsection{The Language Scala and Breeze}
        \subsection{Machine Learning Pipeline}
        \subsubsection{Transformer}
        \subsubsection{Estimator}
        \subsection{Implementing GAM in Apache Spark}
        \subsection{R vs Spark vs SparkR}
        \subsubsection{Architecture}
        
        

    \section{Experiment: Fitting GAMs for large data sets}
    \subsection{Fitting a large GAM with Airbnb data}
        \subsubsection{Why GAMs for Airbnb}
        \subsubsection{Fitting seasonality with GAMs}
    \subsection{How does the performance compare to R?}
        \subsubsection{Memory}
        \subsubsection{CPU}
    \subsection{How does the performance scale?}
        \subsubsection{Memory}
        \subsubsection{CPU}
    
    \section{Conclusion}
    \subsection{Future Work}
    
    
    
    \section{Matrix Algebra Appendix}
        \subsection{QR Decomposition}
        
    


    \newpage

    \bibliography{thesis}    % reference to thesis.bib

    \newpage

\end{document}
